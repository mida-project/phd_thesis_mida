% Encoding: UTF-8
@mastersthesis{calisto2017mimbcdui,
  doi = {10.13140/RG.2.2.15187.02084},
  url = {http://rgdoi.net/10.13140/RG.2.2.15187.02084},
  author = {Calisto, Francisco},
  title = {Medical Imaging Multimodality Breast Cancer Diagnosis User Interface},
  school = {Instituto Superior T\'{e}cnico},
  year = {2017},
  address = {Avenida Rovisco Pais 1, 1049-001 Lisboa - Portugal (EU)},
  month = {10},
  note = {A Medical Imaging Tool for a Multimodality use of Breast Cancer Diagnosis on a User Interface.}
}

@article{Tschandl2020,
author={Tschandl, Philipp
and Rinner, Christoph
and Apalla, Zoe
and Argenziano, Giuseppe
and Codella, Noel
and Halpern, Allan
and Janda, Monika
and Lallas, Aimilios
and Longo, Caterina
and Malvehy, Josep
and Paoli, John
and Puig, Susana
and Rosendahl, Cliff
and Soyer, H. Peter
and Zalaudek, Iris
and Kittler, Harald},
title={Human--computer collaboration for skin cancer recognition},
journal={Nature Medicine},
year={2020},
month={Aug},
day={01},
volume={26},
number={8},
pages={1229-1234},
abstract={The rapid increase in telemedicine coupled with recent advances in diagnostic artificial intelligence (AI) create the imperative to consider the opportunities and risks of inserting AI-based support into new paradigms of care. Here we build on recent achievements in the accuracy of image-based AI for skin cancer diagnosis to address the effects of varied representations of AI-based support across different levels of clinical expertise and multiple clinical workflows. We find that good quality AI-based support of clinical decision-making improves diagnostic accuracy over that of either AI or physicians alone, and that the least experienced clinicians gain the most from AI-based support. We further find that AI-based multiclass probabilities outperformed content-based image retrieval (CBIR) representations of AI in the mobile technology environment, and AI-based support had utility in simulations of second opinions and of telemedicine triage. In addition to demonstrating the potential benefits associated with good quality AI in the hands of non-expert clinicians, we find that faulty AI can mislead the entire spectrum of clinicians, including experts. Lastly, we show that insights derived from AI class-activation maps can inform improvements in human diagnosis. Together, our approach and findings offer a framework for future studies across the spectrum of image-based diagnostics to improve human--computer collaboration in clinical practice.},
issn={1546-170X},
doi={10.1038/s41591-020-0942-0},
url={https://doi.org/10.1038/s41591-020-0942-0}
}

@inproceedings{10.1145/3313831.3376290,
author = {Schaekermann, Mike and Cai, Carrie J. and Huang, Abigail E. and Sayres, Rory},
title = {Expert Discussions Improve Comprehension of Difficult Cases in Medical Image Assessment},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376290},
doi = {10.1145/3313831.3376290},
abstract = {Medical data labeling workflows critically depend on accurate assessments from human experts. Yet human assessments can vary markedly, even among medical experts. Prior research has demonstrated benefits of labeler training on performance. Here we utilized two types of labeler training feedback: highlighting incorrect labels for difficult cases ("individual performance" feedback), and expert discussions from adjudication of these cases. We presented ten generalist eye care professionals with either individual performance alone, or individual performance and expert discussions from specialists. Compared to performance feedback alone, seeing expert discussions significantly improved generalists' understanding of the rationale behind the correct diagnosis while motivating changes in their own labeling approach; and also significantly improved average accuracy on one of four pathologies in a held-out test set. This work suggests that image adjudication may provide benefits beyond developing trusted consensus labels, and that exposure to specialist discussions can be an effective training intervention for medical diagnosis.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {labeler training, diagnosis, medical images, adjudication},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{He2019,
author={He, Jianxing
and Baxter, Sally L.
and Xu, Jie
and Xu, Jiming
and Zhou, Xingtao
and Zhang, Kang},
title={The practical implementation of artificial intelligence technologies in medicine},
journal={Nature Medicine},
year={2019},
month={Jan},
day={01},
volume={25},
number={1},
pages={30-36},
abstract={The development of artificial intelligence (AI)-based technologies in medicine is advancing rapidly, but real-world clinical implementation has not yet become a reality. Here we review some of the key practical issues surrounding the implementation of AI into existing clinical workflows, including data sharing and privacy, transparency of algorithms, data standardization, and interoperability across multiple platforms, and concern for patient safety. We summarize the current regulatory environment in the United States and highlight comparisons with other regions in the world, notably Europe and China.},
issn={1546-170X},
doi={10.1038/s41591-018-0307-0},
url={https://doi.org/10.1038/s41591-018-0307-0}
}

@article{McKinney2020,
author={McKinney, Scott Mayer
and Sieniek, Marcin
and Godbole, Varun
and Godwin, Jonathan
and Antropova, Natasha
and Ashrafian, Hutan
and Back, Trevor
and Chesus, Mary
and Corrado, Greg S.
and Darzi, Ara
and Etemadi, Mozziyar
and Garcia-Vicente, Florencia
and Gilbert, Fiona J.
and Halling-Brown, Mark
and Hassabis, Demis
and Jansen, Sunny
and Karthikesalingam, Alan
and Kelly, Christopher J.
and King, Dominic
and Ledsam, Joseph R.
and Melnick, David
and Mostofi, Hormuz
and Peng, Lily
and Reicher, Joshua Jay
and Romera-Paredes, Bernardino
and Sidebottom, Richard
and Suleyman, Mustafa
and Tse, Daniel
and Young, Kenneth C.
and De Fauw, Jeffrey
and Shetty, Shravya},
title={International evaluation of an AI system for breast cancer screening},
journal={Nature},
year={2020},
month={Jan},
day={01},
volume={577},
number={7788},
pages={89-94},
abstract={Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7{\%} and 1.2{\%} (USA and UK) in false positives and 9.4{\%} and 2.7{\%} in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5{\%}. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88{\%}. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.},
issn={1476-4687},
doi={10.1038/s41586-019-1799-6},
url={https://doi.org/10.1038/s41586-019-1799-6}
}

@article{Ribli2018,
author={Ribli, Dezs{\H{o}}
and Horv{\'a}th, Anna
and Unger, Zsuzsa
and Pollner, P{\'e}ter
and Csabai, Istv{\'a}n},
title={Detecting and classifying lesions in mammograms with Deep Learning},
journal={Scientific Reports},
year={2018},
month={Mar},
day={15},
volume={8},
number={1},
pages={4165},
abstract={In the last two decades, Computer Aided Detection (CAD) systems were developed to help radiologists analyse screening mammograms, however benefits of current CAD technologies appear to be contradictory, therefore they should be improved to be ultimately considered useful. Since 2012, deep convolutional neural networks (CNN) have been a tremendous success in image recognition, reaching human performance. These methods have greatly surpassed the traditional approaches, which are similar to currently used CAD solutions. Deep CNN-s have the potential to revolutionize medical image analysis. We propose a CAD system based on one of the most successful object detection frameworks, Faster R-CNN. The system detects and classifies malignant or benign lesions on a mammogram without any human intervention. The proposed method sets the state of the art classification performance on the public INbreast database, AUC = 0.95. The approach described here has achieved 2nd place in the Digital Mammography DREAM Challenge with AUC = 0.85. When used as a detector, the system reaches high sensitivity with very few false positive marks per image on the INbreast dataset. Source code, the trained model and an OsiriX plugin are published online at https://github.com/riblidezso/frcnn{\_}cad.},
issn={2045-2322},
doi={10.1038/s41598-018-22437-z},
url={https://doi.org/10.1038/s41598-018-22437-z}
}

@inproceedings{10.1145/3290605.3300233,
author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
title = {Guidelines for Human-AI Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300233},
doi = {10.1145/3290605.3300233},
abstract = {Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {design guidelines, human-ai interaction, ai-infused systems},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{10.1145/3411286,
author = {Thieme, Anja and Cutrell, Ed and Morrison, Cecily and Taylor, Alex and Sellen, Abigail},
title = {Interpretability as a Dynamic of Human-AI Interaction},
year = {2020},
issue_date = {September - October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/3411286},
doi = {10.1145/3411286},
journal = {Interactions},
month = sep,
pages = {40–45},
numpages = {6}
}

@inproceedings{10.1145/3313831.3376301,
author = {Yang, Qian and Steinfeld, Aaron and Ros\'{e}, Carolyn and Zimmerman, John},
title = {Re-Examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376301},
doi = {10.1145/3313831.3376301},
abstract = {Artificial Intelligence (AI) plays an increasingly important role in improving HCI and user experience. Yet many challenges persist in designing and innovating valuable human-AI interactions. For example, AI systems can make unpredictable errors, and these errors damage UX and even lead to undesired societal impact. However, HCI routinely grapples with complex technologies and mitigates their unintended consequences. What makes AI different? What makes human-AI interaction appear particularly difficult to design? This paper investigates these questions. We synthesize prior research, our own design and research experience, and our observations when teaching human-AI interaction. We identify two sources of AI's distinctive design challenges: 1) uncertainty surrounding AI's capabilities, 2) AI's output complexity, spanning from simple to adaptive complex. We identify four levels of AI systems. On each level, designers encounter a different subset of the design challenges. We demonstrate how these findings reveal new insights for designers, researchers, and design tool makers in productively addressing the challenges of human-AI interaction going forward.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {prototyping, user experience, sketching, artificial intelligence},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376807,
author = {Xie, Yao and Chen, Melody and Kao, David and Gao, Ge and Chen, Xiang 'Anthony'},
title = {CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376807},
doi = {10.1145/3313831.3376807},
abstract = {The recent development of data-driven AI promises to automate medical diagnosis; however, most AI functions as 'black boxes' to physicians with limited computational knowledge. Using medical imaging as a point of departure, we conducted three iterations of design activities to formulate CheXplain — a system that enables physicians to explore and understand AI-enabled chest X-ray analysis: (i) a paired survey between referring physicians and radiologists reveals whether, when, and what kinds of explanations are needed; (ii) a low-fidelity prototype co-designed with three physicians formulates eight key features; and (iii) a high-fidelity prototype evaluated by another six physicians provides detailed summative insights on how each feature enables the exploration and understanding of AI. We summarize by discussing recommendations for future work to design and implement explainable medical AI systems that encompass four recurring themes: motivation, constraint, explanation, and justification.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {explainable artificial intelligence, system design, physician-centered design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@Article{doi:10.1148/ryai.2020190208,
  author   = {Pacilè, Serena and Lopez, January and Chone, Pauline and Bertinotti, Thomas and Grouin, Jean Marie and Fillard, Pierre},
  title    = {Improving Breast Cancer Detection Accuracy of Mammography with the Concurrent Use of an Artificial Intelligence Tool},
  doi      = {10.1148/ryai.2020190208},
  eprint   = {https://doi.org/10.1148/ryai.2020190208},
  number   = {6},
  pages    = {e190208},
  url      = {https://doi.org/10.1148/ryai.2020190208},
  volume   = {2},
  abstract = {Purpose To evaluate the benefits of an artificial intelligence (AI)–based tool for two-dimensional mammography in the breast cancer detection process. Materials and Methods In this multireader, multicase retrospective study, 14 radiologists assessed a dataset of 240 digital mammography images, acquired between 2013 and 2106, using a counterbalance design in which half of the dataset was read without AI and the other half with the help of AI during a first session and vice versa during a second session, which was separated from the first by a washout period. Area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and reading time were assessed as endpoints. Results The average AUC across readers was 0.769 (95\% CI: 0.724, 0.814) without AI and 0.797 (95\% CI: 0.754, 0.840) with AI. The average difference in AUC was 0.028 (95\% CI: 0.002, 0.055, P = .035). Average sensitivity was increased by 0.033 when using AI support (P = .021). Reading time changed dependently to the AI-tool score. For low likelihood of malignancy (< 2.5\%), the time was about the same in the first reading session and slightly decreased in the second reading session. For higher likelihood of malignancy, the reading time was on average increased with the use of AI. Conclusion This clinical investigation demonstrated that the concurrent use of this AI tool improved the diagnostic performance of radiologists in the detection of breast cancer without prolonging their workflow. Supplemental material is available for this article. © RSNA, 2020},
  journal  = {Radiology: Artificial Intelligence},
  year     = {2020},
}

@Article{doi:10.1148/ryai.2020200057,
  author   = {Wiggins, Walter F. and Caton, M. Travis and Magudia, Kirti and Glomski, Sha-har A. and George, Elizabeth and Rosenthal, Michael H. and Gaviola, Glenn C. and Andriole, Katherine P.},
  title    = {Preparing Radiologists to Lead in the Era of Artificial Intelligence: Designing and Implementing a Focused Data Science Pathway for Senior Radiology Residents},
  doi      = {10.1148/ryai.2020200057},
  eprint   = {https://doi.org/10.1148/ryai.2020200057},
  number   = {ja},
  pages    = {e200057},
  url      = {https://doi.org/10.1148/ryai.2020200057},
  volume   = {0},
  abstract = {Artificial intelligence and machine learning (AI-ML) have taken center stage in medical imaging. To develop as leaders in AI-ML, radiology residents may seek a formative data science experience. We piloted an elective Data Science Pathway (DSP) for fourth year residents at our institution in collaboration with the XX for Clinical Data Science (CCDS). The goal of the DSP was to provide an introduction to AI-ML through a flexible schedule of educational, experiential and research activities. We describe our initial experience with the DSP tailored to the AI-ML interests of three senior radiology residents. We discuss logistics and curricular design with common core elements and shared mentorship. Residents were provided dedicated, full-time immersion into the CCDS work environment. In the initial DSP pilot, residents were successfully integrated into AI-ML projects at CCDS. Residents were exposed to all aspects of AI-ML application development, including data curation, model design, quality control, and clinical testing. Core concepts in AI-ML were taught through didactic sessions and daily collaboration with data scientists and other staff. Work during the pilot period led to twelve accepted abstracts for presentation at national meetings. The DSP is a feasible, well-rounded introductory experience in AI-ML for senior radiology residents. Residents contributed to model and tool development at multiple stages and were academically productive. Feedback from the pilot resulted in establishment of a formal AI-ML curriculum for future residents. The logistical, planning, and curricular considerations we describe provide a framework for DSP implementation at other institutions.},
  journal  = {Radiology: Artificial Intelligence},
  year     = {0},
}

@Article{doi:10.1148/radiol.2019182627,
  author   = {Geras, Krzysztof J. and Mann, Ritse M. and Moy, Linda},
  title    = {Artificial Intelligence for Mammography and Digital Breast Tomosynthesis: Current Concepts and Future Perspectives},
  doi      = {10.1148/radiol.2019182627},
  eprint   = {https://doi.org/10.1148/radiol.2019182627},
  note     = {PMID: 31549948},
  number   = {2},
  pages    = {246-259},
  url      = {https://doi.org/10.1148/radiol.2019182627},
  volume   = {293},
  abstract = {Although computer-aided diagnosis (CAD) is widely used in mammography, conventional CAD programs that use prompts to indicate potential cancers on the mammograms have not led to an improvement in diagnostic accuracy. Because of the advances in machine learning, especially with use of deep (multilayered) convolutional neural networks, artificial intelligence has undergone a transformation that has improved the quality of the predictions of the models. Recently, such deep learning algorithms have been applied to mammography and digital breast tomosynthesis (DBT). In this review, the authors explain how deep learning works in the context of mammography and DBT and define the important technical challenges. Subsequently, they discuss the current status and future perspectives of artificial intelligence–based clinical applications for mammography, DBT, and radiomics. Available algorithms are advanced and approach the performance of radiologists—especially for cancer detection and risk prediction at mammography. However, clinical validation is largely lacking, and it is not clear how the power of deep learning should be used to optimize practice. Further development of deep learning models is necessary for DBT, and this requires collection of larger databases. It is expected that deep learning will eventually have an important role in DBT, including the generation of synthetic images.© RSNA, 2019Online supplemental material is available for this article.},
  journal  = {Radiology},
  year     = {2019},
}

@article{doi:10.3322/caac.21492,
author = {Bray, Freddie and Ferlay, Jacques and Soerjomataram, Isabelle and Siegel, Rebecca L. and Torre, Lindsey A. and Jemal, Ahmedin},
title = {Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries},
journal = {CA: A Cancer Journal for Clinicians},
volume = {68},
number = {6},
pages = {394-424},
keywords = {cancer, epidemiology, incidence, survival},
doi = {10.3322/caac.21492},
url = {https://acsjournals.onlinelibrary.wiley.com/doi/abs/10.3322/caac.21492},
eprint = {https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.3322/caac.21492},
abstract = {Abstract This article provides a status report on the global burden of cancer worldwide using the GLOBOCAN 2018 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer, with a focus on geographic variability across 20 world regions. There will be an estimated 18.1 million new cancer cases (17.0 million excluding nonmelanoma skin cancer) and 9.6 million cancer deaths (9.5 million excluding nonmelanoma skin cancer) in 2018. In both sexes combined, lung cancer is the most commonly diagnosed cancer (11.6\% of the total cases) and the leading cause of cancer death (18.4\% of the total cancer deaths), closely followed by female breast cancer (11.6\%), prostate cancer (7.1\%), and colorectal cancer (6.1\%) for incidence and colorectal cancer (9.2\%), stomach cancer (8.2\%), and liver cancer (8.2\%) for mortality. Lung cancer is the most frequent cancer and the leading cause of cancer death among males, followed by prostate and colorectal cancer (for incidence) and liver and stomach cancer (for mortality). Among females, breast cancer is the most commonly diagnosed cancer and the leading cause of cancer death, followed by colorectal and lung cancer (for incidence), and vice versa (for mortality); cervical cancer ranks fourth for both incidence and mortality. The most frequently diagnosed cancer and the leading cause of cancer death, however, substantially vary across countries and within each country depending on the degree of economic development and associated social and life style factors. It is noteworthy that high-quality cancer registry data, the basis for planning and implementing evidence-based cancer control programs, are not available in most low- and middle-income countries. The Global Initiative for Cancer Registry Development is an international partnership that supports better estimation, as well as the collection and use of local data, to prioritize and evaluate national cancer control efforts. CA: A Cancer Journal for Clinicians 2018;0:1-31. © 2018 American Cancer Society},
year = {2018}
}

@article{doi:10.1002/cncr.32859,
author = {Duffy, Stephen W. and Tabár, László and Yen, Amy Ming-Fang and Dean, Peter B. and Smith, Robert A. and Jonsson, Håkan and Törnberg, Sven and Chen, Sam Li-Sheng and Chiu, Sherry Yueh-Hsia and Fann, Jean Ching-Yuan and Ku, May Mei-Sheng and Wu, Wendy Yi-Ying and Hsu, Chen-Yang and Chen, Yu-Ching and Svane, Gunilla and Azavedo, Edward and Grundström, Helene and Sundén, Per and Leifland, Karin and Frodis, Ewa and Ramos, Joakim and Epstein, Birgitta and Åkerlund, Anders and Sundbom, Ann and Bordás, Pál and Wallin, Hans and Starck, Leena and Björkgren, Annika and Carlson, Stina and Fredriksson, Irma and Ahlgren, Johan and Öhman, Daniel and Holmberg, Lars and Chen, Tony Hsiu-Hsi},
title = {Mammography screening reduces rates of advanced and fatal breast cancers: Results in 549,091 women},
journal = {Cancer},
volume = {126},
number = {13},
pages = {2971-2979},
keywords = {breast cancer, fatality, mammography, mortality, screening},
doi = {10.1002/cncr.32859},
url = {https://acsjournals.onlinelibrary.wiley.com/doi/abs/10.1002/cncr.32859},
eprint = {https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.1002/cncr.32859},
abstract = {Background It is of paramount importance to evaluate the impact of participation in organized mammography service screening independently from changes in breast cancer treatment. This can be done by measuring the incidence of fatal breast cancer, which is based on the date of diagnosis and not on the date of death. Methods Among 549,091 women, covering approximately 30\% of the Swedish screening-eligible population, the authors calculated the incidence rates of 2473 breast cancers that were fatal within 10 years after diagnosis and the incidence rates of 9737 advanced breast cancers. Data regarding each breast cancer diagnosis and the cause and date of death of each breast cancer case were gathered from national Swedish registries. Tumor characteristics were collected from regional cancer centers. Aggregated data concerning invitation and participation were provided by Sectra Medical Systems AB. Incidence rates were analyzed using Poisson regression. Results Women who participated in mammography screening had a statistically significant 41\% reduction in their risk of dying of breast cancer within 10 years (relative risk, 0.59; 95\% CI, 0.51-0.68 [P < .001]) and a 25\% reduction in the rate of advanced breast cancers (relative risk, 0.75; 95\% CI, 0.66-0.84 [P < .001]). Conclusions Substantial reductions in the incidence rate of breast cancers that were fatal within 10 years after diagnosis and in the advanced breast cancer rate were found in this contemporaneous comparison of women participating versus those not participating in screening. These benefits appeared to be independent of recent changes in treatment regimens.},
year = {2020}
}

@article{10.1093/jnci/djaa080,
    author = {Farber, Rachel and Houssami, Nehmat and Wortley, Sally and Jacklyn, Gemma and Marinovich, Michael L and McGeechan, Kevin and Barratt, Alexandra and Bell, Katy},
    title = "{Impact of Full-Field Digital Mammography Versus Film-Screen Mammography in Population Screening: A Meta-Analysis}",
    journal = {JNCI: Journal of the National Cancer Institute},
    year = {2020},
    month = {06},
    abstract = "{Breast screening programs replaced film mammography with digital mammography, and the effects of this practice shift in population screening on health outcomes can be measured through examination of cancer detection and interval cancer rates.A systematic review and random effects meta-analysis were undertaken. Seven databases were searched for publications that compared film with digital mammography within the same population of asymptomatic women and reported cancer detection and/or interval cancer rates.The analysis included 24 studies with 16 583 743 screening examinations (10 968 843 film and 5 614 900 digital). The pooled difference in the cancer detection rate showed an increase of 0.51 per 1000 screens (95\\% confidence interval [CI] = 0.19 to 0.83), greater relative increase for ductal carcinoma in situ (25.2\\%, 95\\% CI = 17.4\\% to 33.5\\%) than invasive (4\\%, 95\\% CI = −3\\% to 13\\%), and a recall rate increase of 6.95 (95\\% CI = 3.47 to 10.42) per 1000 screens after the transition from film to digital mammography. Seven studies (80.8\\% of screens) reported interval cancers: the pooled difference showed no change in the interval cancer rate with −0.02 per 1000 screens (95\\% CI = −0.06 to 0.03). Restricting analysis to studies at low risk of bias resulted in findings consistent with the overall pooled results for all outcomes.The increase in cancer detection following the practice shift to digital mammography did not translate into a reduction in the interval cancer rate. Recall rates were increased. These results suggest the transition from film to digital mammography did not result in health benefits for screened women. This analysis reinforces the need to carefully evaluate effects of future changes in technology, such as tomosynthesis, to ensure new technology leads to improved health outcomes and beyond technical gains.}",
    issn = {0027-8874},
    doi = {10.1093/jnci/djaa080},
    url = {https://doi.org/10.1093/jnci/djaa080},
    note = {djaa080},
    eprint = {https://academic.oup.com/jnci/advance-article-pdf/doi/10.1093/jnci/djaa080/33801195/djaa080.pdf},
}

@article{Seely2018,
author={Seely, J. M.
and Alhassan, T.},
title={Screening for breast cancer in 2018-what should we be doing today?},
journal={Current oncology (Toronto, Ont.)},
year={2018},
month={Jun},
edition={2018/06/13},
publisher={Multimed Inc.},
volume={25},
number={Suppl 1},
pages={S115-S124},
keywords={*Breast cancer; *digital breast tomosynthesis; *overdiagnosis; *screening mammography; Biopsy; Breast Neoplasms/*diagnosis/epidemiology/pathology; *Early Detection of Cancer/history/methods/trends; False Positive Reactions; Female; History, 21st Century; Humans; Magnetic Resonance Imaging; Mammography; Mass Screening/history/methods/trends; Medical Oncology/history/methods/trends; Ultrasonography, Mammary},
abstract={Although screening mammography has delivered many benefits since its introduction in Canada in 1988, questions about perceived harms warrant an up-to-date review. To help oncologists and physicians provide optimal patient recommendations, the literature was reviewed to find the latest guidelines for screening mammography, including benefits and perceived harms of overdiagnosis, false positives, false negatives, and technologic advances. For women 40-74 years of age who actually participate in screening every 1-2 years, breast cancer mortality is reduced by 40{\%}. With appropriate corrections, overdiagnosis accounts for 10{\%} or fewer breast cancers. False positives occur in about 10{\%} of screened women, 80{\%} of which are resolved with additional imaging, and 10{\%}, with breast biopsy. An important limitation of screening is the false negatives (15{\%}-20{\%}). The technologic advances of digital breast tomosynthesis, breast ultrasonography, and magnetic resonance imaging counter the false negatives of screening mammography, particularly in women with dense breast tissue.},
issn={1718-7729},
doi={10.3747/co.25.3770},
url={https://doi.org/10.3747/co.25.3770},
language={eng}
}

@article{Oeffinger2015,
author={Oeffinger, Kevin C.
and Fontham, Elizabeth T. H.
and Etzioni, Ruth
and Herzig, Abbe
and Michaelson, James S.
and Shih, Ya-Chen Tina
and Walter, Louise C.
and Church, Timothy R.
and Flowers, Christopher R.
and LaMonte, Samuel J.
and Wolf, Andrew M. D.
and DeSantis, Carol
and Lortet-Tieulent, Joannie
and Andrews, Kimberly
and Manassaram-Baptiste, Deana
and Saslow, Debbie
and Smith, Robert A.
and Brawley, Otis W.
and Wender, Richard
and Society, American Cancer},
title={Breast Cancer Screening for Women at Average Risk: 2015 Guideline Update From the American Cancer Society},
journal={JAMA},
year={2015},
month={Oct},
day={20},
volume={314},
number={15},
pages={1599-1614},
keywords={Adult; Age Factors; Breast Neoplasms/*diagnostic imaging/mortality; Early Detection of Cancer; Evidence-Based Medicine; Female; Health Status; Humans; Life Expectancy; Mammography/*standards; Middle Aged; Review Literature as Topic; Risk; Ultrasonography},
abstract={IMPORTANCE: Breast cancer is a leading cause of premature mortality among US women. Early detection has been shown to be associated with reduced breast cancer morbidity and mortality. OBJECTIVE: To update the American Cancer Society (ACS) 2003 breast cancer screening guideline for women at average risk for breast cancer. PROCESS: The ACS commissioned a systematic evidence review of the breast cancer screening literature to inform the update and a supplemental analysis of mammography registry data to address questions related to the screening interval. Formulation of recommendations was based on the quality of the evidence and judgment (incorporating values and preferences) about the balance of benefits and harms. EVIDENCE SYNTHESIS: Screening mammography in women aged 40 to 69 years is associated with a reduction in breast cancer deaths across a range of study designs, and inferential evidence supports breast cancer screening for women 70 years and older who are in good health. Estimates of the cumulative lifetime risk of false-positive examination results are greater if screening begins at younger ages because of the greater number of mammograms, as well as the higher recall rate in younger women. The quality of the evidence for overdiagnosis is not sufficient to estimate a lifetime risk with confidence. Analysis examining the screening interval demonstrates more favorable tumor characteristics when premenopausal women are screened annually vs biennially. Evidence does not support routine clinical breast examination as a screening method for women at average risk. RECOMMENDATIONS: The ACS recommends that women with an average risk of breast cancer should undergo regular screening mammography starting at age 45 years (strong recommendation). Women aged 45 to 54 years should be screened annually (qualified recommendation). Women 55 years and older should transition to biennial screening or have the opportunity to continue screening annually (qualified recommendation). Women should have the opportunity to begin annual screening between the ages of 40 and 44 years (qualified recommendation). Women should continue screening mammography as long as their overall health is good and they have a life expectancy of 10 years or longer (qualified recommendation). The ACS does not recommend clinical breast examination for breast cancer screening among average-risk women at any age (qualified recommendation). CONCLUSIONS AND RELEVANCE: These updated ACS guidelines provide evidence-based recommendations for breast cancer screening for women at average risk of breast cancer. These recommendations should be considered by physicians and women in discussions about breast cancer screening.},
note={26501536[pmid]},
issn={1538-3598},
doi={10.1001/jama.2015.12783},
url={https://doi.org/10.1001/jama.2015.12783},
language={eng}
}

@article{Koczkodaj2019,
author={Koczkodaj, Pawe{\l}
and Sulkowska, Urszula
and Gotlib, Joanna
and Ma{\'{n}}czuk, Marta},
title={Breast cancer mortality trends in Europe among women in perimenopausal and postmenopausal age (45+)},
journal={Archives of medical science : AMS},
year={2019},
month={Jul},
day={11},
publisher={Termedia Publishing House},
volume={16},
number={1},
pages={146-156},
keywords={Europe; European Union; breast cancer; mammography; mortality; screening},
abstract={INTRODUCTION: The aim of the study was to analyze breast cancer (BC) mortality trends among women at the age of 45 years old and older (45+) in the 28 European Union (EU) countries, as well as in 3 non-EU countries - Norway, Switzerland and the Russian Federation (control group) within the period 1959-2017. MATERIAL AND METHODS: Mortality and population data were sourced from the World Health Organization (WHO) database, and age-standardized mortality rates were calculated using the standard world population. Changes in mortality trends were analyzed using Joinpoint Trend Analysis Software. RESULTS: The majority of analyzed countries showed a meaningful decrease in BC mortality among women aged 45+. However, the results of our study suggest that there are 4 EU countries - Croatia, Poland, Romania and Slovakia - where increasing BC mortality trends started to be visible in the analyzed age group. Currently, the observed increase is still not significant, but the obtained data suggest the possibility of further continuation of the observed trend in the future. Moreover, in Bulgaria we also noted continuation of the increase in BC mortality (statistically significant). CONCLUSIONS: Due to the availability of better treatment options, as well as presence of effective tools for detecting BC at the early stages of progression, BC mortality is falling in most analyzed European countries. To maintain this situation and to stop BC mortality increase in the analyzed age group in Bulgaria, Croatia, Poland, Romania and Slovakia, immediate actions for improvement of BC management in the European health care systems should be considered.},
note={32051718[pmid]},
issn={1734-1922},
doi={10.5114/aoms.2019.85198},
url={https://doi.org/10.5114/aoms.2019.85198},
language={eng}
}

@article{Dafni2019,
author={Dafni, U.
and Tsourti, Z.
and Alatsathianos, I.},
title={Breast Cancer Statistics in the European Union: Incidence and Survival across European Countries},
journal={Breast Care},
year={2019},
volume={14},
number={6},
pages={344-353},
abstract={The current status and time trends in breast cancer incidence and survival in the 28 European Union countries (EU-28) is presented here. Rates reported are age adjusted and standardized (ASR). A high incidence and high survival rates were observed in the Northern and Western European countries, with the exception of the Baltic countries. The higher incidence is partly attributed to the higher prevalence of lifestyle risk factors, while the higher survival is attributed to better access to beneficial treatments and general health care. Most of the countries in Southern Europe or the former Eastern Bloc have not yet reached the high GDP per capita status (2017 purchasing power parity; PPP) of the earlier established Western democracies. The breast cancer incidence and survival are associated with the PPP level (both higher for the higher PPP category; 2017 PPP above USD 40,000). Overall, a trend toward higher survival rates was observed throughout this first period of the 21st century, with the incidence for most countries either stabilizing at the 2010 levels or decreasing further.},
issn={1661-3791},
doi={10.1159/000503219},
url={https://doi.org/10.1159/000503219}
}

@article{KIM2020e138,
title = "Changes in cancer detection and false-positive recall in mammography using artificial intelligence: a retrospective, multireader study",
journal = "The Lancet Digital Health",
volume = "2",
number = "3",
pages = "e138 - e148",
year = "2020",
issn = "2589-7500",
doi = "https://doi.org/10.1016/S2589-7500(20)30003-0",
url = "http://www.sciencedirect.com/science/article/pii/S2589750020300030",
author = "Hyo-Eun Kim and Hak Hee Kim and Boo-Kyung Han and Ki Hwan Kim and Kyunghwa Han and Hyeonseob Nam and Eun Hye Lee and Eun-Kyung Kim",
abstract = "Summary
Background
Mammography is the current standard for breast cancer screening. This study aimed to develop an artificial intelligence (AI) algorithm for diagnosis of breast cancer in mammography, and explore whether it could benefit radiologists by improving accuracy of diagnosis.
Methods
In this retrospective study, an AI algorithm was developed and validated with 170 230 mammography examinations collected from five institutions in South Korea, the USA, and the UK, including 36 468 cancer positive confirmed by biopsy, 59 544 benign confirmed by biopsy (8827 mammograms) or follow-up imaging (50 717 mammograms), and 74 218 normal. For the multicentre, observer-blinded, reader study, 320 mammograms (160 cancer positive, 64 benign, 96 normal) were independently obtained from two institutions. 14 radiologists participated as readers and assessed each mammogram in terms of likelihood of malignancy (LOM), location of malignancy, and necessity to recall the patient, first without and then with assistance of the AI algorithm. The performance of AI and radiologists was evaluated in terms of LOM-based area under the receiver operating characteristic curve (AUROC) and recall-based sensitivity and specificity.
Findings
The AI standalone performance was AUROC 0·959 (95% CI 0·952–0·966) overall, and 0·970 (0·963–0·978) in the South Korea dataset, 0·953 (0·938–0·968) in the USA dataset, and 0·938 (0·918–0·958) in the UK dataset. In the reader study, the performance level of AI was 0·940 (0·915–0·965), significantly higher than that of the radiologists without AI assistance (0·810, 95% CI 0·770–0·850; p<0·0001). With the assistance of AI, radiologists' performance was improved to 0·881 (0·850–0·911; p<0·0001). AI was more sensitive to detect cancers with mass (53 [90%] vs 46 [78%] of 59 cancers detected; p=0·044) or distortion or asymmetry (18 [90%] vs ten [50%] of 20 cancers detected; p=0·023) than radiologists. AI was better in detection of T1 cancers (73 [91%] vs 59 [74%] of 80; p=0·0039) or node-negative cancers (104 [87%] vs 88 [74%] of 119; p=0·0025) than radiologists.
Interpretation
The AI algorithm developed with large-scale mammography data showed better diagnostic performance in breast cancer detection compared with radiologists. The significant improvement in radiologists' performance when aided by AI supports application of AI to mammograms as a diagnostic support tool.
Funding
Lunit."
}

@article{10.1001/jamainternmed.2015.5231,
    author = {Lehman, Constance D. and Wellman, Robert D. and Buist, Diana S. M. and Kerlikowske, Karla and Tosteson, Anna N. A. and Miglioretti, Diana L. and for the Breast Cancer Surveillance Consortium},
    title = "{Diagnostic Accuracy of Digital Screening Mammography With and Without Computer-Aided Detection}",
    journal = {JAMA Internal Medicine},
    volume = {175},
    number = {11},
    pages = {1828-1837},
    year = {2015},
    month = {11},
    abstract = "{After the US Food and Drug Administration (FDA) approved computer-aided detection (CAD) for mammography in 1998, and the Centers for Medicare and Medicaid Services (CMS) provided increased payment in 2002, CAD technology disseminated rapidly. Despite sparse evidence that CAD improves accuracy of mammographic interpretations and costs over \\$400 million a year, CAD is currently used for most screening mammograms in the United States.To measure performance of digital screening mammography with and without CAD in US community practice.We compared the accuracy of digital screening mammography interpreted with (n = 495 818) vs without (n = 129 807) CAD from 2003 through 2009 in 323 973 women. Mammograms were interpreted by 271 radiologists from 66 facilities in the Breast Cancer Surveillance Consortium. Linkage with tumor registries identified 3159 breast cancers in 323 973 women within 1 year of the screening.Mammography performance (sensitivity, specificity, and screen-detected and interval cancers per 1000 women) was modeled using logistic regression with radiologist-specific random effects to account for correlation among examinations interpreted by the same radiologist, adjusting for patient age, race/ethnicity, time since prior mammogram, examination year, and registry. Conditional logistic regression was used to compare performance among 107 radiologists who interpreted mammograms both with and without CAD.Screening performance was not improved with CAD on any metric assessed. Mammography sensitivity was 85.3\\% (95\\% CI, 83.6\\%-86.9\\%) with and 87.3\\% (95\\% CI, 84.5\\%-89.7\\%) without CAD. Specificity was 91.6\\% (95\\% CI, 91.0\\%-92.2\\%) with and 91.4\\% (95\\% CI, 90.6\\%-92.0\\%) without CAD. There was no difference in cancer detection rate (4.1 in 1000 women screened with and without CAD). Computer-aided detection did not improve intraradiologist performance. Sensitivity was significantly decreased for mammograms interpreted with vs without CAD in the subset of radiologists who interpreted both with and without CAD (odds ratio, 0.53; 95\\% CI, 0.29-0.97).Computer-aided detection does not improve diagnostic accuracy of mammography. These results suggest that insurers pay more for CAD with no established benefit to women.}",
    issn = {2168-6106},
    doi = {10.1001/jamainternmed.2015.5231},
    url = {https://doi.org/10.1001/jamainternmed.2015.5231},
    eprint = {https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2443369/ioi150084.pdf},
}

@article{10.1001/jamainternmed.2014.981,
    author = {Tosteson, Anna N. A. and Fryback, Dennis G. and Hammond, Cristina S. and Hanna, Lucy G. and Grove, Margaret R. and Brown, Mary and Wang, Qianfei and Lindfors, Karen and Pisano, Etta D.},
    title = "{Consequences of False-Positive Screening Mammograms}",
    journal = {JAMA Internal Medicine},
    volume = {174},
    number = {6},
    pages = {954-961},
    year = {2014},
    month = {06},
    abstract = "{False-positive mammograms, a common occurrence in breast cancer screening programs, represent a potential screening harm that is currently being evaluated by the US Preventive Services Task Force.To measure the effect of false-positive mammograms on quality of life by measuring personal anxiety, health utility, and attitudes toward future screening.The Digital Mammographic Imaging Screening Trial (DMIST) quality-of-life substudy telephone survey was performed shortly after screening and 1 year later at 22 DMIST sites and included randomly selected DMIST participants with positive and negative mammograms.Mammogram requiring follow-up testing or referral without a cancer diagnosis.The 6-question short form of the Spielberger State-Trait Anxiety Inventory state scale (STAI-6) and the EuroQol EQ-5D instrument with US scoring. Attitudes toward future screening as measured by women’s self-report of future intention to undergo mammographic screening and willingness to travel and stay overnight to undergo a hypothetical new type of mammography that would identify as many cancers with half the false-positive results.Among 1450 eligible women invited to participate, 1226 (84.6\\%) were enrolled, with follow-up interviews obtained in 1028 (83.8\\%). Anxiety was significantly higher for women with false-positive mammograms (STAI-6, 35.2 vs 32.7), but health utility scores did not differ and there were no significant differences between groups at 1 year. Future screening intentions differed by group (25.7\\% vs 14.2\\% more likely in false-positive vs negative groups); willingness to travel and stay overnight did not (9.9\\% vs 10.5\\% in false-positive vs negative groups). Future screening intention was significantly increased among women with false-positive mammograms (odds ratio, 2.12; 95\\% CI, 1.54-2.93), younger age (2.78; 1.5-5.0), and poorer health (1.63; 1.09-2.43). Women’s anticipated high-level anxiety regarding future false-positive mammograms was associated with willingness to travel overnight (odds ratio, 1.94; 95\\% CI, 1.28-2.95).False-positive mammograms were associated with increased short-term anxiety but not long-term anxiety, and there was no measurable health utility decrement. False-positive mammograms increased women’s intention to undergo future breast cancer screening and did not increase their stated willingness to travel to avoid a false-positive result. Our finding of time-limited harm after false-positive screening mammograms is relevant for clinicians who counsel women on mammographic screening and for screening guideline development groups.}",
    issn = {2168-6106},
    doi = {10.1001/jamainternmed.2014.981},
    url = {https://doi.org/10.1001/jamainternmed.2014.981},
    eprint = {https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/1861037/ioi140027.pdf},
}

@article{Houssami2017,
author={Houssami, Nehmat
and Hunter, Kylie},
title={The epidemiology, radiology and biological characteristics of interval breast cancers in population mammography screening},
journal={npj Breast Cancer},
year={2017},
month={Apr},
day={13},
volume={3},
number={1},
pages={12},
abstract={An interval breast cancer is a cancer that emerges following a negative mammographic screen. This overview describes the epidemiology, and the radiological and biological characteristics of interval breast cancers in population mammography screening. Notwithstanding possible differences in ascertainment of interval breast cancers, there was broad variability in reported interval breast cancer rates (range 7.0 to 49.3 per 10,000 screens) reflecting heterogeneity in underlying breast cancer rates, screening rounds (initial or repeat screens), and the length and phase of the inter-screening interval. The majority of studies (based on biennial screening) reported interval breast cancer rates in the range of 8.4 to 21.1 per 10,000 screens spanning the two-year interval with the larger proportion occurring in the second year. Despite methodological limitations inherent in radiological surveillance (retrospective mammographic review) of interval breast cancers, this form of surveillance consistently reveals that the majority of interval cancers represent either true interval or occult cancers that were not visible on the index mammographic screen; approximately 20--25{\%} of interval breast cancers are classified as having been missed (false-negatives). The biological characteristics of interval breast cancers show that they have relatively worse tumour prognostic characteristics and biomarker profile, and also survival outcomes, than screen-detected breast cancers; however, they have similar characteristics and prognosis as breast cancers occurring in non-screened women. There was limited evidence on the effect on interval breast cancer frequency and outcomes following transition from film to digital mammography screening.},
issn={2374-4677},
doi={10.1038/s41523-017-0014-x},
url={https://doi.org/10.1038/s41523-017-0014-x}
}

@Article{doi:10.1056/NEJMe1912943,
  author  = {Longo, Dan L.},
  title   = {Detecting Breast Cancer in Women with Dense Breasts},
  doi     = {10.1056/NEJMe1912943},
  eprint  = {https://doi.org/10.1056/NEJMe1912943},
  note    = {PMID: 31774964},
  number  = {22},
  pages   = {2169-2170},
  url     = {https://doi.org/10.1056/NEJMe1912943},
  volume  = {381},
  journal = {New England Journal of Medicine},
  year    = {2019},
}

@article{Topol2019,
author={Topol, Eric J.},
title={High-performance medicine: the convergence of human and artificial intelligence},
journal={Nature Medicine},
year={2019},
month={Jan},
day={01},
volume={25},
number={1},
pages={44-56},
abstract={The use of artificial intelligence, and the deep-learning subtype in particular, has been enabled by the use of labeled big data, along with markedly enhanced computing power and cloud storage, across all sectors. In medicine, this is beginning to have an impact at three levels: for clinicians, predominantly via rapid, accurate image interpretation; for health systems, by improving workflow and the potential for reducing medical errors; and for patients, by enabling them to process their own data to promote health. The current limitations, including bias, privacy and security, and lack of transparency, along with the future directions of these applications will be discussed in this article. Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient--doctor relationship or facilitate its erosion remains to be seen.},
issn={1546-170X},
doi={10.1038/s41591-018-0300-7},
url={https://doi.org/10.1038/s41591-018-0300-7}
}

@Article{info:doi/10.2196/10010,
  author   = {Shen, Jiayi and Zhang, Casper J P and Jiang, Bangsheng and Chen, Jiebin and Song, Jian and Liu, Zherui and He, Zonglin and Wong, Sum Yi and Fang, Po-Han and Ming, Wai-Kit},
  title    = {Artificial Intelligence Versus Clinicians in Disease Diagnosis: Systematic Review},
  doi      = {10.2196/10010},
  issn     = {2291-9694},
  number   = {3},
  pages    = {e10010},
  url      = {https://doi.org/10.2196/10010},
  volume   = {7},
  abstract = {Background: Artificial intelligence (AI) has been extensively used in a range of medical fields to promote therapeutic development. The development of diverse AI techniques has also contributed to early detections, disease diagnoses, and referral management. However, concerns about the value of advanced AI in disease diagnosis have been raised by health care professionals, medical service providers, and health policy decision makers. Objective: This review aimed to systematically examine the literature, in particular, focusing on the performance comparison between advanced AI and human clinicians to provide an up-to-date summary regarding the extent of the application of AI to disease diagnoses. By doing so, this review discussed the relationship between the current advanced AI development and clinicians with respect to disease diagnosis and thus therapeutic development in the long run. Methods: We systematically searched articles published between January 2000 and March 2019 following the Preferred Reporting Items for Systematic reviews and Meta-Analysis in the following databases: Scopus, PubMed, CINAHL, Web of Science, and the Cochrane Library. According to the preset inclusion and exclusion criteria, only articles comparing the medical performance between advanced AI and human experts were considered. Results: A total of 9 articles were identified. A convolutional neural network was the commonly applied advanced AI technology. Owing to the variation in medical fields, there is a distinction between individual studies in terms of classification, labeling, training process, dataset size, and algorithm validation of AI. Performance indices reported in articles included diagnostic accuracy, weighted errors, false-positive rate, sensitivity, specificity, and the area under the receiver operating characteristic curve. The results showed that the performance of AI was at par with that of clinicians and exceeded that of clinicians with less experience. Conclusions: Current AI development has a diagnostic performance that is comparable with medical experts, especially in image recognition-related fields. Further studies can be extended to other types of medical imaging such as magnetic resonance imaging and other medical practices unrelated to images. With the continued development of AI-assisted technologies, the clinical implications underpinned by clinicians' experience and guided by patient-centered health care principle should be constantly considered in future AI-related and other technology-based medical research.},
  day      = {16},
  journal  = {JMIR Med Inform},
  keywords = {artificial intelligence; deep learning; diagnosis; diagnostic imaging; image interpretation, computer-assisted; patient-centered care},
  month    = {Aug},
  year     = {2019},
}

@article{doi:10.1002/j.2051-3909.2012.tb00169.x,
author = {Moran, S and Warren-Forward, H},
title = {The Australian BreastScreen workforce: a snapshot},
journal = {Radiographer},
volume = {59},
number = {1},
pages = {26-30},
keywords = {BreastScreen Australia, demographics, mammography},
doi = {10.1002/j.2051-3909.2012.tb00169.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2051-3909.2012.tb00169.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2051-3909.2012.tb00169.x},
abstract = {Abstract Purpose: BreastScreen Australia programmes employ qualified, skilled radiographers who provide radiologists with high quality mammographic images for interpretation. Workforce issues are a key factor in productivity and capacity, particularly in light of the aging of the population and the possible expansion of the target age group. The purpose of this paper is to provide a snapshot of the demographics and current working practices of radiographers currently employed by BreastScreen Australia. Methods: A questionnaire was distributed to radiographers employed in BreastScreen Australia Services. This paper reports on responses to questions relating to demographics and current working practices. Results: Two hundred and fifty three questionnaires were returned. Of radiographers within Australian BreastScreen programmes 53\% are over 50 years old, 69\% were trained in Australia and have been undertaking mammography for 10 years or more. Radiographers under 35 years old make up 12\% of the current workforce. Of the BreastScreen workforce, 63\% are employed part time. Conclusion: An estimated 78\% response rate implies that the data obtained from Australian radiographers currently working in BreastScreen is representative. Within the next five to ten years it is estimated that 30\% of BreastScreen radiographers may retire. Strategies are needed to increase workforce numbers, in order to cope with increased participation rates due to population growth and an increased target age range.},
year = {2012}
}

@article{rimmer2017radiologist,
  title={Radiologist shortage leaves patient care at risk, warns royal college},
  author={Rimmer, Abi},
  journal={BMJ: British Medical Journal (Online)},
  volume={359},
  year={2017},
  publisher={BMJ Publishing Group LTD}
}

@conference{https://doi.org/10.13140/rg.2.2.29816.70409,
  doi = {10.13140/RG.2.2.29816.70409},
  url = {http://rgdoi.net/10.13140/RG.2.2.29816.70409},
  author = {Calisto,  Francisco Maria and Miraldo,  Pedro and Nunes,  Nuno Jardim and Nascimento,  Jacinto C.},
  language = {en},
  title = {BreastScreening: A Multimodality Diagnostic Assistant},
  booktitle = {LARSyS 2018 Annual Meeting},
  series = {LARSyS AM '18},
  location = {Lisbon, Portugal},
  publisher = {Interactive Technologies Institute},
  year = {2018},
  month = {6},
  pages={1--2}
}

@conference{https://doi.org/10.13140/rg.2.2.25412.68486,
  doi = {10.13140/RG.2.2.25412.68486},
  url = {http://rgdoi.net/10.13140/RG.2.2.25412.68486},
  author = {Calisto,  Francisco Maria and Nunes,  Nuno Jardim and Nascimento,  Jacinto C. and Miraldo,  Pedro},
  language = {en},
  title = {BreastScreening: A Multimodality Diagnostic Assistant},
  booktitle = {National Science Summit 2018},
  series = {NSS '18},
  location = {Lisbon, Portugal},
  publisher = {Funda\c{c}\~{a}o para a Ci\^{e}ncia e Tecnologia},
  year = {2018},
  month = {6},
  pages={1--2}
}

@INPROCEEDINGS{9098470,
  author={G. {Maicas} and C. {Nguyen} and F. {Motlagh} and J. C. {Nascimento} and G. {Carneiro}},
  booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)}, 
  title={Unsupervised Task Design to Meta-Train Medical Image Classifiers}, 
  year={2020},
  number={9098470},
  pages={1339-1342},
  doi={10.1109/ISBI45749.2020.9098470}
}

@techreport{hugo2020si,
  doi = {10.13140/RG.2.2.35800.24329/5},
  url = {http://rgdoi.net/10.13140/RG.2.2.35800.24329/5},
  author = {Lencastre,  Hugo},
  title = {Master Project: Breast Cancer Multimodality Scalable Interactions},
  institution = {Instituto Superior T\'{e}cnico},
  year = {2020},
  address = {Avenida Rovisco Pais 1, 1049-001 Lisboa - Portugal (EU)},
  month = {1},
  publisher = {ResearchGate},
  note = {Improving a framework with novel interactive techniques which enhances the diagnostic process.}
}

@conference{https://doi.org/10.13140/rg.2.2.16086.88649,
  doi = {10.13140/RG.2.2.16086.88649},
  url = {http://rgdoi.net/10.13140/RG.2.2.16086.88649},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Medical Imaging Multimodality Annotating Framework},
  booktitle = {PhD Open Days 2020},
  series = {POD '20},
  location = {Lisbon, Portugal},
  publisher = {Instituto Superior T\'{e}cnico},
  year = {2020},
  month = {10},
  pages={1--2}
}

@techreport{https://doi.org/10.13140/rg.2.2.14792.55049,
  doi = {10.13140/RG.2.2.14792.55049},
  url = {http://rgdoi.net/10.13140/RG.2.2.14792.55049},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Breast Cancer Medical Imaging Multimodality Lesion Contours Annotating Method},
  institution = {Instituto Superior T\'{e}cnico},
  year = {2020},
  number = {116801},
  address = {Avenida Rovisco Pais 1, 1049-001 Lisboa - Portugal (EU)},
  month = {10},
  note = {Method and process using a system to annotate and visualize masses and microcalcifications of breast cancer lesions in a multimodality strategy.}
}

@InProceedings{Yue_2020_CVPR,
author = {Yue, Huanjing and Cao, Cong and Liao, Lei and Chu, Ronghe and Yang, Jingyu},
title = {Supervised Raw Video Denoising With a Benchmark Dataset on Dynamic Scenes},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{https://doi.org/10.13140/rg.2.2.14314.95685/1,
  doi = {10.13140/RG.2.2.14314.95685/1},
  url = {http://rgdoi.net/10.13140/RG.2.2.14314.95685/1},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Towards the Human-Centered Design of Intelligent Agents in Medical Imaging Diagnosis Thesis Proposal Problems and Contributions},
  publisher = {ResearchGate},
  year = {2020}
}

@InProceedings{Huang_2017_CVPR,
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
title = {Densely Connected Convolutional Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@inproceedings{10.1117/12.2549103,
author = {Tomoki Uemura and Janne J. Näppi and Toru Hironaka and Hyoungseop Kim and Hiroyuki Yoshida},
title = {{Comparative performance of 3D-DenseNet, 3D-ResNet, and 3D-VGG models in polyp detection for CT colonography}},
volume = {11314},
booktitle = {Medical Imaging 2020: Computer-Aided Diagnosis},
editor = {Horst K. Hahn and Maciej A. Mazurowski},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {736 -- 741},
keywords = {3D-DenseNet, Deep Learning, CT Colonography, Colorectal Cancer},
year = {2020},
doi = {10.1117/12.2549103},
URL = {https://doi.org/10.1117/12.2549103}
}

@ARTICLE{8515234,
  author={J. {Dolz} and K. {Gopinath} and J. {Yuan} and H. {Lombaert} and C. {Desrosiers} and I. {Ben Ayed}},
  journal={IEEE Transactions on Medical Imaging}, 
  title={HyperDense-Net: A Hyper-Densely Connected CNN for Multi-Modal Image Segmentation}, 
  year={2019},
  volume={38},
  number={5},
  pages={1116-1126},
  doi={10.1109/TMI.2018.2878669}}

@InProceedings{10.1007/978-3-319-24574-4_28,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}

@InProceedings{10.1007/978-3-030-46640-4_23,
author="Cheng, Xinchao
and Jiang, Zongkang
and Sun, Qiule
and Zhang, Jianxin",
editor="Crimi, Alessandro
and Bakas, Spyridon",
title="Memory-Efficient Cascade 3D U-Net for Brain Tumor Segmentation",
booktitle="Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="242--253",
abstract="Segmentation is a routine and crucial procedure for the treatment of brain tumors. Deep learning based brain tumor segmentation methods have achieved promising performance in recent years. However, to pursue high segmentation accuracy, most of them require too much memory and computation resources. Motivated by a recently proposed partially reversible U-Net architecture that pays more attention to memory footprint, we further present a novel Memory-Efficient Cascade 3D U-Net (MECU-Net) for brain tumor segmentation in this work, which can achieve comparable segmentation accuracy with less memory and computation consumption. More specifically, MECU-Net utilizes fewer down-sampling channels to reduce the utilization of memory and computation resources. To make up the accuracy loss, MECU-Net employs multi-scale feature fusion module to enhance the feature representation capability. Additionally, a light-weight cascade model, which resolves the problem of small target segmentation accuracy caused by model compression to some extent, is further introduced into the segmentation network. Finally, edge loss and weighted dice loss are combined to refine the brain tumor segmentation results. Experiment results on BraTS 2019 validation set illuminate that MECU-Net can achieve average Dice coefficients of 0.902, 0.824 and 0.777 on the whole tumor, tumor core and enhancing tumor, respectively.",
isbn="978-3-030-46640-4"
}

@inproceedings{Kocielnik:2019:YAI:3290605.3300641,
 author = {Kocielnik, Rafal and Amershi, Saleema and Bennett, Paul N.},
 title = {Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End-user Expectations of AI Systems},
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '19},
 year = {2019},
 isbn = {978-1-4503-5970-2},
 location = {Glasgow, Scotland Uk},
 pages = {411:1--411:14},
 articleno = {411},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3290605.3300641},
 doi = {10.1145/3290605.3300641},
 acmid = {3300641},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ai infused systems, ai system on-boarding, perception and acceptance of ai, shaping ai expectations},
}

@article{DALILA2017749,
title = "Segmentation and classification of melanoma and benign skin lesions",
journal = "Optik",
volume = "140",
pages = "749 - 761",
year = "2017",
issn = "0030-4026",
doi = "https://doi.org/10.1016/j.ijleo.2017.04.084",
url = "http://www.sciencedirect.com/science/article/pii/S0030402617304886",
author = "Fekrache Dalila and Ameur Zohra and Kasmi Reda and Cherifi Hocine",
keywords = "Computer-aided diagnosis, Melanoma, Segmentation, Ant colony, Feature extraction, Dermoscopy, K-Nearest Neighbor, Neural network",
abstract = "The incidence ofmalignant melanoma has been increasing worldwide. An efficient non-invasive computer-aided diagnosis (CAD) is seen as a solution to make identification process faster, and accessible to a large population. Such automated system relies on three things: reliable lesion segmentation, pertinent features’ extraction and good lesion classifier. In this paper, we propose an automated system that uses an Ant colony based segmentation algorithm, takes into consideration three types of features to describe malignant lesion:geometrical properties, textureand relative colors from which pertinent ones are selected, and uses two classifiers K-Nearest Neighbor (KNN) and Artificial Neural Network (ANN). The objective of this paper is to test the efficiency of the proposed segmentation algorithm, extract most pertinent features that describe melanomas and compare the two classifiers. Our automated system is tested on 172 dermoscopic images where 88 are malignant melanomas and 84 benign lesions. The results of the proposed segmentation algorithm are encouraging as they gave promising results. 12 features seem to be sufficient to detect malignant melanoma. Moreover, ANN gives better results than KNN."
}

@inproceedings{pacheco2019alignment,
  title={Alignment of Player and Non-Player Character Assertiveness Levels},
  author={Pacheco, Ant{\'o}nio C and Martinho, Carlos},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume={15},
  pages={181--187},
  year={2019},
  publisher={AAAI},
  address={Georgia Institute of Technology, Atlanta, Georgia, USA}
}

@inproceedings{10.1145/3311350.3347162,
author = {Paradeda, Raul and Ferreira, Maria Jos\'{e} and Oliveira, Raquel and Martinho, Carlos and Paiva, Ana},
title = {The Role of Assertiveness in a Storytelling Game with Persuasive Robotic Non-Player Characters},
year = {2019},
isbn = {9781450366885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311350.3347162},
doi = {10.1145/3311350.3347162},
booktitle = {Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
pages = {453–465},
numpages = {13},
keywords = {interactive digital storytelling, assertiveness, persuasion, personality trait, social robotics, non-player character},
location = {Barcelona, Spain},
series = {CHI PLAY ’19}
}

@article{Lambin2017,
author={Lambin, Philippe
and Leijenaar, Ralph T.H.
and Deist, Timo M.
and Peerlings, Jurgen
and de Jong, Evelyn E.C.
and van Timmeren, Janita
and Sanduleanu, Sebastian
and Larue, Ruben T.H.M.
and Even, Aniek J.G.
and Jochems, Arthur
and van Wijk, Yvonka
and Woodruff, Henry
and van Soest, Johan
and Lustberg, Tim
and Roelofs, Erik
and van Elmpt, Wouter
and Dekker, Andre
and Mottaghy, Felix M.
and Wildberger, Joachim E.
and Walsh, Sean},
title={Radiomics: the bridge between medical imaging and personalized medicine},
journal={Nature Reviews Clinical Oncology},
year={2017},
month={Dec},
day={01},
volume={14},
number={12},
pages={749-762},
abstract={Radiomics is becoming increasingly more important in medical imagingThe explosion of medical imaging data creates an environment ideal for machine-learning and data-based scienceRadiomics-based decision-support systems for precision diagnosis and treatment can be a powerful tool in modern medicineLarge-scale data sharing is necessary for the validation and full potential that radiomics representsStandardized data collection, evaluation criteria, and reporting guidelines are required for radiomics to mature as a discipline},
issn={1759-4782},
doi={10.1038/nrclinonc.2017.141},
url={https://doi.org/10.1038/nrclinonc.2017.141}
}

@Article{doi:10.1148/radiol.2015151169,
  author   = {Gillies, Robert J. and Kinahan, Paul E. and Hricak, Hedvig},
  title    = {Radiomics: Images Are More than Pictures, They Are Data},
  doi      = {10.1148/radiol.2015151169},
  eprint   = {https://doi.org/10.1148/radiol.2015151169},
  note     = {PMID: 26579733},
  number   = {2},
  pages    = {563-577},
  url      = {https://doi.org/10.1148/radiol.2015151169},
  volume   = {278},
  abstract = {In the past decade, the field of medical image analysis has grown exponentially, with an increased number of pattern recognition tools and an increase in data set sizes. These advances have facilitated the development of processes for high-throughput extraction of quantitative features that result in the conversion of images into mineable data and the subsequent analysis of these data for decision support; this practice is termed radiomics. This is in contrast to the traditional practice of treating medical images as pictures intended solely for visual interpretation. Radiomic data contain first-, second-, and higher-order statistics. These data are combined with other patient data and are mined with sophisticated bioinformatics tools to develop models that may potentially improve diagnostic, prognostic, and predictive accuracy. Because radiomics analyses are intended to be conducted with standard of care images, it is conceivable that conversion of digital images to mineable data will eventually become routine practice. This report describes the process of radiomics, its challenges, and its potential power to facilitate better clinical decision making, particularly in the care of patients with cancer.},
  journal  = {Radiology},
  year     = {2016},
}

@article{cheung2017integral,
  title={Integral Multimodality Imaging in Breast Cancer Diagnosis},
  author={Cheung, Yun-Chung},
  journal={Ultrasound in Medicine and Biology},
  volume={43},
  pages={S17},
  year={2017},
  publisher={Elsevier}
}

@article{MIAO201817,
title = "Extraction of BI-RADS findings from breast ultrasound reports in Chinese using deep learning approaches",
journal = "International Journal of Medical Informatics",
volume = "119",
pages = "17 - 21",
year = "2018",
issn = "1386-5056",
doi = "https://doi.org/10.1016/j.ijmedinf.2018.08.009",
url = "http://www.sciencedirect.com/science/article/pii/S1386505618309225",
author = "Shumei Miao and Tingyu Xu and Yonghui Wu and Hui Xie and Jingqi Wang and Shenqi Jing and Yaoyun Zhang and Xiaoliang Zhang and Yinshuang Yang and Xin Zhang and Tao Shan and Li Wang and Hua Xu and Shui Wang and Yun Liu",
keywords = "Named entity recognition, Clinical natural language processing, Deep learning",
abstract = "Background
The wide adoption of electronic health record systems (EHRs) in hospitals in China has made large amounts of data available for clinical research including breast cancer. Unfortunately, much of detailed clinical information is embedded in clinical narratives e.g., breast radiology reports. The American College of Radiology (ACR) has developed a Breast Imaging Reporting and Data System (BI-RADS) to standardize the clinical findings from breast radiology reports.
Objectives
This study aims to develop natural language processing (NLP) methods to extract BI-RADS findings from breast ultrasound reports in Chinese, thus to support clinical operation and breast cancer research in China.
Methods
We developed and compared three different types of NLP approaches, including a rule-based method, a traditional machine learning-based method using the Conditional Random Fields (CRF) algorithm, and deep learning-based approaches, to extract all BI-RADS finding categories from breast ultrasound reports in Chinese.
Results
Using a manually annotated dataset containing 540 reports, our evaluation shows that the deep learning-based method achieved the best F1-score of 0.904, when compared with rule-based and CRF-based approaches (0.848 and 0.881 respectively).
Conclusions
This is the first study that applies deep learning technologies to BI-RADS findings extraction in Chinese breast ultrasound reports, demonstrating its potential on enabling international collaborations on breast cancer research."
}

@article{10.1117/1.JBO.22.4.046008,
author = {Bernhard B. Zimmermann and Bin Deng and Bhawana Singh and Mark Martino and Juliette J. Selb and Qianqian Fang and Amir Y. Sajjadi and Jayne A. Cormier and Richard H. Moore and Daniel B. Kopans and David A. Boas and Mansi A. Saksena and Stefan A. Carp},
title = {{Multimodal breast cancer imaging using coregistered dynamic diffuse optical tomography and digital breast tomosynthesis}},
volume = {22},
journal = {Journal of Biomedical Optics},
number = {4},
publisher = {SPIE},
pages = {1 -- 10},
keywords = {diffuse optical tomography, dynamic imaging, breast cancer, digital breast tomosynthesis, multimodal, optical mammography, Digital breast tomosynthesis, Breast, Sensors, Optical fibers, Absorption, X-rays, Breast cancer, Mammography, Tumors, Imaging systems},
year = {2017},
doi = {10.1117/1.JBO.22.4.046008},
URL = {https://doi.org/10.1117/1.JBO.22.4.046008}
}

@article{10.1093/jbi/wbaa010,
    author = {Baird, Grayson L and Dibble, Elizabeth H and Mainiero, Martha B and Miles, Randy C and Lourenco, Ana P},
    title = "{Dense Breast Notification Letters: What Do Breast Radiologists Think?}",
    journal = {Journal of Breast Imaging},
    volume = {2},
    number = {3},
    pages = {225-231},
    year = {2020},
    month = {04},
    abstract = "{The Food and Drug Administration is currently creating national standards for language used in letters sent to women after mammography concerning dense breasts. The purpose of the current study is to survey breast radiologists on their opinions about language to be included in dense breast notification (DBN) letters.An anonymous survey (17 questions and 10 open-ended response fields) was sent to Society of Breast Imaging members between May 2019 and June 2019. Analyses were conducted using a chi-square test and the generalized linear model.A total of 262 surveys were completed (25\\% response rate). The majority of breast radiologists believe letters should be sent to patients (91\\%), with most (66\\%) believing that patients should receive DBN letters regardless of having dense breasts or not. The majority of breast radiologists believe DBNs should be sent to referring physicians (69\\%), include statements that define masking (89\\%), inform patients that dense breasts are associated with cancer risk (77\\%), inform patients about the possible benefits of supplemental screening (86\\%), be written at the sixth- or eighth-grade reading level (92\\%), and should be provided in other languages in addition to English (89\\%); half of the respondents (51\\%) believe the letters should contain BI-RADS density descriptors.There is consensus that patients and referring physicians should receive DBN letters and that those letters should address masking, increased cancer risk, and supplemental screening. Respondents believe the letters should be written at a sixth- or eighth-grade reading level.}",
    issn = {2631-6110},
    doi = {10.1093/jbi/wbaa010},
    url = {https://doi.org/10.1093/jbi/wbaa010},
    eprint = {https://academic.oup.com/jbi/article-pdf/2/3/225/33502034/wbaa010.pdf},
}

@article{SHAN2016980,
title = "Computer-Aided Diagnosis for Breast Ultrasound Using Computerized BI-RADS Features and Machine Learning Methods",
journal = "Ultrasound in Medicine \& Biology",
volume = "42",
number = "4",
pages = "980 - 988",
year = "2016",
issn = "0301-5629",
doi = "https://doi.org/10.1016/j.ultrasmedbio.2015.11.016",
url = "http://www.sciencedirect.com/science/article/pii/S0301562915006808",
author = "Juan Shan and S. Kaisar Alam and Brian Garra and Yingtao Zhang and Tahira Ahmed",
keywords = "Breast cancer, Computer-aided diagnosis, Computerized features, Breast Imaging Reporting and Data System, BI-RADS, Machine learning, Receiver operating characteristic, Tissue characterization, Tumor classification, Ultrasonic imaging",
abstract = "This work identifies effective computable features from the Breast Imaging Reporting and Data System (BI-RADS), to develop a computer-aided diagnosis (CAD) system for breast ultrasound. Computerized features corresponding to ultrasound BI-RADs categories were designed and tested using a database of 283 pathology-proven benign and malignant lesions. Features were selected based on classification performance using a “bottom-up” approach for different machine learning methods, including decision tree, artificial neural network, random forest and support vector machine. Using 10-fold cross-validation on the database of 283 cases, the highest area under the receiver operating characteristic (ROC) curve (AUC) was 0.84 from a support vector machine with 77.7% overall accuracy; the highest overall accuracy, 78.5%, was from a random forest with the AUC 0.83. Lesion margin and orientation were optimum features common to all of the different machine learning methods. These features can be used in CAD systems to help distinguish benign from worrisome lesions."
}

@article{pesapane2018artificial,
  title={Artificial intelligence in medical imaging: threat or opportunity? Radiologists again at the forefront of innovation in medicine},
  author={Pesapane, Filippo and Codari, Marina and Sardanelli, Francesco},
  journal={European radiology experimental},
  volume={2},
  number={1},
  pages={35},
  year={2018},
  publisher={Springer}
}

@article{HANNA20181709,
title = "The Effects of Fatigue From Overnight Shifts on Radiology Search Patterns and Diagnostic Performance",
journal = "Journal of the American College of Radiology",
volume = "15",
number = "12",
pages = "1709 - 1716",
year = "2018",
issn = "1546-1440",
doi = "https://doi.org/10.1016/j.jacr.2017.12.019",
url = "http://www.sciencedirect.com/science/article/pii/S1546144017316617",
author = "Tarek N. Hanna and Matthew E. Zygmont and Ryan Peterson and David Theriot and Haris Shekhani and Jamlik-Omari Johnson and Elizabeth A. Krupinski",
keywords = "Fatigue, medical error, radiology, overnight, search pattern",
abstract = "Purpose
The aim of this study was to assess the effect of overnight shifts (ONS) on radiologist fatigue, visual search pattern, and diagnostic performance.
Methods
This experimental study was approved by the institutional review board. Twelve radiologists (five faculty members and seven residents) each completed two sessions: one during a normal workday (“not fatigued”) and another in the morning after an ONS (“fatigued”). Each radiologist completed the Swedish Occupational Fatigue Inventory. During each session, radiologists viewed 20 bone radiographs consisting of normal and abnormal findings. Viewing time, diagnostic confidence, and eye-tracking data were recorded.
Results
Swedish Occupational Fatigue Inventory results demonstrated worsening in all five variables (lack of energy, physical exertion, physical discomfort, lack of motivation, and sleepiness) after ONS (P < .01). Overall, participants demonstrated worse diagnostic performance in the fatigued versus not fatigued state (P < .05). Total viewing time per case was longer when fatigued (35.9 ± 25.8 seconds) than not fatigued (24.8 ± 16.3 seconds) (P < .0001). Total viewing time per case was longer for residents (P < .05). Mean total fixations generated during the search increased by 60% during fatigued sessions (P < .0001). Mean time to first fixate on the fracture increased by 34% during fatigued sessions (P < .0001) and was longer for residents (P < .01). Dwell times associated with true- and false-positive decisions increased, whereas those with false negatives decreased.
Conclusions
After ONS, radiologists were more fatigued with worse diagnostic performance, a 45% increase in view time per case, a 60% increase in total gaze fixations, and a 34% increase in time to fixate on the fracture. The effects of fatigue were more pronounced in residents."
}

@article{stec2018systematic,
  title={A systematic review of fatigue in radiology: is it a problem?},
  author={Stec, Nadia and Arje, Danielle and Moody, Alan R and Krupinski, Elizabeth A and Tyrrell, Pascal N},
  journal={American Journal of Roentgenology},
  volume={210},
  number={4},
  pages={799--806},
  year={2018},
  publisher={Am Roentgen Ray Soc}
}

@article{waite2017systemic,
  title={Systemic error in radiology},
  author={Waite, Stephen and Scott, Jinel Moore and Legasto, Alan and Kolla, Srinivas and Gale, Brian and Krupinski, Elizabeth A},
  journal={American Journal of Roentgenology},
  volume={209},
  number={3},
  pages={629--639},
  year={2017},
  publisher={Am Roentgen Ray Soc}
}

@article{d2018breast,
  title={Breast imaging reporting and data system (BI-RADS)},
  author={D'Orsi, Carl and Bassett, L and Feig, S and others},
  journal={Breast Imaging. Lee CI, Lehman CD, Bassett LW (ed): Oxford University Press, New York},
  year={2018}
}

@misc{nadia2020maivelt,
  doi = {10.13140/RG.2.2.33693.26086},
  url = {http://rgdoi.net/10.13140/RG.2.2.33693.26086},
  author = {Mour\~{a}o, N\'{a}dia Liladar and Calisto,  Francisco Maria  and Nascimento,  Jacinto C.},
  language = {en},
  title = {MIMBCD-UI: AI Visual Explaination - Lesions Types},
  publisher = {ResearchGate},
  year = {2020}
}

@misc{nadia2020maivect,
  doi = {10.13140/RG.2.2.26667.80164},
  url = {http://rgdoi.net/10.13140/RG.2.2.26667.80164},
  author = {Mour\~{a}o, N\'{a}dia Liladar and  Calisto,  Francisco Maria and Nascimento,  Jacinto C.},
  language = {en},
  title = {MIMBCD-UI: AI Visual Explanation - Calcification Types},
  publisher = {ResearchGate},
  year = {2020}
}

@Article{doi:10.1148/radiol.2018181371,
  author   = {Rodríguez-Ruiz, Alejandro and Krupinski, Elizabeth and Mordang, Jan-Jurre and Schilling, Kathy and Heywang-Köbrunner, Sylvia H. and Sechopoulos, Ioannis and Mann, Ritse M.},
  title    = {Detection of Breast Cancer with Mammography: Effect of an Artificial Intelligence Support System},
  doi      = {10.1148/radiol.2018181371},
  eprint   = {https://doi.org/10.1148/radiol.2018181371},
  note     = {PMID: 30457482},
  number   = {2},
  pages    = {305-314},
  url      = {https://doi.org/10.1148/radiol.2018181371},
  volume   = {290},
  abstract = {PurposeTo compare breast cancer detection performance of radiologists reading mammographic examinations unaided versus supported by an artificial intelligence (AI) system.Materials and MethodsAn enriched retrospective, fully crossed, multireader, multicase, HIPAA-compliant study was performed. Screening digital mammographic examinations from 240 women (median age, 62 years; range, 39–89 years) performed between 2013 and 2017 were included. The 240 examinations (100 showing cancers, 40 leading to false-positive recalls, 100 normal) were interpreted by 14 Mammography Quality Standards Act–qualified radiologists, once with and once without AI support. The readers provided a Breast Imaging Reporting and Data System score and probability of malignancy. AI support provided radiologists with interactive decision support (clicking on a breast region yields a local cancer likelihood score), traditional lesion markers for computer-detected abnormalities, and an examination-based cancer likelihood score. The area under the receiver operating characteristic curve (AUC), specificity and sensitivity, and reading time were compared between conditions by using mixed-models analysis dof variance and generalized linear models for multiple repeated measurements.ResultsOn average, the AUC was higher with AI support than with unaided reading (0.89 vs 0.87, respectively; P = .002). Sensitivity increased with AI support (86\% [86 of 100] vs 83\% [83 of 100]; P = .046), whereas specificity trended toward improvement (79\% [111 of 140]) vs 77\% [108 of 140]; P = .06). Reading time per case was similar (unaided, 146 seconds; supported by AI, 149 seconds; P = .15). The AUC with the AI system alone was similar to the average AUC of the radiologists (0.89 vs 0.87).ConclusionRadiologists improved their cancer detection at mammography when using an artificial intelligence system for support, without requiring additional reading time.Published under a CC BY 4.0 license.See also the editorial by Bahl in this issue.},
  journal  = {Radiology},
  year     = {2019},
}

@ARTICLE{8611096,
  author={Q. {Huang} and Y. {Chen} and L. {Liu} and D. {Tao} and X. {Li}},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={On Combining Biclustering Mining and AdaBoost for Breast Tumor Classification}, 
  year={2020},
  volume={32},
  number={4},
  pages={728-738},
  doi={10.1109/TKDE.2019.2891622}
}

@INPROCEEDINGS{9231684,
  author={B. {Jaafar} and H. {Mahersia} and Z. {Lachiri}},
  booktitle={2020 5th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, 
  title={A survey on deep learning techniques used for breast cancer detection}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ATSIP49331.2020.9231684}
}

@techreport{nadia2020xai,
  doi         = {10.13140/RG.2.2.31605.93928/4},
  url         = {http://rgdoi.net/10.13140/RG.2.2.31605.93928/4},
  author      = {{N\'{a}dia Mour\~{a}o}},
  language    = {en},
  title       = {Master Project: 2D Breast Cancer Diagnosis Explainable Visualizations},
  institution = {Instituto Superior T\'{e}cnico},
  address     = {Avenida Rovisco Pais 1, 1049-001 Lisboa - Portugal (EU)},
  month       = {1},
  year        = {2020},
  publisher   = {ResearchGate},
  note        = {Introduction of explainable methods on medical decision making.}
}

@article{https://doi.org/10.1002/cncr.32910,
author = {Mutebi, Miriam and Anderson, Benjamin O. and Duggan, Catherine and Adebamowo, Clement and Agarwal, Gaurav and Ali, Zipporah and Bird, Peter and Bourque, Jean-Marc and DeBoer, Rebecca and Gebrim, Luiz Henrique and Masetti, Riccardo and Masood, Shahla and Menon, Manoj and Nakigudde, Gertrude and Ng’ang’a, Anne and Niyonzima, Nixon and Rositch, Anne F. and Unger-Saldaña, Karla and Villarreal-Garza, Cynthia and Dvaladze, Allison and El Saghir, Nagi S. and Gralow, Julie R. and Eniu, Alexandru},
title = {Breast cancer treatment: A phased approach to implementation},
journal = {Cancer},
volume = {126},
number = {S10},
pages = {2365-2378},
keywords = {breast cancer, cancer center of excellence, centralized care, decentralized care, dissemination and implementation science, early diagnosis, health disparities, low- and middle-income countries (LMICs), multidisciplinary evaluation, phased implementation, resource-stratification, supportive and palliative care, treatment, underserved communities},
doi = {https://doi.org/10.1002/cncr.32910},
url = {https://acsjournals.onlinelibrary.wiley.com/doi/abs/10.1002/cncr.32910},
eprint = {https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.1002/cncr.32910},
abstract = {Optimal treatment outcomes for breast cancer are dependent on a timely diagnosis followed by an organized, multidisciplinary approach to care. However, in many low- and middle-income countries, effective care management pathways can be difficult to follow because of financial constraints, a lack of resources, an insufficiently trained workforce, and/or poor infrastructure. On the basis of prior work by the Breast Health Global Initiative, this article proposes a phased implementation strategy for developing sustainable approaches to enhancing patient care in limited-resource settings by creating roadmaps that are individualized and adapted to the baseline environment. This strategy proposes that, after a situational analysis, implementation phases begin with bolstering palliative care capacity, especially in settings where a late-stage diagnosis is common. This is followed by strengthening the patient pathway, with consideration given to a dynamic balance between centralization of services into centers of excellence to achieve better quality and decentralization of services to increase patient access. The use of resource checklists ensures that comprehensive therapy or palliative care can be delivered safely and effectively. Episodic or continuous monitoring with established process and quality metrics facilitates ongoing assessment, which should drive continual process improvements. A series of case studies provides a snapshot of country experiences with enhancing patient care, including the implementation of national cancer control plans in Kenya, palliative care in Romania, the introduction of a 1-stop clinic for diagnosis in Brazil, the surgical management of breast cancer in India, and the establishment of a women's cancer center in Ghana.},
year = {2020}
}

@article{DANA2020541,
title = "Multimodality Imaging and Artificial Intelligence for Tumor Characterization: Current Status and Future Perspective",
journal = "Seminars in Nuclear Medicine",
volume = "50",
number = "6",
pages = "541 - 548",
year = "2020",
note = "Imaging Biomarkers for Therapy Assessment of Malignant Disease",
issn = "0001-2998",
doi = "https://doi.org/10.1053/j.semnuclmed.2020.07.003",
url = "http://www.sciencedirect.com/science/article/pii/S000129982030074X",
author = "Jérémy Dana and Vincent Agnus and Farid Ouhmich and Benoit Gallix",
abstract = "Research in medical imaging has yet to do to achieve precision oncology. Over the past 30 years, only the simplest imaging biomarkers (RECIST, SUV,…) have become widespread clinical tools. This may be due to our inability to accurately characterize tumors and monitor intratumoral changes in imaging. Artificial intelligence, through machine learning and deep learning, opens a new path in medical research because it can bring together a large amount of heterogeneous data into the same analysis to reach a single outcome. Supervised or unsupervised learning may lead to new paradigms by identifying unrevealed structural patterns across data. Deep learning will provide human-free, undefined upstream, reproducible, and automated quantitative imaging biomarkers. Since tumor phenotype is driven by its genotype and thus indirectly defines tumoral progression, tumor characterization using machine learning and deep learning algorithms will allow us to monitor molecular expression noninvasively, anticipate therapeutic failure, and lead therapeutic management. To follow this path, quality standards have to be set: standardization of imaging acquisition as it has been done in the field of biology, transparency of the model development as it should be reproducible by different institutions, validation, and testing through a high-quality process using large and complex open databases and better interpretability of these algorithms."
}

@article{DIROBERTO2016950,
title = "Improving the Transcription of Patient Information From Image Requisitions to the Radiology Information System",
journal = "Journal of the American College of Radiology",
volume = "13",
number = "8",
pages = "950 - 955",
year = "2016",
issn = "1546-1440",
doi = "https://doi.org/10.1016/j.jacr.2016.03.030",
url = "http://www.sciencedirect.com/science/article/pii/S1546144016301740",
author = "Cole DiRoberto and Crystal Lehto and Steven J. Baccei",
keywords = "Patient information, image requisition, transcription, RIS, radiology information system, transcribing information, improving, improvement, improving communication, communication, technologists, indication, clinical order, RIS indication, CPOE, computerized physician order entry",
abstract = "Purpose
The purpose of this study was to improve the transcription of patient information from imaging study requisitions to the radiology information database at a single institution.
Methods
Five hundred radiology reports from adult outpatient radiographic examinations were chosen randomly from the radiology information system (RIS) and categorized according to their degree of concordance with their corresponding clinical order indications. The number and types of grammatical errors and types of order forms were also recorded. Countermeasures centered on the education of the technical staff and referring physician offices and the implementation of a checklist. Another sample of 500 reports was taken after the implementation of the countermeasures and compared with the baseline data using a χ2 test.
Results
The number of RIS indications perfectly concordant with their corresponding clinical order indications increased from 232 (46.4%) to 314 (62.8%) after the implementation of the countermeasures (P < .0001). The number of partially concordant matches due to inadequate RIS indications dropped from 162 (32.4%) to 114 (22.8%) (P < .001), whereas the number of partially concordant matches due to inadequate clinical order indications increased from 22 (4.4%) to 57 (11.4%) (P < .0001). The number of discordant pairings dropped from 84 (16.8%) to 15 (3%) (P < .0001). Technologists began to input additional patient information obtained from the patients (not present in the image requisitions) in the RIS after the implementation of the countermeasures.
Conclusions
The education of technical staff members and the implementation of a checklist markedly improved the information provided to radiologists on image requisitions from referring providers."
}

@article{islam2018recent,
  title={Recent advancement of clinical information systems: Opportunities and challenges},
  author={Islam, Md Mohaimenul and Poly, Tahmina Nasrin and Li, Yu-Chuan Jack},
  journal={Yearbook of medical informatics},
  volume={27},
  number={1},
  pages={83},
  year={2018},
  publisher={Thieme Medical Publishers}
}

@INPROCEEDINGS{8621479,
  author={D. {Chen} and G. {Qian} and Q. {Pan}},
  booktitle={2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Breast Cancer Classification with Electronic Medical Records Using Hierarchical Attention Bidirectional Networks}, 
  year={2018},
  volume={},
  number={},
  pages={983-988},
  doi={10.1109/BIBM.2018.8621479}
}

@article{GIBSON2018113,
title = "NiftyNet: a deep-learning platform for medical imaging",
journal = "Computer Methods and Programs in Biomedicine",
volume = "158",
pages = "113 - 122",
year = "2018",
issn = "0169-2607",
doi = "https://doi.org/10.1016/j.cmpb.2018.01.025",
url = "http://www.sciencedirect.com/science/article/pii/S0169260717311823",
author = "Eli Gibson and Wenqi Li and Carole Sudre and Lucas Fidon and Dzhoshkun I. Shakir and Guotai Wang and Zach Eaton-Rosen and Robert Gray and Tom Doel and Yipeng Hu and Tom Whyntie and Parashkev Nachev and Marc Modat and Dean C. Barratt and Sébastien Ourselin and M. Jorge Cardoso and Tom Vercauteren",
keywords = "Medical image analysis, Deep learning, Convolutional neural network, Segmentation, Image regression, Generative adversarial network",
abstract = "Background and objectives
Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort. Consequently, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon.
Methods
The NiftyNet infrastructure provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such as TensorBoard visualization of 2D and 3D images and computational graphs by default.
Results
We present three illustrative medical image analysis applications built using NiftyNet infrastructure: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses.
Conclusions
The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications."
}

@article{https://doi.org/10.1002/cncr.32872,
author = {Anglade, Fabienne and Milner Jr, Danny A. and Brock, Jane E.},
title = {Can pathology diagnostic services for cancer be stratified and serve global health?},
journal = {Cancer},
volume = {126},
number = {S10},
pages = {2431-2438},
keywords = {breast cancer, diagnostics, guidelines, pathology, stratification, tiered},
doi = {https://doi.org/10.1002/cncr.32872},
url = {https://acsjournals.onlinelibrary.wiley.com/doi/abs/10.1002/cncr.32872},
eprint = {https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.1002/cncr.32872},
abstract = {Background Before initiating cancer therapy, a diagnostic tumor tissue sample evaluated within a pathology laboratory by a pathologist is essential to confirm the malignancy type and provide key prognostic factors that direct the treatment offered. Methods Pathology evaluation includes multiple expensive reagents, complex equipment, and both laboratory and pathologist technical skills. By using breast cancer as an example, at a minimum, key tumor prognostic information required before the initiation of treatment includes subtype, tumor grade, tumor size, lymph node status when possible, and biomarker expression determined by immunohistochemistry for estrogen receptor. The additional determination of biomarker expression of progesterone receptor and human epidermal growth factor receptor (HER2) is the standard of care in high-resource settings, but assays may not be affordable in low-income and middle-income countries. Results With positive tests, patients are eligible for either tamoxifen (for estrogen receptor-positive/progesterone receptor-positive cancers) or monoclonal antibody therapy (for HER2-positive cancers). For settings in which endocrine therapy and/or HER2-targeted therapy is unavailable, biomarker studies have no utility, and high-resource setting standards for pathology evaluation and reporting are unachievable. Resource-stratified pathology evaluation guidelines in cancer diagnosis have not been developed, in contrast to excellent comprehensive, resource-stratified clinical guidelines for use in low-income and middle-income countries, and these are long overdue. Conclusions The challenges of pathology evaluation in the context of global health are being met by innovative solutions, which may change the face of pathology practice.},
year = {2020}
}

@article{jiang2018interpretation,
  title={Interpretation time using a concurrent-read computer-aided detection system for automated breast ultrasound in breast cancer screening of women with dense breast tissue},
  author={Jiang, Yulei and Inciardi, Marc F and Edwards, Alexandra V and Papaioannou, John},
  journal={American Journal of Roentgenology},
  volume={211},
  number={2},
  pages={452--461},
  year={2018},
  publisher={Am Roentgen Ray Soc}
}

@article{bruno2015understanding,
  title={{Understanding and confronting our mistakes: the epidemiology of error in radiology and strategies for error reduction}},
  author={Bruno, Michael A and Walker, Eric A and Abujudeh, Hani H},
  journal={Radiographics},
  volume={35},
  number={6},
  pages={1668--1676},
  year={2015},
  publisher={RSNA}
}

@article{segrelles2017increasing,
  title={Increasing the efficiency on producing radiology reports for breast cancer diagnosis by means of structured reports},
  author={Segrelles Quilis, Jos{\'e} Dami{\'a}n and Medina, Rosana and Blanquer Espert, Ignacio and Marti Bonmati, Luis},
  journal={Methods of information in medicine},
  volume={56},
  pages={1--13},
  year={2017},
  publisher={Schattauer}
}

@article{SENG202079,
title = "Reporting and Abstracting Variability in Technical Standards for Breast Cancer Operations",
journal = "Journal of Surgical Research",
volume = "253",
pages = "79 - 85",
year = "2020",
issn = "0022-4804",
doi = "https://doi.org/10.1016/j.jss.2020.03.041",
url = "http://www.sciencedirect.com/science/article/pii/S0022480420301736",
author = "Sirivan S. Seng and Jenny H. Chang and June Yoo and Sara Grossi and Zachary Tran and Lucyna Kryzwon and Sasha Swensen and Eric {van Baarsel} and Jacqueline Chen and EunJee Lee and Ana Jacinto and Ross Mudgway and Jukes Namm and Sharon S. Lum",
keywords = "Breast cancer, Operative standards, Synoptic reporting",
abstract = "Background
The American College of Surgeons Commission on Cancer has incorporated documentation of critical elements outlined in Operative Standards for Cancer Surgery into revised standards for cancer center accreditation. This study assessed the current documentation of critical elements in partial mastectomy (PM) and sentinel lymph node biopsy (SLNB) operative reports.
Materials and methods
Operative reports for PM + SLNB at a single academic institution from 2013 to 2018 were reviewed for compliance and surveyor interobserver reliability with the Oncologic Elements of Operative Record defined in Operative Standards and compared with a nonredundant American Society of Breast Surgeons Mastery of Breast Surgery (MBS) quality measure for specimen orientation.
Results
Ten reviewers each evaluated 66 PM + SLNB operative reports for 13 Oncologic Elements and one MBS measure. No operative records reported all critical elements for PM + SLNB or PM alone. Residents completed 36.4% of operative reports: Element documentation was similar for PM but varied significantly for SLNB between resident and attending authorship. Combined reporting performance and interrater reliability varied across all elements and was highest for the use of SLNB tracer (97.1% and κ = 0.95, respectively) and lowest for intraoperative assessment of SLNB (30.6%, κ = 0.43). MBS specimen orientation had both high proportion reported (87.0%) and interrater reliability (κ = 0.84).
Conclusions
Adherence to reporting critical elements for PM and SLNB varied. Whether differential compliance was tied to discrepancies in documentation or reviewer abstraction, clarification of synoptic choices may improve reporting consistency. Evolving techniques or technologies will require continuous appraisal of mandated reporting for breast surgery."
}

@article{wagner2015analysis,
  title={{Analysis and classification of oncology activities on the way to workflow based single source documentation in clinical information systems}},
  author={Wagner, Stefan and Beckmann, Matthias W and Wullich, Bernd and Seggewies, Christof and Ries, Markus and B{\"u}rkle, Thomas and Prokosch, Hans-Ulrich},
  journal={Medical Informatics and Decision Making},
  volume={15},
  number={107},
  pages={1-13},
  year={2015},
  publisher={BMC}
}

@article{van2020radiomics,
  title={Radiomics in medical imaging—“how-to” guide and critical reflection},
  author={van Timmeren, Janita E and Cester, Davide and Tanadini-Lang, Stephanie and Alkadhi, Hatem and Baessler, Bettina},
  journal={Insights into Imaging},
  volume={11},
  number={1},
  pages={1--16},
  year={2020},
  publisher={SpringerOpen}
}

@article{valdora2018rapid,
  title={Rapid review: radiomics and breast cancer},
  author={Valdora, Francesca and Houssami, Nehmat and Rossi, Federica and Calabrese, Massimo and Tagliafico, Alberto Stefano},
  journal={Breast cancer research and treatment},
  volume={169},
  number={2},
  pages={217--229},
  year={2018},
  publisher={Springer}
}

@InProceedings{10.1007/978-3-030-59716-0_71,
author="Gammulle, Harshala
and Denman, Simon
and Sridharan, Sridha
and Fookes, Clinton",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="742--751",
abstract="Automating the analysis of imagery of the Gastrointestinal (GI) tract captured during endoscopy procedures has substantial potential benefits for patients, as it can provide diagnostic support to medical practitioners and reduce mistakes via human error. To further the development of such methods, we propose a two-stream model for endoscopic image analysis. Our model fuses two streams of deep feature inputs by mapping their inherent relations through a novel relational network model, to better model symptoms and classify the image. In contrast to handcrafted feature-based models, our proposed network is able to learn features automatically and outperforms existing state-of-the-art methods on two public datasets: KVASIR and Nerthus. Our extensive evaluations illustrate the importance of having two streams of inputs instead of a single stream and also demonstrates the merits of the proposed relational network architecture to combine those streams.",
isbn="978-3-030-59716-0"
}

@Article{doi:10.1148/radiol.2020192039,
  author   = {Kim, Jin You and Kim, Jin Joo and Hwangbo, Lee and Suh, Hie Bum and Kim, Suk and Choo, Ki Seok and Nam, Kyung Jin and Kang, Taewoo},
  title    = {Kinetic Heterogeneity of Breast Cancer Determined Using Computer-aided Diagnosis of Preoperative MRI Scans: Relationship to Distant Metastasis-Free Survival},
  doi      = {10.1148/radiol.2020192039},
  eprint   = {https://doi.org/10.1148/radiol.2020192039},
  note     = {PMID: 32228293},
  number   = {3},
  pages    = {517-526},
  url      = {https://doi.org/10.1148/radiol.2020192039},
  volume   = {295},
  abstract = {Background Higher peak enhancement and washout component values measured on preoperative breast MRI scans with computer-aided diagnosis (CAD) are presumed to be associated with worse recurrence-free survival. Purpose To investigate whether CAD-extracted kinetic features of breast cancer and the heterogeneity of these features at preoperative MRI are associated with distant metastasis-free survival in women with invasive breast cancer. Materials and Methods Consecutive women with newly diagnosed invasive breast cancer who underwent preoperative MRI were retrospectively evaluated between 2011 and 2012. A commercially available CAD system was used to extract the peak enhancement and delayed enhancement profiles of each breast cancer case from preoperative MRI data. The kinetic heterogeneity of these features (a measure of heterogeneity in the proportions of tumor pixels with delayed washout, plateau, and persistent components within a tumor) was calculated to evaluate intratumoral heterogeneity. Cox proportional hazards models were used to investigate the associations between CAD-extracted kinetic features and distant metastasis-free survival after adjusting for clinical-pathologic factors. Results A total of 276 consecutive women (mean age, 53 years) were evaluated. In 28 of 276 (10.1\%) women, distant metastasis developed at a median follow-up of 79 months. A higher degree of kinetic heterogeneity was observed in women with distant metastases than in those without distant metastases (mean, 0.70 ± 0.2 vs 0.43 ± 0.3; P < .001). Multivariable Cox proportional hazards analysis revealed that a higher degree of kinetic heterogeneity (hazard ratio [HR], 19.2; 95\% confidence interval [CI]: 4.2, 87.1; P < .001), higher peak enhancement (HR, 1.001; 95\% CI: 1.000, 1.002; P = .045), the presence of lymphovascular invasion (HR, 3.3; 95\% CI: 1.5, 7.5; P = .004), and a higher histologic grade (ie, grade 3) (HR, 2.2; 95\% CI: 1.0, 4.9; P = .044) were associated with worse distant metastasis-free survival. Conclusion Higher values of kinetic heterogeneity and peak enhancement as determined with computer-aided diagnosis of preoperative MRI were associated with worse distant metastasis-free survival in women with invasive breast cancer. © RSNA, 2020 See also the editorial by El Khouli and Jacobs in this issue.},
  journal  = {Radiology},
  year     = {2020},
}

@inproceedings{Cai:2019:HTC:3290605.3300234,
 author = {Cai, Carrie J. and Reif, Emily and Hegde, Narayan and Hipp, Jason and Kim, Been and Smilkov, Daniel and Wattenberg, Martin and Viegas, Fernanda and Corrado, Greg S. and Stumpe, Martin C. and Terry, Michael},
 title = {Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making},
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '19},
 year = {2019},
 isbn = {978-1-4503-5970-2},
 location = {Glasgow, Scotland Uk},
 pages = {4:1--4:14},
 articleno = {4},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3290605.3300234},
 doi = {10.1145/3290605.3300234},
 acmid = {3300234},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clinical health, human-ai interaction, machine learning},
}

@article{delvaux2017effects,
  title={The effects of computerized clinical decision support systems on laboratory test ordering: a systematic review},
  author={Delvaux, Nicolas and Van Thienen, Katrien and Heselmans, Annemie and de Velde, Stijn Van and Ramaekers, Dirk and Aertgeerts, Bert},
  journal={Archives of pathology \& laboratory medicine},
  volume={141},
  number={4},
  pages={585--595},
  year={2017},
  publisher={the College of American Pathologists}
}

@article{middleton2016clinical,
  title={Clinical decision support: a 25 year retrospective and a 25 year vision},
  author={Middleton, B and Sittig, DF and Wright, A},
  journal={Yearbook of medical informatics},
  volume={25},
  number={S 01},
  pages={S103--S116},
  year={2016},
  publisher={Georg Thieme Verlag KG}
}

@article{khairat2018reasons,
  title={Reasons For Physicians Not Adopting Clinical Decision Support Systems: Critical Analysis},
  author={Khairat, Saif and Marc, David and Crosby, William and Al Sanousi, Ali},
  journal={JMIR medical informatics},
  volume={6},
  number={2},
  pages={e24},
  year={2018},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@article{kohli2018cad,
  title={Why CAD failed in mammography},
  author={Kohli, Ajay and Jha, Saurabh},
  journal={Journal of the American College of Radiology},
  volume={15},
  number={3},
  pages={535--537},
  year={2018},
  publisher={Elsevier}
}

@article{litjens2017survey,
  title={A survey on deep learning in medical image analysis},
  author={Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and Van Der Laak, Jeroen Awm and Van Ginneken, Bram and S{\'a}nchez, Clara I},
  journal={Medical image analysis},
  volume={42},
  pages={60--88},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{10.1145/3132272.3134111,
author = {Calisto, Francisco M. and Ferreira, Alfredo and Nascimento, Jacinto C. and Gon\c{c}alves, Daniel},
title = {Towards Touch-Based Medical Image Diagnosis Annotation},
year = {2017},
isbn = {9781450346917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132272.3134111},
doi = {10.1145/3132272.3134111},
abstract = {A fundamental step in medical diagnosis for patient follow-up relies on the ability of radiologists to perform a trusty diagnostic from acquired images. Basically, the diagnosis strongly depends on the visual inspection over the shape of the lesions. As datasets increase in size, such visual evaluation becomes harder. For this reason, it is crucial to introduce easy-to-use interfaces that help the radiologists to perform a reliable visual inspection and allow the efficient delineation of the lesions. We will explore the radiologist's receptivity to the current touch environment solution. The advantages of touch are threefold: (i) the time performance is superior regarding the traditional use, (ii) it has more intuitive control and, (iii) for less time, the user interface delivers more information per action, concerning annotations. From our studies, we conclude that the radiologists still exhibit a resistance to change from traditional to touch based interfaces in current clinical setups.},
booktitle = {Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces},
pages = {390–395},
numpages = {6},
keywords = {Medical Image Diagnosis, Human-Computer Interaction, Medical Visualization, Interaction Design, Touch-Based},
location = {Brighton, United Kingdom},
series = {ISS '17}
}

@misc{calisto2019mips,
  doi = {10.13140/RG.2.2.19014.32321},
  url = {http://rgdoi.net/10.13140/RG.2.2.19014.32321},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Medical Imaging Patient Structure},
  publisher = {ResearchGate},
  year = {2019}
}

@Inbook{Trivedi2019,
author="Trivedi, Deven N.
and Shah, Nimit D.
and Kothari, Ashish M.
and Thanki, Rohit M.",
title="DICOM® Medical Image Standard",
bookTitle="Dental Image Processing for Human Identification",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="41--49",
abstract="This chapter gives information on Digital Imaging and Communications in Medicine (DICOM), a medical image standard, and about DICOM dental images, which are used to identify a person.",
isbn="978-3-319-99471-0",
doi="10.1007/978-3-319-99471-0_4",
url="https://doi.org/10.1007/978-3-319-99471-0_4"
}

@book{carter2018digital,
  title={Digital Radiography and PACS E-Book},
  author={Carter, Christi and Veale, Beth},
  year={2018},
  publisher={Elsevier Health Sciences}
}

@article{faraji2019radiologic,
  title={Radiologic Modalities and Response Assessment Schemes for Clinical and Preclinical Oncology Imaging},
  author={Faraji, Farshid and Gaba, Ron C},
  journal={Frontiers in Oncology},
  volume={9},
  year={2019},
  publisher={Frontiers Media SA}
}

@inproceedings{seifabadi2019correlation,
  title={Correlation of ultrasound tomography to MRI and pathology for the detection of prostate cancer},
  author={Seifabadi, Reza and Cheng, Alexis and Malik, Bilal and Kishimoto, Shun and Wiskin, James and Munasinghe, Jeeva and Negussie, Ayele H and Bakhutashvili, Ivane and Krishna, Murali C and Choyke, Peter and others},
  booktitle={Medical Imaging 2019: Ultrasonic Imaging and Tomography},
  volume={10955},
  pages={109550C},
  year={2019},
  organization={International Society for Optics and Photonics}
}

@inproceedings{Igarashi:2016:IVS:2984511.2984537,
 author={Igarashi, Takeo and Shono, Naoyuki and Kin, Taichi and Saito, Toki},
 title={{Interactive Volume Segmentation with Threshold Field Painting}},
 booktitle={Annual Symposium on User Interface Software and Technology (UIST)},
 year={2016},
 pages={403--413},
 doi={10.1145/2984511.2984537},
 publisher={ACM},
 address={New York, NY, USA},
}

@inproceedings{Ocegueda-Hernandez:2016:CMN:2876456.2879485,
 author={Ocegueda-Hern\'{a}ndez, Vladimir and Mendizabal-Ruiz, Gerardo},
 title={{Computational Methods for the Natural and Intuitive Visualization of Volumetric Medical Data}},
 booktitle={Int'l Conf. Intelligent User Interfaces (IUI)},
 year={2016},
 pages={54--57},
 doi={10.1145/2876456.2879485},
 publisher={ACM},
 address={New York, NY, USA},
}

@inproceedings{Sousa:2017:VVR:3025453.3025566,
 author={Sousa, Maur\'{\i}cio and Mendes, Daniel and Paulo, Soraia and Matela, Nuno and Jorge, Joaquim and Lopes, Daniel Sim\~{o}es},
 title={{VRRRRoom: Virtual Reality for Radiologists in the Reading Room}},
 booktitle={Conf. Human Factors in Computing Systems (CHI)},
 year={2017},
 location={Denver, Colorado, USA},
 pages={4057--4062},
 doi={10.1145/3025453.3025566},
 publisher={ACM},
 address={New York, NY, USA},
}

@article{Lopes:2017:UHC:3143820.3144118,
 author = {Lopes, Daniel Simes and Parreira, Pedro Duarte de Figueiredo and Paulo, Soraia Figueiredo and Nunes, Vitor and Rego, Paulo Amaral and Neves, Manuel Cassiano and Rodrigues, Pedro Silva and Jorge, Joaquim Armando},
 title = {On the Utility of 3D Hand Cursors to Explore Medical Volume Datasets with a Touchless Interface},
 journal = {J. of Biomedical Informatics},
 issue_date = {August 2017},
 volume = {72},
 number = {C},
 month = aug,
 year = {2017},
 issn = {1532-0464},
 pages = {140--149},
 numpages = {10},
 url = {https://doi.org/10.1016/j.jbi.2017.07.009},
 doi = {10.1016/j.jbi.2017.07.009},
 acmid = {3144118},
 publisher = {Elsevier Science},
 address = {San Diego, USA},
 keywords = {2Dtwo-dimensional, 3D hand cursor, 3Dthree-dimensional, Bi-manual gestures, CTcomputed tomography, DICOMdigital imaging and communications in medicine, GUIgraphical user interface, LANlocal wireless network, MRImagnetic resonance imaging, Medical volume data, Mmean, SDstandard deviation, Touchless interface, USBuniversal serial bus, User study, VEVoxel Explorer, VolVolview, WIMPwindows-icons-menus-pointers},
}

@inproceedings{lopes2018interaction,
  title={Interaction techniques for immersive ct colonography: A professional assessment},
  author={Lopes, Daniel Simoes and Medeiros, Daniel and Paulo, Soraia Figueiredo and Borges, Pedro Brasil and Nunes, Vitor and Mascarenhas, Vasco and Veiga, Marcos and Jorge, Joaquim Armando},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={629--637},
  year={2018},
  organization={Springer}
}

@book{edge2019clinical,
  title={Clinical Decision Support Systems for Appropriate Medical Imaging: Clinical Evidence and Cost-Effectiveness},
  author={Edge, R and Ford, C},
  year={2019},
  publisher={Canadian Agency for Drugs and Technologies in Health}
}

@inproceedings{Cai:2019:EEE:3301275.3302289,
 author = {Cai, Carrie J. and Jongejan, Jonas and Holbrook, Jess},
 title = {The Effects of Example-based Explanations in a Machine Learning Interface},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 series = {IUI '19},
 year = {2019},
 isbn = {978-1-4503-6272-6},
 location = {Marina del Ray, California},
 pages = {258--262},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3301275.3302289},
 doi = {10.1145/3301275.3302289},
 acmid = {3302289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {example-based explanations, explainable AI, human-AI interaction, machine learning},
}

@inproceedings{yang2016investigating,
  title={Investigating the heart pump implant decision process: opportunities for decision support tools to help},
  author={Yang, Qian and Zimmerman, John and Steinfeld, Aaron and Carey, Lisa and Antaki, James F},
  booktitle={Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
  pages={4477--4488},
  year={2016},
  organization={ACM}
}

@article{hwang2019artificial,
  title={Artificial intelligence-based decision-making for age-related macular degeneration},
  author={Hwang, De-Kuang and Hsu, Chih-Chien and Chang, Kao-Jung and Chao, Daniel and Sun, Chuan-Hu and Jheng, Ying-Chun and Yarmishyn, Aliaksandr A and Wu, Jau-Ching and Tsai, Ching-Yao and Wang, Mong-Lien and others},
  journal={Theranostics},
  volume={9},
  number={1},
  pages={232},
  year={2019},
  publisher={Ivyspring International Publisher}
}

@article{gagnon2014electronic,
  title={Electronic health record acceptance by physicians: testing an integrated theoretical model},
  author={Gagnon, Marie-Pierre and Ghandour, El Kebir and Talla, Pascaline Kengne and Simonyan, David and Godin, Gaston and Labrecque, Michel and Ouimet, Mathieu and Rousseau, Michel},
  journal={Journal of biomedical informatics},
  volume={48},
  pages={17--27},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{meacham2019towards,
  title={Towards Explainable AI: Design and Development for Explanation of Machine Learning Predictions for a Patient Readmittance Medical Application},
  author={Meacham, Sofia and Isaac, Georgia and Nauck, Detlef and Virginas, Botond},
  booktitle={Intelligent Computing-Proceedings of the Computing Conference},
  pages={939--955},
  year={2019},
  organization={Springer}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{esteva2017dermatologist,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={Nature},
  volume={542},
  number={7639},
  pages={115},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{esteva2019guide,
  title={A guide to deep learning in healthcare},
  author={Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={24},
  year={2019},
  publisher={Nature Publishing Group}
}

@inproceedings{medley2019segmenting,
  title={Segmenting The Left Ventricle In Cardiac In Cardiac MRI: From Handcrafted To Deep Region Based Descriptors},
  author={Medley, Daniela O and Santiago, Carlos and Nascimento, Jacinto C},
  booktitle={2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)},
  pages={644--648},
  year={2019},
  organization={IEEE}
}

@article{cole2017predicting,
  title={Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker},
  author={Cole, James H and Poudel, Rudra PK and Tsagkrasoulis, Dimosthenis and Caan, Matthan WA and Steves, Claire and Spector, Tim D and Montana, Giovanni},
  journal={NeuroImage},
  volume={163},
  pages={115--124},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{gonzalez2018deep,
  title={Deep learning for biomarker regression: application to osteoporosis and emphysema on chest CT scans},
  author={Gonz{\'a}lez, Germ{\'a}n and Washko, George R and Est{\'e}par, Ra{\'u}l San Jos{\'e}},
  booktitle={Medical Imaging 2018: Image Processing},
  volume={10574},
  pages={105741H},
  year={2018},
  organization={International Society for Optics and Photonics}
}

@article{wang2019deep,
  title={Deep learning provides a new computed tomography-based prognostic biomarker for recurrence prediction in high-grade serous ovarian cancer},
  author={Wang, Shuo and Liu, Zhenyu and Rong, Yu and Zhou, Bin and Bai, Yan and Wei, Wei and Wang, Meiyun and Guo, Yingkun and Tian, Jie},
  journal={Radiotherapy and Oncology},
  volume={132},
  pages={171--177},
  year={2019},
  publisher={Elsevier}
}

@article{holzinger2019causability,
  title={Causability and explainabilty of artificial intelligence in medicine},
  author={Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"u}ller, Heimo},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  pages={e1312},
  year={2019},
  publisher={Wiley Online Library}
}

@article{gunning2017explainable,
  title={Explainable artificial intelligence (xai)},
  author={Gunning, David},
  journal={Defense Advanced Research Projects Agency (DARPA), nd Web},
  year={2017}
}

@article{miller2018explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial Intelligence},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{holzinger2018current,
  title={Current advances, trends and challenges of machine learning and knowledge extraction: From machine learning to explainable ai},
  author={Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A Min},
  booktitle={International Cross-Domain Conference for Machine Learning and Knowledge Extraction},
  pages={1--8},
  year={2018},
  organization={Springer}
}

@inproceedings{Bharadhwaj:2019:ERS:3308557.3308699,
 author = {Bharadhwaj, Homanga},
 title = {Explainable Recommender System That Maximizes Exploration},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
 series = {IUI '19},
 year = {2019},
 isbn = {978-1-4503-6673-1},
 location = {Marina del Ray, California},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3308557.3308699},
 doi = {10.1145/3308557.3308699},
 acmid = {3308699},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RNN, explainable AI, recommender systems},
}

@inproceedings{Dominguez:2019:EEA:3301275.3302274,
 author = {Dominguez, Vicente and Messina, Pablo and Donoso-Guzm\'{a}n, Ivania and Parra, Denis},
 title = {The Effect of Explanations and Algorithmic Accuracy on Visual Recommender Systems of Artistic Images},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 series = {IUI '19},
 year = {2019},
 isbn = {978-1-4503-6272-6},
 location = {Marina del Ray, California},
 pages = {408--416},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3301275.3302274},
 doi = {10.1145/3301275.3302274},
 acmid = {3302274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {art, explainable AI, visual recommender systems},
}

@inproceedings{Weisz:2019:BTS:3301275.3302290,
 author = {Weisz, Justin D. and Jain, Mohit and Joshi, Narendra Nath and Johnson, James and Lange, Ingrid},
 title = {BigBlueBot: Teaching Strategies for Successful Human-agent Interactions},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 series = {IUI '19},
 year = {2019},
 isbn = {978-1-4503-6272-6},
 location = {Marina del Ray, California},
 pages = {448--459},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3301275.3302290},
 doi = {10.1145/3301275.3302290},
 acmid = {3302290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conversational agents, explainable AI, mechanical turk},
}

@article{shah2019artificial,
  title={Artificial intelligence and machine learning in clinical development: a translational perspective},
  author={Shah, Pratik and Kendall, Francis and Khozin, Sean and Goosen, Ryan and Hu, Jianying and Laramie, Jason and Ringel, Michael and Schork, Nicholas},
  journal={NPJ digital medicine},
  volume={2},
  number={1},
  pages={69},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{10.1145/3359206,
author = {Cai, Carrie J. and Winter, Samantha and Steiner, David and Wilcox, Lauren and Terry, Michael},
title = {"Hello AI": Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359206},
doi = {10.1145/3359206},
abstract = {Although rapid advances in machine learning have made it increasingly applicable to expert decision-making, the delivery of accurate algorithmic predictions alone is insufficient for effective human-AI collaboration. In this work, we investigate the key types of information medical experts desire when they are first introduced to a diagnostic AI assistant. In a qualitative lab study, we interviewed 21 pathologists before, during, and after being presented deep neural network (DNN) predictions for prostate cancer diagnosis, to learn the types of information that they desired about the AI assistant. Our findings reveal that, far beyond understanding the local, case-specific reasoning behind any model decision, clinicians desired upfront information about basic, global properties of the model, such as its known strengths and limitations, its subjective point-of-view, and its overall design objective--what it's designed to be optimized for. Participants compared these information needs to the collaborative mental models they develop of their medical colleagues when seeking a second opinion: the medical perspectives and standards that those colleagues embody, and the compatibility of those perspectives with their own diagnostic patterns. These findings broaden and enrich discussions surrounding AI transparency for collaborative decision-making, providing a richer understanding of what experts find important in their introduction to AI assistants before integrating them into routine practice.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {104},
numpages = {24},
keywords = {human-ai interaction, clinical health, machine learning}
}

@InProceedings{10.1007/978-3-030-50334-5_4,
author="Meske, Christian
and Bunde, Enrico",
editor="Degen, Helmut
and Reinerman-Jones, Lauren",
title="Transparency and Trust in Human-AI-Interaction: The Role of Model-Agnostic Explanations in Computer Vision-Based Decision Support",
booktitle="Artificial Intelligence in HCI",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="54--69",
abstract="Computer Vision, and hence Artificial Intelligence-based extraction of information from images, has increasingly received attention over the last years, for instance in medical diagnostics. While the algorithms' complexity is a reason for their increased performance, it also leads to the `black box' problem, consequently decreasing trust towards AI. In this regard, ``Explainable Artificial Intelligence'' (XAI) allows to open that black box and to improve the degree of AI transparency. In this paper, we first discuss the theoretical impact of explainability on trust towards AI, followed by showcasing how the usage of XAI in a health-related setting can look like. More specifically, we show how XAI can be applied to understand why Computer Vision, based on deep learning, did or did not detect a disease (malaria) on image data (thin blood smear slide images). Furthermore, we investigate, how XAI can be used to compare the detection strategy of two different deep learning models often used for Computer Vision: Convolutional Neural Network and Multi-Layer Perceptron. Our empirical results show that i) the AI sometimes used questionable or irrelevant data features of an image to detect malaria (even if correctly predicted), and ii) that there may be significant discrepancies in how different deep learning models explain the same prediction. Our theoretical discussion highlights that XAI can support trust in Computer Vision systems, and AI systems in general, especially through an increased understandability and predictability.",
isbn="978-3-030-50334-5"
}

@inproceedings{10.1145/3306618.3314293,
author = {Teso, Stefano and Kersting, Kristian},
title = {Explanatory Interactive Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314293},
doi = {10.1145/3306618.3314293},
abstract = {Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {239–245},
numpages = {7},
keywords = {active learning, explainable artificial intelligence, interpretability, machine learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@INPROCEEDINGS{8851763,
  author={C. {Han} and W. {Yoon} and G. {Kwon} and D. {Kim} and S. {Nam}},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Representation of white- and black-box adversarial examples in deep neural networks and humans: A functional magnetic resonance imaging study}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN.2019.8851763}
}

@article{zednik2019solving,
  title={Solving the black box problem: A normative framework for explainable artificial intelligence},
  author={Zednik, Carlos},
  journal={Philosophy \& Technology},
  pages={1--24},
  year={2019},
  publisher={Springer}
}

@inproceedings{10.1145/3206505.3206555,
author = {Balducci, Fabrizio and Buono, Paolo},
title = {Building a Qualified Annotation Dataset for Skin Lesion Analysis Trough Gamification},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206555},
doi = {10.1145/3206505.3206555},
abstract = {The deep learning approach has increased the quality of automatic medical diagnoses at the cost of building qualified datasets to train and test such supervised machine learning methods. Image annotation is one of the main activity of dermatologists and the quality of annotation depends on the physician experience and on the number of studied cases: manual annotations are very useful to extract features like contours, intersections and shapes that can be used in the processes of lesion segmentation and classification made by automatic agents. This paper proposes the design of an interactive multimedia platform that enhance the annotation process of medical images, in the domain of dermatology, adopting gamification and "games with a purpose" (GWAP) strategies in order to improve the engagement and the production of qualified datasets also fostering their sharing and practical evaluation. A special attention is given to the design choices, theories and assumptions as well as the implementation and technological details.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {36},
numpages = {5},
keywords = {annotation, machine learning, dermatology, GWAP, gamification},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3399715.3399744,
author = {Calisto, Francisco Maria and Nunes, Nuno and Nascimento, Jacinto C.},
title = {BreastScreening: On the Use of Multi-Modality in Medical Imaging Diagnosis},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399744},
doi = {10.1145/3399715.3399744},
abstract = {This paper describes the field research, design and comparative deployment of a multimodal medical imaging user interface for breast screening. The main contributions described here are threefold: 1) The design of an advanced visual interface for multimodal diagnosis of breast cancer (BreastScreening); 2) Insights from the field comparison of Single-Modality vs Multi-Modality screening of breast cancer diagnosis with 31 clinicians and 566 images; and 3) The visualization of the two main types of breast lesions in the following image modalities: (i) MammoGraphy (MG) in both Craniocaudal (CC) and Mediolateral oblique (MLO) views; (ii) UltraSound (US); and (iii) Magnetic Resonance Imaging (MRI). We summarize our work with recommendations from the radiologists for guiding the future design of medical imaging interfaces.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {49},
numpages = {5},
keywords = {breast cancer, multimodality, annotations, medical imaging, human-computer interaction, healthcare systems, user-centered design},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3290605.3300468,
author = {Yang, Qian and Steinfeld, Aaron and Zimmerman, John},
title = {Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300468},
doi = {10.1145/3290605.3300468},
abstract = {Clinical decision support tools (DST) promise improved healthcare outcomes by offering data-driven insights. While effective in lab settings, almost all DSTs have failed in practice. Empirical research diagnosed poor contextual fit as the cause. This paper describes the design and field evaluation of a radically new form of DST. It automatically generates slides for clinicians' decision meetings with subtly embedded machine prognostics. This design took inspiration from the notion of Unremarkable Computing, that by augmenting the users' routines technology/AI can have significant importance for the users yet remain unobtrusive. Our field evaluation suggests clinicians are more likely to encounter and embrace such a DST. Drawing on their responses, we discuss the importance and intricacies of finding the right level of unremarkableness in DST design, and share lessons learned in prototyping critical AI systems as a situated experience.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {healthcare, decision support systems, user experience},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{https://doi.org/10.1002/mp.13562,
author = {Mahadevaiah, Geetha and RV, Prasad and Bermejo, Inigo and Jaffray, David and Dekker, Andre and Wee, Leonard},
title = {Artificial intelligence-based clinical decision support in modern medical physics: Selection, acceptance, commissioning, and quality assurance},
journal = {Medical Physics},
volume = {47},
number = {5},
pages = {e228-e235},
keywords = {artificial intelligence, clinical decision support, machine learning},
doi = {https://doi.org/10.1002/mp.13562},
url = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.13562},
eprint = {https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.13562},
abstract = {Background Recent advances in machine and deep learning based on an increased availability of clinical data have fueled renewed interest in computerized clinical decision support systems (CDSSs). CDSSs have shown great potential to improve healthcare, increase patient safety and reduce costs. However, the use of CDSSs is not without pitfalls, as an inadequate or faulty CDSS can potentially deteriorate the quality of healthcare and put patients at risk. In addition, the adoption of a CDSS might fail because its intended users ignore the output of the CDSS due to lack of trust, relevancy or actionability. Aim In this article, we provide guidance based on literature for the different aspects involved in the adoption of a CDSS with a special focus on machine and deep learning based systems: selection, acceptance testing, commissioning, implementation and quality assurance. Results A rigorous selection process will help identify the CDSS that best fits the preferences and requirements of the local site. Acceptance testing will make sure that the selected CDSS fulfills the defined specifications and satisfies the safety requirements. The commissioning process will prepare the CDSS for safe clinical use at the local site. An effective implementation phase should result in an orderly roll out of the CDSS to the well-trained end-users whose expectations have been managed. And finally, quality assurance will make sure that the performance of the CDSS is maintained and that any issues are promptly identified and solved. Conclusion We conclude that a systematic approach to the adoption of a CDSS will help avoid pitfalls, improve patient safety and increase the chances of success.},
year = {2020}
}

@inproceedings{dellermann2019future,
  title={The future of human-AI collaboration: a taxonomy of design knowledge for hybrid intelligence systems},
  author={Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and Weber, Thorsten and Weigel, Sascha and Ebel, Philipp},
  booktitle={Proceedings of the 52nd Hawaii International Conference on System Sciences},
  year={2019}
}

@article{leung2019health,
  title={E-health/m-health adoption and lifestyle improvements: Exploring the roles of technology readiness, the expectation-confirmation model, and health-related information activities},
  author={Leung, Louis and Chen, Cheng},
  journal={Telecommunications Policy},
  year={2019},
  publisher={Elsevier}
}

@article {Challen231,
	author = {Challen, Robert and Denny, Joshua and Pitt, Martin and Gompels, Luke and Edwards, Tom and Tsaneva-Atanasova, Krasimira},
	title = {Artificial intelligence, bias and clinical safety},
	volume = {28},
	number = {3},
	pages = {231--237},
	year = {2019},
	doi = {10.1136/bmjqs-2018-008370},
	publisher = {BMJ Publishing Group Ltd},
	issn = {2044-5415},
	URL = {https://qualitysafety.bmj.com/content/28/3/231},
	eprint = {https://qualitysafety.bmj.com/content/28/3/231.full.pdf},
	journal = {BMJ Quality \& Safety}
}

@inproceedings{10.1145/3351095.3375709,
author = {Yang, Kaiyu and Qinami, Klint and Fei-Fei, Li and Deng, Jia and Russakovsky, Olga},
title = {Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375709},
doi = {10.1145/3351095.3375709},
abstract = {Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {547–558},
numpages = {12},
keywords = {dataset construction, computer vision, representative datasets, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3287560.3287596,
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {fairness evaluation, datasheets, ethical considerations, documentation, ML model evaluation, disaggregated evaluation, model cards},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{10.1145/3290605.3300509,
author = {Yin, Ming and Wortman Vaughan, Jennifer and Wallach, Hanna},
title = {Understanding the Effect of Accuracy on Trust in Machine Learning Models},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300509},
doi = {10.1145/3290605.3300509},
abstract = {We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {trust, machine learning, human-subject experiments},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.5555/3305381.3305576,
author = {Koh, Pang Wei and Liang, Percy},
title = {Understanding Black-Box Predictions via Influence Functions},
year = {2017},
publisher = {JMLR.org},
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1885–1894},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.1145/2939672.2939778,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {black box classifier, interpretable machine learning, interpretability, explaining machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{Shakerin_Gupta_2019, title={Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4163}, DOI={10.1609/aaai.v33i01.33013052}, abstractNote={&lt;p&gt;We present a heuristic based algorithm to induce &lt;em&gt;nonmonotonic&lt;/em&gt; logic programs that will explain the behavior of XGBoost trained classifiers. We use the technique based on the LIME approach to locally select the most important features contributing to the classification decision. Then, in order to explain the model’s global behavior, we propose the LIME-FOLD algorithm —a heuristic-based inductive logic programming (ILP) algorithm capable of learning nonmonotonic logic programs—that we apply to a transformed dataset produced by LIME. Our proposed approach is agnostic to the choice of the ILP algorithm. Our experiments with UCI standard benchmarks suggest a significant improvement in terms of classification evaluation metrics. Meanwhile, the number of induced rules dramatically decreases compared to ALEPH, a state-of-the-art ILP system.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shakerin, Farhad and Gupta, Gopal}, year={2019}, month={Jul.}, pages={3052-3059} }

@inproceedings{10.1145/3329859.3329878,
author = {Di Cicco, Vincenzo and Firmani, Donatella and Koudas, Nick and Merialdo, Paolo and Srivastava, Divesh},
title = {Interpreting Deep Learning Models for Entity Resolution: An Experience Report Using LIME},
year = {2019},
isbn = {9781450368025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329859.3329878},
doi = {10.1145/3329859.3329878},
abstract = {Entity Resolution (ER) seeks to understand which records refer to the same entity (e.g., matching products sold on multiple websites). The sheer number of ways humans represent and misrepresent information about real-world entities makes ER a challenging problem. Deep Learning (DL) has provided impressive results in the field of natural language processing, thus recent works started exploring DL approaches to the ER problem, with encouraging results. However, we are still far from understanding why and when these approaches work in the ER setting. We are developing a methodology, Mojito, to produce explainable interpretations of the output of DL models for the ER task. Our methodology is based on LIME, a popular tool for producing prediction explanations for generic classification tasks. In this paper we report our first experiences in interpreting recent DL models for the ER task. Our results demonstrate the importance of explanations in the DL space, and suggest that, when assessing performance of DL algorithms for ER, accuracy alone may not be sufficient to demonstrate generality and reproducibility in a production environment.},
booktitle = {Proceedings of the Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {8},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {aiDM '19}
}

@InProceedings{pmlr-v80-kim18d, title = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})}, author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and sayres, Rory}, pages = {2668--2677}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, booktitle = {Proceedings of Machine Learning Research}, series = {PMLR '18}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/kim18d/kim18d.pdf}, url = {http://proceedings.mlr.press/v80/kim18d.html}, abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.} }

@inproceedings{10.1145/3173574.3174156,
author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
title = {Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174156},
doi = {10.1145/3173574.3174156},
abstract = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–18},
numpages = {18},
keywords = {explainable artificial intelli-gence, intelligibility, interpretable machine learning, explanations},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@Inbook{Crabtree2020,
author="Crabtree, Andy
and Tolmie, Peter
and Chamberlain, Alan",
editor="Chamberlain, Alan
and Crabtree, Andy",
title="``Research in the Wild'': Approaches to Understanding the Unremarkable as a Resource for Design",
bookTitle="Into the Wild: Beyond the Design Research Lab",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="31--53",
abstract="This chapter outlines some key approaches towards understanding the unremarkable. It focuses first on a sociological orientation to the everyday world as key to the enterprise, and then on a variety of complimentary approaches for elaborating or surfacing the unremarkable character of everyday life. It considers the kinds of data resources that are routinely used to elaborate the unremarkable, and the relationship between data resource and analysis as a constituent element of working `in the wild'. We hope this will be a valuable resource for researchers and students alike.",
isbn="978-3-030-18020-1",
doi="10.1007/978-3-030-18020-1_3",
url="https://doi.org/10.1007/978-3-030-18020-1_3"
}

@article{10.1145/3359178,
author = {Schaekermann, Mike and Beaton, Graeme and Habib, Minahz and Lim, Andrew and Larson, Kate and Law, Edith},
title = {Understanding Expert Disagreement in Medical Data Analysis through Structured Adjudication},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359178},
doi = {10.1145/3359178},
abstract = {Expert disagreement is pervasive in clinical decision making and collective adjudication is a useful approach for resolving divergent assessments. Prior work shows that expert disagreement can arise due to diverse factors including expert background, the quality and presentation of data, and guideline clarity. In this work, we study how these factors predict initial discrepancies in the context of medical time series analysis, examining why certain disagreements persist after adjudication, and how adjudication impacts clinical decisions. Results from a case study with 36 experts and 4,543 adjudicated cases in a sleep stage classification task show that these factors contribute to both initial disagreement and resolvability, each in their own unique way. We provide evidence suggesting that structured adjudication can lead to significant revisions in treatment-relevant clinical parameters. Our work demonstrates how structured adjudication can support consensus and facilitate a deep understanding of expert disagreement in medical data analysis.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {76},
numpages = {23},
keywords = {adjudication, ambiguity, disagreement, medical time series}
}

@article{Aroyo_Welty_2014, title={The Three Sides of CrowdTruth}, volume={1}, url={https://hcjournal.org/index.php/jhc/article/view/34}, DOI={10.15346/hc.v1i1.34}, abstractNote={&lt;p&gt;Crowdsourcing is often used to gather annotated data for training and evaluating computational systems that attempt to solve cognitive problems, such as understanding Natural Language sentences. Crowd workers are asked to perform semantic interpretation of sentences to establish a ground truth. This has always been done under the assumption that each task unit, e.g. each sentence, has a single correct interpretation that is contained in the ground truth. We have countered this assumption with CrowdTruth, and have shown that it can be better suited to tasks for which semantic interpretation is subjective. In this paper we investigate the dependence of worker metrics for detecting spam on the quality of sentences in the dataset, and the quality of the target semantics. We show that worker quality metrics can improve significantly when the quality of these other aspects of semantic interpretation are considered.&lt;/p&gt;}, number={1}, journal={Human Computation}, author={Aroyo, Lora and Welty, Chris}, year={2014}, month={Sep.} }

@Misc{SchaekermannMike2020,
author={{Schaekermann, Mike}},
title={Human-AI Interaction in the Presence of Ambiguity: From Deliberation-based Labeling to Ambiguity-aware AI},
year={2020},
publisher="UWSpace",
url={http://hdl.handle.net/10012/16284}
}

@article{10.1167/tvst.8.6.40,
    author = {Schaekermann, Mike and Hammel, Naama and Terry, Michael and Ali, Tayyeba K. and Liu, Yun and Basham, Brian and Campana, Bilson and Chen, William and Ji, Xiang and Krause, Jonathan and Corrado, Greg S. and Peng, Lily and Webster, Dale R. and Law, Edith and Sayres, Rory},
    title = "{Remote Tool-Based Adjudication for Grading Diabetic Retinopathy}",
    journal = {Translational Vision Science \& Technology},
    volume = {8},
    number = {6},
    pages = {40-40},
    year = {2019},
    month = {12},
    abstract = "{   To present and evaluate a remote, tool-based system and structured grading rubric for adjudicating image-based diabetic retinopathy (DR) grades.    We compared three different procedures for adjudicating DR severity assessments among retina specialist panels, including (1) in-person adjudication based on a previously described procedure (Baseline), (2) remote, tool-based adjudication for assessing DR severity alone (TA), and (3) remote, tool-based adjudication using a feature-based rubric (TA-F). We developed a system allowing graders to review images remotely and asynchronously. For both TA and TA-F approaches, images with disagreement were reviewed by all graders in a round-robin fashion until disagreements were resolved. Five panels of three retina specialists each adjudicated a set of 499 retinal fundus images (1 panel using Baseline, 2 using TA, and 2 using TA-F adjudication). Reliability was measured as grade agreement among the panels using Cohen's quadratically weighted kappa. Efficiency was measured as the number of rounds needed to reach a consensus for tool-based adjudication.    The grades from remote, tool-based adjudication showed high agreement with the Baseline procedure, with Cohen's kappa scores of 0.948 and 0.943 for the two TA panels, and 0.921 and 0.963 for the two TA-F panels. Cases adjudicated using TA-F were resolved in fewer rounds compared with TA (P \\&lt; 0.001; standard permutation test).    Remote, tool-based adjudication presents a flexible and reliable alternative to in-person adjudication for DR diagnosis. Feature-based rubrics can help accelerate consensus for tool-based adjudication of DR without compromising label quality.    This approach can generate reference standards to validate automated methods, and resolve ambiguous diagnoses by integrating into existing telemedical workflows.  }",
    issn = {2164-2591},
    doi = {10.1167/tvst.8.6.40},
    url = {https://doi.org/10.1167/tvst.8.6.40},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/tvst/938258/i2164-2591-8-6-40.pdf},
}

@article{MIRANDA2015334,
title = "Computer-aided diagnosis system based on fuzzy logic for breast cancer categorization",
journal = "Computers in Biology and Medicine",
volume = "64",
pages = "334 - 346",
year = "2015",
issn = "0010-4825",
doi = "https://doi.org/10.1016/j.compbiomed.2014.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S0010482514002704",
author = "Gisele Helena Barboni Miranda and Joaquim Cezar Felipe",
keywords = "Computer-aided diagnosis, Breast cancer, Fuzzy logic, Decision support, Healthcare informatics",
abstract = "Background
Fuzzy logic can help reduce the difficulties faced by computational systems to represent and simulate the reasoning and the style adopted by radiologists in the process of medical image analysis. The study described in this paper consists of a new method that applies fuzzy logic concepts to improve the representation of features related to image description in order to make it semantically more consistent. Specifically, we have developed a computer-aided diagnosis tool for automatic BI-RADS categorization of breast lesions. The user provides parameters such as contour, shape and density and the system gives a suggestion about the BI-RADS classification.
Methods
Initially, values of malignancy were defined for each image descriptor, according to the BI-RADS standard. When analyzing contour, for example, our method considers the matching of features and linguistic variables. Next, we created the fuzzy inference system. The generation of membership functions was carried out by the Fuzzy Omega algorithm, which is based on the statistical analysis of the dataset. This algorithm maps the distribution of different classes in a set.
Results
Images were analyzed by a group of physicians and the resulting evaluations were submitted to the Fuzzy Omega algorithm. The results were compared, achieving an accuracy of 76.67% for nodules and 83.34% for calcifications.
Conclusions
The fit of definitions and linguistic rules to numerical models provided by our method can lead to a tighter connection between the specialist and the computer system, yielding more effective and reliable results."
}

@article{NIAZI2019e253,
title = "Digital pathology and artificial intelligence",
journal = "The Lancet Oncology",
volume = "20",
number = "5",
pages = "e253 - e261",
year = "2019",
issn = "1470-2045",
doi = "https://doi.org/10.1016/S1470-2045(19)30154-8",
url = "http://www.sciencedirect.com/science/article/pii/S1470204519301548",
author = "Muhammad Khalid Khan Niazi and Anil V Parwani and Metin N Gurcan",
abstract = "Summary
In modern clinical practice, digital pathology has a crucial role and is increasingly a technological requirement in the scientific laboratory environment. The advent of whole-slide imaging, availability of faster networks, and cheaper storage solutions has made it easier for pathologists to manage digital slide images and share them for clinical use. In parallel, unprecedented advances in machine learning have enabled the synergy of artificial intelligence and digital pathology, which offers image-based diagnosis possibilities that were once limited only to radiology and cardiology. Integration of digital slides into the pathology workflow, advanced algorithms, and computer-aided diagnostic techniques extend the frontiers of the pathologist's view beyond a microscopic slide and enable true utilisation and integration of knowledge that is beyond human limits and boundaries, and we believe there is clear potential for artificial intelligence breakthroughs in the pathology setting. In this Review, we discuss advancements in digital slide-based image diagnosis for cancer along with some challenges and opportunities for artificial intelligence in digital pathology."
}

@article{granzier2020mri,
  title={MRI-based radiomics in breast cancer: feature robustness with respect to inter-observer segmentation variability},
  author={Granzier, RWY and Verbakel, NMH and Ibrahim, A and van Timmeren, JE and van Nijnatten, TJA and Leijenaar, RTH and Lobbes, MBI and Smidt, ML and Woodruff, HC},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--11},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{DEMCHIG201962,
title = "Observer Variability in Breast Cancer Diagnosis between Countries with and without Breast Screening",
journal = "Academic Radiology",
volume = "26",
number = "1",
pages = "62 - 68",
year = "2019",
issn = "1076-6332",
doi = "https://doi.org/10.1016/j.acra.2018.03.003",
url = "http://www.sciencedirect.com/science/article/pii/S1076633218301156",
author = "Delgermaa Demchig and Claudia Mello-Thoms and Warwick Lee and Khulan Khurelsukh and Asai Ramish and Patrick Brennan",
keywords = "Breast cancer, mammographic screening, reader performance, observer variability, developing country",
abstract = "Rational and Objectives
Image reporting is a vital component of patient management depending on individual radiologists' performance. Our objective was to explore mammographic diagnostic efficacy in a country where breast cancer screening does not exist.
Materials and Methods
Two mammographic test sets were used: a typical screening (TS) and high-difficulty (HD) test set. Nonscreening (NS) radiologists (n = 11) read both test sets, while 52 and 49 screening radiologists read the TS and HD test sets, respectively. The screening radiologists were classified into two groups: a less experienced (LE) group with ≤5 years' experience and a more experienced (ME) group with ≥5 years' experience. A Kruskal–Wallis and Tukey–Kramer post hoc test were used to compare reading performance among reader groups, and the Wilcoxon matched pairs tests was used to compare TS and ND test sets for the NS radiologists.
Results
Across the three reader groups, there were significant differences in case sensitivity (χ2 [2] = 9.4, P = .008), specificity (χ2 [2] = 10.3, P = .006), location sensitivity (χ2 [2] = 19.8, P < .001), receiver operating characteristics, area under the curve (χ2 [2] = 19.7, P < .001) and jack-knife free-response receiver operating characteristics (JAFROCs) (χ2 [2] = 18.1, P < .001). NS performance for all measured scores was significantly lower than those for the ME readers (P < .006), while only location sensitivity was lower (χ2 [2] = 17.5, P = .026) for the NS compared to the LE group. No other significant differences were observed.
Conclusion
Large variations in mammographic performance exist between radiologists from screening and nonscreening countries."
}

@misc{raghu2019direct,
      title={Direct Uncertainty Prediction for Medical Second Opinions}, 
      author={Maithra Raghu and Katy Blumer and Rory Sayres and Ziad Obermeyer and Robert Kleinberg and Sendhil Mullainathan and Jon Kleinberg},
      year={2019},
      eprint={1807.01771},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{TANNO2021117366,
title = "Uncertainty modelling in deep learning for safer neuroimage enhancement: Demonstration in diffusion MRI",
journal = "NeuroImage",
volume = "225",
pages = "117366",
year = "2021",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2020.117366",
url = "http://www.sciencedirect.com/science/article/pii/S1053811920308521",
author = "Ryutaro Tanno and Daniel E. Worrall and Enrico Kaden and Aurobrata Ghosh and Francesco Grussu and Alberto Bizzi and Stamatios N. Sotiropoulos and Antonio Criminisi and Daniel C. Alexander",
keywords = "Uncertainty quantification, Deep learning, Safety, Robustness, Interpretability, Super-resolution, Image enhancement, Image synthesis, Neuroimaging, Diffusion MRI, Tractography",
abstract = "Deep learning (DL) has shown great potential in medical image enhancement problems, such as super-resolution or image synthesis. However, to date, most existing approaches are based on deterministic models, neglecting the presence of different sources of uncertainty in such problems. Here we introduce methods to characterise different components of uncertainty, and demonstrate the ideas using diffusion MRI super-resolution. Specifically, we propose to account for intrinsic uncertainty through a heteroscedastic noise model and for parameter uncertainty through approximate Bayesian inference, and integrate the two to quantify predictive uncertainty over the output image. Moreover, we introduce a method to propagate the predictive uncertainty on a multi-channelled image to derived scalar parameters, and separately quantify the effects of intrinsic and parameter uncertainty therein. The methods are evaluated for super-resolution of two different signal representations of diffusion MR images—Diffusion Tensor images and Mean Apparent Propagator MRI—and their derived quantities such as mean diffusivity and fractional anisotropy, on multiple datasets of both healthy and pathological human brains. Results highlight three key potential benefits of modelling uncertainty for improving the safety of DL-based image enhancement systems. Firstly, modelling uncertainty improves the predictive performance even when test data departs from training data (“out-of-distribution” datasets). Secondly, the predictive uncertainty highly correlates with reconstruction errors, and is therefore capable of detecting predictive “failures”. Results on both healthy subjects and patients with brain glioma or multiple sclerosis demonstrate that such an uncertainty measure enables subject-specific and voxel-wise risk assessment of the super-resolved images that can be accounted for in subsequent analysis. Thirdly, we show that the method for decomposing predictive uncertainty into its independent sources provides high-level “explanations” for the model performance by separately quantifying how much uncertainty arises from the inherent difficulty of the task or the limited training examples. The introduced concepts of uncertainty modelling extend naturally to many other imaging modalities and data enhancement applications."
}

@article{10.1145/3152889,
author = {Dumitrache, Anca and Aroyo, Lora and Welty, Chris},
title = {Crowdsourcing Ground Truth for Medical Relation Extraction},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3152889},
doi = {10.1145/3152889},
abstract = {Cognitive computing systems require human labeled data for evaluation and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, which reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, which account for ambiguity in both human and machine performance on this task.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {11},
numpages = {20},
keywords = {natural language ambiguity, Ground truth, relation extraction, clinical natural language processing, crowd truth, crowdtruth, inter-annotator disagreement}
}

@inproceedings{schaekermann2018expert,
  title={Expert Disagreement in Sequential Labeling: A Case Study on Adjudication in Medical Time Series Analysis.},
  author={Schaekermann, Mike and Law, Edith and Larson, Kate and Lim, Andrew},
  booktitle={SAD/CrowdBias@ HCOMP},
  pages={55--66},
  year={2018}
}

@inproceedings{10.1145/3308560.3317085,
author = {Schaekermann, Mike and Beaton, Graeme and Habib, Minahz and Lim, Andrew and Larson, Kate and Law, Edith},
title = {Capturing Expert Arguments from Medical Adjudication Discussions in a Machine-Readable Format},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317085},
doi = {10.1145/3308560.3317085},
abstract = {Group-based discussion among human graders can be a useful tool to capture sources of disagreement in ambiguous classification tasks and to adjudicate any resolvable disagreements. Existing workflows for panel-based adjudication, however, capture graders’ arguments and rationales in a free-form, unstructured format, limiting the potential for automatic analysis of the discussion contents. We designed and implemented a structured adjudication system that collects graders’ arguments in a machine-readable format without limiting graders’ abilities to provide free-form justifications for their classification decisions. Our system enables graders to cite instructions from a set of labeling guidelines, specified in the form of discrete classification rules and conditions that need to be met in order for each rule to be applicable. In the present work, we outline the process of designing and implementing this adjudication system, and report preliminary findings from deploying our system in the context of medical time series analysis for sleep stage classification.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1131–1137},
numpages = {7},
keywords = {Disagreement, Ambiguity, Adjudication, Medical time series},
location = {San Francisco, USA},
series = {WWW '19}
}

@article{castro2020causality,
  title={Causality matters in medical imaging},
  author={Castro, Daniel C and Walker, Ian and Glocker, Ben},
  journal={Nature Communications},
  volume={11},
  number={1},
  pages={1--10},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{10.1145/3313831.3376506,
author = {Schaekermann, Mike and Beaton, Graeme and Sanoubari, Elaheh and Lim, Andrew and Larson, Kate and Law, Edith},
title = {Ambiguity-Aware AI Assistants for Medical Data Analysis},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376506},
doi = {10.1145/3313831.3376506},
abstract = {Artificial intelligence (AI) assistants for clinical decision making show increasing promise in medicine. However, medical assessments can be contentious, leading to expert disagreement. This raises the question of how AI assistants should be designed to handle the classification of ambiguous cases. Our study compared two AI assistants that provide classification labels for medical time series data along with quantitative uncertainty estimates: conventional vs. ambiguity-aware. We simulated our ambiguity-aware AI based on real-world expert discussions to highlight cases likely to lead to expert disagreement, and to present arguments for conflicting classification choices. Our results demonstrate that ambiguity-aware AI can alter expert workflows by significantly increasing the proportion of contentious cases reviewed. We also found that the relevance of AI-provided arguments (selected from guidelines either randomly or by experts) affected experts' accuracy at revising AI-suggested labels. Our work contributes a novel perspective on the design of AI for contentious clinical assessments.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {artificial intelligence, medical data analysis, ambiguity},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376590,
author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
title = {Questioning the AI: Informing Design Practices for Explainable AI User Experiences},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376590},
doi = {10.1145/3313831.3376590},
abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {human-AI interaction, explainable AI, user experience},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@InProceedings{pmlr-v97-raghu19a,
  title = 	 {Direct Uncertainty Prediction for Medical Second Opinions},
  author =       {Raghu, Maithra and Blumer, Katy and Sayres, Rory and Obermeyer, Ziad and Kleinberg, Bobby and Mullainathan, Sendhil and Kleinberg, Jon},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5281--5290},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/raghu19a/raghu19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/raghu19a.html},
  abstract = 	 {The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.}
}

@article{10.1001/jamanetworkopen.2019.0096,
    author = {Barnett, Michael L. and Boddupalli, Dhruv and Nundy, Shantanu and Bates, David W.},
    title = "{Comparative Accuracy of Diagnosis by Collective Intelligence of Multiple Physicians vs Individual Physicians}",
    journal = {JAMA Network Open},
    volume = {2},
    number = {3},
    pages = {e190096-e190096},
    year = {2019},
    month = {03},
    abstract = "{The traditional approach of diagnosis by individual physicians has a high rate of misdiagnosis. Pooling multiple physicians’ diagnoses (collective intelligence) is a promising approach to reducing misdiagnoses, but its accuracy in clinical cases is unknown to date.To assess how the diagnostic accuracy of groups of physicians and trainees compares with the diagnostic accuracy of individual physicians.Cross-sectional study using data from the Human Diagnosis Project (Human Dx), a multicountry data set of ranked differential diagnoses by individual physicians, graduate trainees, and medical students (users) solving user-submitted, structured clinical cases. From May 7, 2014, to October 5, 2016, groups of 2 to 9 randomly selected physicians solved individual cases. Data analysis was performed from March 16, 2017, to July 30, 2018.The primary outcome was diagnostic accuracy, assessed as a correct diagnosis in the top 3 ranked diagnoses for an individual; for groups, the top 3 diagnoses were a collective differential generated using a weighted combination of user diagnoses with a variety of approaches. A version of the McNemar test was used to account for clustering across repeated solvers to compare diagnostic accuracy.Of the 2069 users solving 1572 cases from the Human Dx data set, 1228 (59.4\\%) were residents or fellows, 431 (20.8\\%) were attending physicians, and 410 (19.8\\%) were medical students. Collective intelligence was associated with increasing diagnostic accuracy, from 62.5\\% (95\\% CI, 60.1\\%-64.9\\%) for individual physicians up to 85.6\\% (95\\% CI, 83.9\\%-87.4\\%) for groups of 9 (23.0\\% difference; 95\\% CI, 14.9\\%-31.2\\%; P \\&lt; .001). The range of improvement varied by the specifications used for combining groups’ diagnoses, but groups consistently outperformed individuals regardless of approach. Absolute improvement in accuracy from individuals to groups of 9 varied by presenting symptom from an increase of 17.3\\% (95\\% CI, 6.4\\%-28.2\\%; P = .002) for abdominal pain to 29.8\\% (95\\% CI, 3.7\\%-55.8\\%; P = .02) for fever. Groups from 2 users (77.7\\% accuracy; 95\\% CI, 70.1\\%-84.6\\%) to 9 users (85.5\\% accuracy; 95\\% CI, 75.1\\%-95.9\\%) outperformed individual specialists in their subspecialty (66.3\\% accuracy; 95\\% CI, 59.1\\%-73.5\\%; P \\&lt; .001 vs groups of 2 and 9).A collective intelligence approach was associated with higher diagnostic accuracy compared with individuals, including individual specialists whose expertise matched the case diagnosis, across a range of medical cases. Given the few proven strategies to address misdiagnosis, this technique merits further study in clinical settings.}",
    issn = {2574-3805},
    doi = {10.1001/jamanetworkopen.2019.0096},
    url = {https://doi.org/10.1001/jamanetworkopen.2019.0096},
    eprint = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2726709/barnett\_2019\_oi\_190011.pdf},
}

@InProceedings{10.1007/978-3-319-11915-1_31,
author="Inel, Oana
and Khamkham, Khalid
and Cristea, Tatiana
and Dumitrache, Anca
and Rutjes, Arne
and van der Ploeg, Jelle
and Romaszko, Lukasz
and Aroyo, Lora
and Sips, Robert-Jan",
editor="Mika, Peter
and Tudorache, Tania
and Bernstein, Abraham
and Welty, Chris
and Knoblock, Craig
and Vrande{\v{c}}i{\'{c}}, Denny
and Groth, Paul
and Noy, Natasha
and Janowicz, Krzysztof
and Goble, Carole",
title="CrowdTruth: Machine-Human Computation Framework for Harnessing Disagreement in Gathering Annotated Data",
booktitle="The Semantic Web -- ISWC 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="486--504",
abstract="In this paper we introduce the CrowdTruth open-source software framework for machine-human computation, that implements a novel approach to gathering human annotation data for a variety of media (e.g. text, image, video). The CrowdTruth approach embodied in the software captures human semantics through a pipeline of four processes: a) combining various machine processing of media in order to better understand the input content and optimize its suitability for micro-tasks, thus optimize the time and cost of the crowdsourcing process; b) providing reusable human-computing task templates to collect the maximum diversity in the human interpretation, thus collect richer human semantics; c) implementing 'disagreement metrics', i.e. CrowdTruth metrics, to support deep analysis of the quality and semantics of the crowdsourcing data; and d) providing an interface to support data and results visualization. Instead of the traditional inter-annotator agreement, we use their disagreement as a useful signal to evaluate the data quality, ambiguity and vagueness. We demonstrate the applicability and robustness of this approach to a variety of problems across multiple domains. Moreover, we show the advantages of using open standards and the extensibility of the framework with new data modalities and annotation tasks.",
isbn="978-3-319-11915-1"
}

@inproceedings{10.1145/3290605.3300831,
author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
title = {Designing Theory-Driven User-Centric Explainable AI},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300831},
doi = {10.1145/3290605.3300831},
abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {clinical decision making, explanations, explainable artificial intelligence, decision making, intelligibility},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{DODGION2017196,
title = "Institutional variation in surgical care for early-stage breast cancer at community hospitals",
journal = "Journal of Surgical Research",
volume = "211",
pages = "196 - 205",
year = "2017",
issn = "0022-4804",
doi = "https://doi.org/10.1016/j.jss.2016.11.065",
url = "http://www.sciencedirect.com/science/article/pii/S0022480416305431",
author = "Christopher M. Dodgion and Stuart R. Lipsitz and Marquita R. Decker and Yue-Yung Hu and Sudha R. Pavuluri Quamme and Anita Karcz and Leonard D'Avolio and Caprice C. Greenberg",
abstract = "Background
There is significant institutional variation in the surgical care of breast cancer, and this may reflect access to services and resultant physician practice patterns. In previous studies, specialty care has been associated with variation in the operative treatment of breast cancer but has not been evaluated in a community setting. This study investigates these issues in a cohort of 59 community hospitals in the United States.
Materials and methods
Data on patients receiving an operation for breast cancer (2006-2009) in a large, geographically diverse cohort of hospitals were obtained. Administrative data, autoabstracted cancer-specific variables from free text, and multiple other data sets were combined. Polymotous logistic regression with multilevel outcomes identified associations between these variables and surgical treatment.
Results
At 59 community hospitals, 4766 patients underwent breast conserving surgery (BCS), mastectomy, or mastectomy with reconstruction. The older patients were most likely to receive mastectomy alone, whereas the younger age group underwent more reconstruction (age <50), and BCS was most likely in patients aged 50-65. Surgical procedure also varied according to tumor characteristics. BCS was more likely at smaller hospitals, those with ambulatory surgery centers, and those located in nonmetropolitan areas. The likelihood of reconstruction doubled when there were more reconstructive surgeons in the health services area (P = 0.02). BCS was more likely when radiation oncology services were available within the hospital or network (P = 0.04).
Conclusions
Interpretation of these results for practice redesign is not straightforward. Although access to specialty care is statistically associated with type of breast surgical procedure, clinical impact is limited. It may be more effective to target other aspects of care to ensure each patient receives treatment consistent with her individual preferences."
}

@Article{10.1200EDBK200141,
  author   = {Sirintrapun, S. Joseph and Lopez, Ana Maria},
  title    = {Telemedicine in Cancer Care},
  doi      = {10.1200/EDBK\_200141},
  eprint   = {https://doi.org/10.1200/EDBK_200141},
  note     = {PMID: 30231354},
  number   = {38},
  pages    = {540-545},
  url      = {https://doi.org/10.1200/EDBK_200141},
  abstract = {Telemedicine uses telecommunications technology as a tool to deliver health care to populations with limited access to care. Telemedicine has been tested in multiple clinical settings, demonstrating at least equivalency to in-person care and high levels of patient and health professional satisfaction. Teleoncology has been demonstrated to improve access to care and decrease health care costs. Teleconsultations may take place in a synchronous, asynchronous, or blended format. Examples of successful teleoncology applications include cancer telegenetics, bundling of cancer-related teleapplications, remote chemotherapy supervision, symptom management, survivorship care, palliative care, and approaches to increase access to cancer clinical trials. Telepathology is critical to cancer care and may be accomplished synchronously and asynchronously for both cytology and tissue diagnoses. Mobile applications support symptom management, lifestyle modification, and medication adherence as a tool for home-based care. Telemedicine can support the oncologist with access to interactive tele-education. Teleoncology practice should maintain in-person professional standards, including documentation integrated into the patient’s electronic health record. Telemedicine training is essential to facilitate rapport, maximize engagement, and conduct an accurate virtual exam. With the appropriate attachments, the only limitation to the virtual exam is palpation. The national telehealth resource centers can provide interested clinicians with the latest information on telemedicine reimbursement, parity, and practice. To experience the gains of teleoncology, appropriate training, education, as well as paying close attention to gaps, such as those inherent in the digital divide, are essential.},
  journal  = {American Society of Clinical Oncology Educational Book},
  year     = {2018},
}

@article{LIU2019e271,
title = "A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis",
journal = "The Lancet Digital Health",
volume = "1",
number = "6",
pages = "e271 - e297",
year = "2019",
issn = "2589-7500",
doi = "https://doi.org/10.1016/S2589-7500(19)30123-2",
url = "http://www.sciencedirect.com/science/article/pii/S2589750019301232",
author = "Xiaoxuan Liu and Livia Faes and Aditya U Kale and Siegfried K Wagner and Dun Jack Fu and Alice Bruynseels and Thushika Mahendiran and Gabriella Moraes and Mohith Shamdas and Christoph Kern and Joseph R Ledsam and Martin K Schmid and Konstantinos Balaskas and Eric J Topol and Lucas M Bachmann and Pearse A Keane and Alastair K Denniston",
abstract = "Summary
Background
Deep learning offers considerable promise for medical diagnostics. We aimed to evaluate the diagnostic accuracy of deep learning algorithms versus health-care professionals in classifying diseases using medical imaging.
Methods
In this systematic review and meta-analysis, we searched Ovid-MEDLINE, Embase, Science Citation Index, and Conference Proceedings Citation Index for studies published from Jan 1, 2012, to June 6, 2019. Studies comparing the diagnostic performance of deep learning models and health-care professionals based on medical imaging, for any disease, were included. We excluded studies that used medical waveform data graphics material or investigated the accuracy of image segmentation rather than disease classification. We extracted binary diagnostic accuracy data and constructed contingency tables to derive the outcomes of interest: sensitivity and specificity. Studies undertaking an out-of-sample external validation were included in a meta-analysis, using a unified hierarchical model. This study is registered with PROSPERO, CRD42018091176.
Findings
Our search identified 31 587 studies, of which 82 (describing 147 patient cohorts) were included. 69 studies provided enough data to construct contingency tables, enabling calculation of test accuracy, with sensitivity ranging from 9·7% to 100·0% (mean 79·1%, SD 0·2) and specificity ranging from 38·9% to 100·0% (mean 88·3%, SD 0·1). An out-of-sample external validation was done in 25 studies, of which 14 made the comparison between deep learning models and health-care professionals in the same sample. Comparison of the performance between health-care professionals in these 14 studies, when restricting the analysis to the contingency table for each study reporting the highest accuracy, found a pooled sensitivity of 87·0% (95% CI 83·0–90·2) for deep learning models and 86·4% (79·9–91·0) for health-care professionals, and a pooled specificity of 92·5% (95% CI 85·1–96·4) for deep learning models and 90·5% (80·6–95·7) for health-care professionals.
Interpretation
Our review found the diagnostic performance of deep learning models to be equivalent to that of health-care professionals. However, a major finding of the review is that few studies presented externally validated results or compared the performance of deep learning models and health-care professionals using the same sample. Additionally, poor reporting is prevalent in deep learning studies, which limits reliable interpretation of the reported diagnostic accuracy. New reporting standards that address specific challenges of deep learning could improve future studies, enabling greater confidence in the results of future evaluations of this promising technology.
Funding
None."
}

@inproceedings{10.1145/3136755.3143016,
author = {Wang, Shuai and Wang, Wenxuan and Zhao, Jinming and Chen, Shizhe and Jin, Qin and Zhang, Shilei and Qin, Yong},
title = {Emotion Recognition with Multimodal Features and Temporal Models},
year = {2017},
isbn = {9781450355438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136755.3143016},
doi = {10.1145/3136755.3143016},
abstract = {This paper presents our methods to the Audio-Video Based Emotion Recognition subtask in the 2017 Emotion Recognition in the Wild (EmotiW) Challenge. The task aims to predict one of the seven basic emotions for short video segments. We extract different features from audio and facial expression modalities. We also explore the temporal LSTM model with the input of frame facial features, which improves the performance of the non-temporal model. The fusion of different modality features and the temporal model lead us to achieve a 58.5% accuracy on the testing set, which shows the effectiveness of our methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
pages = {598–602},
numpages = {5},
keywords = {CNN, Multimodal Features, LSTM, Emotion Recognition},
location = {Glasgow, UK},
series = {ICMI '17}
}

@book{zheng2014marginal,
  title={Marginal space learning for medical image analysis},
  author={Zheng, Yefeng and Comaniciu, Dorin},
  year={2014},
  publisher={Springer}
}

@InProceedings{10.1007/978-3-030-00934-2_99,
author="Mehta, Sachin
and Mercan, Ezgi
and Bartlett, Jamen
and Weaver, Donald
and Elmore, Joann G.
and Shapiro, Linda",
editor="Frangi, Alejandro F.
and Schnabel, Julia A.
and Davatzikos, Christos
and Alberola-L{\'o}pez, Carlos
and Fichtinger, Gabor",
title="Y-Net: Joint Segmentation and Classification for Diagnosis of Breast Biopsy Images",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="893--901",
abstract="In this paper, we introduce a conceptually simple network for generating discriminative tissue-level segmentation masks for the purpose of breast cancer diagnosis. Our method efficiently segments different types of tissues in breast biopsy images while simultaneously predicting a discriminative map for identifying important areas in an image. Our network, Y-Net, extends and generalizes U-Net by adding a parallel branch for discriminative map generation and by supporting convolutional block modularity, which allows the user to adjust network efficiency without altering the network topology. Y-Net delivers state-of-the-art segmentation accuracy while learning {\$}{\$}6.6{\backslash}times {\$}{\$}fewer parameters than its closest competitors. The addition of descriptive power from Y-Net's discriminative segmentation masks improve diagnostic classification accuracy by 7{\%} over state-of-the-art methods for diagnostic classification. Source code is available at: https://sacmehta.github.io/YNet.",
isbn="978-3-030-00934-2"
}

@InProceedings{Hou_2016_CVPR,
author = {Hou, Le and Samaras, Dimitris and Kurc, Tahsin M. and Gao, Yi and Davis, James E. and Saltz, Joel H.},
title = {Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{shen2019deep,
  title={Deep learning to improve breast cancer detection on screening mammography},
  author={Shen, Li and Margolies, Laurie R and Rothstein, Joseph H and Fluder, Eugene and McBride, Russell and Sieh, Weiva},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={1--12},
  year={2019},
  publisher={Nature Publishing Group}
}


@Article{s20143903,
AUTHOR = {Chowdhary, Chiranji Lal and Mittal, Mohit and P., Kumaresan and Pattanaik, P. A. and Marszalek, Zbigniew},
TITLE = {An Efficient Segmentation and Classification System in Medical Images Using Intuitionist Possibilistic Fuzzy C-Mean Clustering and Fuzzy SVM Algorithm},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3903},
URL = {https://www.mdpi.com/1424-8220/20/14/3903},
ISSN = {1424-8220},
ABSTRACT = {The herpesvirus, polyomavirus, papillomavirus, and retrovirus families are associated with breast cancer. More effort is needed to assess the role of these viruses in the detection and diagnosis of breast cancer cases in women. The aim of this paper is to propose an efficient segmentation and classification system in the Mammography Image Analysis Society (MIAS) images of medical images. Segmentation became challenging for medical images because they are not illuminated in the correct way. The role of segmentation is essential in concern with detecting syndromes in human. This research work is on the segmentation of medical images based on intuitionistic possibilistic fuzzy c-mean (IPFCM) clustering. Intuitionist fuzzy c-mean (IFCM) and possibilistic fuzzy c-mean (PFCM) algorithms are hybridised to deal with problems of fuzzy c-mean. The introduced clustering methodology, in this article, retains the positive points of PFCM which helps to overcome the problem of the coincident clusters, thus the noise and less sensitivity to the outlier. The IPFCM improves the fundamentals of fuzzy c-mean by using intuitionist fuzzy sets. For the clustering of mammogram images for breast cancer detector of abnormal images, IPFCM technique has been applied. The proposed method has been compared with other available fuzzy clustering methods to prove the efficacy of the proposed approach. We compared support vector machine (SVM), decision tree (DT), rough set data analysis (RSDA) and Fuzzy-SVM classification algorithms for achieving an optimal classification result. The outcomes of the studies show that the proposed approach is highly effective with clustering and also with classification of breast cancer. The performance average segmentation accuracy for MIAS images with different noise level 5%, 7% and 9% of IPFCM is 91.25%, 87.50% and 85.30% accordingly. The average classification accuracy rates of the methods (Otsu, Fuzzy c-mean, IFCM, PFCM and IPFCM) for Fuzzy-SVM are 79.69%, 92.19%, 93.13%, 95.00%, and 98.85%, respectively.},
DOI = {10.3390/s20143903}
}

@article{ammar2015semantically,
  title={Semantically segmented clustering based on possibilistic and rough set theories},
  author={Ammar, Asma and Elouedi, Zied and Lingras, Pawan},
  journal={International Journal of Intelligent Systems},
  volume={30},
  number={6},
  pages={676--706},
  year={2015},
  publisher={Wiley Online Library}
}

@article{wang2018support,
  title={A support vector machine-based ensemble algorithm for breast cancer diagnosis},
  author={Wang, Haifeng and Zheng, Bichen and Yoon, Sang Won and Ko, Hoo Sang},
  journal={European Journal of Operational Research},
  volume={267},
  number={2},
  pages={687--699},
  year={2018},
  publisher={Elsevier}
}

@article{milosevic2017comparison,
  title={A comparison of methods for three-class mammograms classification},
  author={Milosevic, Marina and Jovanovic, Zeljko and Jankovic, Dragan},
  journal={Technology and Health Care},
  volume={25},
  number={4},
  pages={657--670},
  year={2017},
  publisher={IOS Press}
}

@article{sharma2020model,
  title={A Model for Mammogram Image Segmentation based on Hybrid Enhancement},
  author={Sharma, Kamal Nain and Kamra, Amit},
  journal={Int J Cur Res Rev| Vol},
  volume={12},
  number={16},
  pages={34},
  year={2020}
}

@ARTICLE{9247957,
  author={A. M. {Alhassan} and W. M. N. W. {Zainon}},
  journal={IEEE Access}, 
  title={BAT Algorithm With fuzzy C-Ordered Means (BAFCOM) Clustering Segmentation and Enhanced Capsule Networks (ECN) for Brain Cancer MRI Images Classification}, 
  year={2020},
  volume={8},
  number={},
  pages={201741-201751},
  doi={10.1109/ACCESS.2020.3035803}
}

@article{AGRAWAL201927,
title = "Combining clustering and classification ensembles: A novel pipeline to identify breast cancer profiles",
journal = "Artificial Intelligence in Medicine",
volume = "97",
pages = "27 - 37",
year = "2019",
issn = "0933-3657",
doi = "https://doi.org/10.1016/j.artmed.2019.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0933365717303913",
author = "Utkarsh Agrawal and Daniele Soria and Christian Wagner and Jonathan Garibaldi and Ian O. Ellis and John M.S. Bartlett and David Cameron and Emad A. Rakha and Andrew R. Green",
keywords = "Ensemble clustering, Ensemble classification, Class level fusion, Refining cluster results, Breast cancer, Pipeline",
abstract = "Breast Cancer is one of the most common causes of cancer death in women, representing a very complex disease with varied molecular alterations. To assist breast cancer prognosis, the classification of patients into biological groups is of great significance for treatment strategies. Recent studies have used an ensemble of multiple clustering algorithms to elucidate the most characteristic biological groups of breast cancer. However, the combination of various clustering methods resulted in a number of patients remaining unclustered. Therefore, a framework still needs to be developed which can assign as many unclustered (i.e. biologically diverse) patients to one of the identified groups in order to improve classification. Therefore, in this paper we develop a novel classification framework which introduces a new ensemble classification stage after the ensemble clustering stage to target the unclustered patients. Thus, a step-by-step pipeline is introduced which couples ensemble clustering with ensemble classification for the identification of core groups, data distribution in them and improvement in final classification results by targeting the unclustered data. The proposed pipeline is employed on a novel real world breast cancer dataset and subsequently its robustness and stability are examined by testing it on standard datasets. The results show that by using the presented framework, an improved classification is obtained. Finally, the results have been verified using statistical tests, visualisation techniques, cluster quality assessment and interpretation from clinical experts."
}

@INPROCEEDINGS{8451510,
  author={I. {Domingues} and P. H. {Abreu} and J. {Santos}},
  booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)}, 
  title={Bi-Rads Classification of Breast Cancer: A New Pre-Processing Pipeline for Deep Models Training}, 
  year={2018},
  volume={},
  number={},
  pages={1378-1382},
  doi={10.1109/ICIP.2018.8451510}
}

@INPROCEEDINGS{8462671,
  author={N. {Wu} and K. J. {Geras} and Y. {Shen} and J. {Su} and S. G. {Kim} and E. {Kim} and S. {Wolfson} and L. {Moy} and K. {Cho}},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Breast Density Classification with Deep Convolutional Neural Networks}, 
  year={2018},
  volume={},
  number={},
  pages={6682-6686},
  doi={10.1109/ICASSP.2018.8462671}
}

@article{BHARDWAJ20154611,
title = "Breast cancer diagnosis using Genetically Optimized Neural Network model",
journal = "Expert Systems with Applications",
volume = "42",
number = "10",
pages = "4611 - 4620",
year = "2015",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2015.01.065",
url = "http://www.sciencedirect.com/science/article/pii/S0957417415000883",
author = "Arpit Bhardwaj and Aruna Tiwari",
keywords = "Genetically Optimized Neural Network, Artificial Neural Network, Genetic Programming, Modified Crossover Operator",
abstract = "One in every eight women is susceptible to breast cancer, at some point of time in her life. Early detection and effective treatment is the only rescue to reduce breast cancer mortality. Accurate classification of a breast cancer tumor is an important task in medical diagnosis. Machine learning techniques are gaining importance in medical diagnosis because of their classification capability. In this paper, we propose a new, Genetically Optimized Neural Network (GONN) algorithm, for solving classification problems. We evolve a neural network genetically to optimize its architecture (structure and weight) for classification. We introduce new crossover and mutation operators which differ from standard crossover and mutation operators to reduce the destructive nature of these operators. We use the GONN algorithm to classify breast cancer tumors as benign or malignant. To demonstrate our results, we had taken the WBCD database from UCI Machine Learning repository and compared the classification accuracy, sensitivity, specificity, confusion matrix, ROC curves and AUC under ROC curves of GONN with classical model and classical back propagation model. Our algorithm gives classification accuracy of 98.24%, 99.63% and 100% for 50–50, 60–40, 70–30 training–testing partition respectively and 100% for 10 fold cross validation. The results show that our approach works well with the breast cancer database and can be a good alternative to the well-known machine learning methods."
}

@article{goldenberg2011computer,
  title={Computer-aided simple triage},
  author={Goldenberg, Roman and Peled, Nathan},
  journal={International journal of computer assisted radiology and surgery},
  volume={6},
  number={5},
  pages={705},
  year={2011},
  publisher={Springer}
}

@inproceedings{10.1117/12.2081576,
author = {Pablo Fonseca and Julio Mendoza and Jacques Wainer and Jose Ferrer and Joseph Pinto and Jorge Guerrero and Benjamin Castaneda},
title = {{Automatic breast density classification using a convolutional neural network architecture search procedure}},
volume = {9414},
booktitle = {Medical Imaging 2015: Computer-Aided Diagnosis},
editor = {Lubomir M. Hadjiiski and Georgia D. Tourassi},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {556 -- 563},
abstract = {Breast parenchymal density is considered a strong indicator of breast cancer risk and therefore useful for preventive tasks. Measurement of breast density is often qualitative and requires the subjective judgment of radiologists. Here we explore an automatic breast composition classification workflow based on convolutional neural networks for feature extraction in combination with a support vector machines classifier. This is compared to the assessments of seven experienced radiologists. The experiments yielded an average kappa value of 0.58 when using the mode of the radiologists’ classifications as ground truth. Individual radiologist performance against this ground truth yielded kappa values between 0.56 and 0.79.},
keywords = {Mammograms, Breast Density, Automatic assessment, Convolutional Neural Networks, Feature Learning},
year = {2015},
doi = {10.1117/12.2081576},
URL = {https://doi.org/10.1117/12.2081576}
}

@INPROCEEDINGS{8622433,
  author={B. {Kovalerchuk} and N. {Neuhaus}},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Toward Efficient Automation of Interpretable Machine Learning}, 
  year={2018},
  volume={},
  number={},
  pages={4940-4947},
  doi={10.1109/BigData.2018.8622433}
}

@article{ahmed2020images,
  title={Images data practices for Semantic Segmentation of Breast Cancer using Deep Neural Network},
  author={Ahmed, Luqman and Iqbal, Muhammad Munwar and Aldabbas, Hamza and Khalid, Shehzad and Saleem, Yasir and Saeed, Saqib},
  journal={Journal of Ambient Intelligence and Humanized Computing},
  pages={1--17},
  year={2020},
  publisher={Springer}
}

@article{hesamian2019deep,
  title={Deep learning techniques for medical image segmentation: Achievements and challenges},
  author={Hesamian, Mohammad Hesam and Jia, Wenjing and He, Xiangjian and Kennedy, Paul},
  journal={Journal of digital imaging},
  volume={32},
  number={4},
  pages={582--596},
  year={2019},
  publisher={Springer}
}

@article{murtaza2019deep,
  title={Deep learning-based breast cancer classification through medical imaging modalities: state of the art and research challenges},
  author={Murtaza, Ghulam and Shuib, Liyana and Wahab, Ainuddin Wahid Abdul and Mujtaba, Ghulam and Nweke, Henry Friday and Al-garadi, Mohammed Ali and Zulfiqar, Fariha and Raza, Ghulam and Azmi, Nor Aniza},
  journal={Artificial Intelligence Review},
  pages={1--66},
  year={2019},
  publisher={Springer}
}

@inproceedings{10.1145/3373017.3373051,
author = {Ke, Jing and Liu, Changchang and Lu, Yizhou and Jing, Naifeng and Liang, Xiaoyao and Jiang, Fusong},
title = {FIMIL: A High-Throughput Deep Learning Model for Abnormality Detection with Weak Annotation in Microscopy Images},
year = {2020},
isbn = {9781450376976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373017.3373051},
doi = {10.1145/3373017.3373051},
abstract = {Automatic computer-aided detection plays an important role in biomedical image analysis. Many studies have focused on weak supervised learning as annotation tasks are time-consuming and tedious. Compared with pixel-wise annotation by particular software on the scanned digital high-resolution images, an alternative method of marking out of suspicious regions on microscopy slides is significantly more convenient for pathologists. Additionally, with a focus on dysplasias in the central area, there is a high likelihood of the similar tissues to be found around in clusters. In this paper, for weak annotation on microscopy images, we propose an efficient Foveated Imaging based Multiple Instance Learning (FIMIL) framework to classify weakly-labeled microscopy images. The model also provides multi-scale algorithm for arbitrary image size, in which the patches with highest possibility to contain dysplasia are considered as ”fixation points” in the image. The developed model combines deep convolutional neural networks (CNNs) with multiple instance learning (MIL) for dysplasias detection with only image-level labeling. The benchmark tests are carried out on the marked regions of 40x magnified whole-slide cytology images and the normal/abnormal label and their corresponding possibilities are predicted. Evaluated on the real-life clinical data, our proposed model shows high accuracy and efficiency by weakly-supervised learning. 1},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {34},
numpages = {6},
keywords = {microscopy image, multiple instance learning, performance acceleration, foveated imaging},
location = {Melbourne, VIC, Australia},
series = {ACSW '20}
}

@article{TAJBAKHSH2020101693,
title = "Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation",
journal = "Medical Image Analysis",
volume = "63",
pages = "101693",
year = "2020",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2020.101693",
url = "http://www.sciencedirect.com/science/article/pii/S136184152030058X",
author = "Nima Tajbakhsh and Laura Jeyaseelan and Qian Li and Jeffrey N. Chiang and Zhihao Wu and Xiaowei Ding",
keywords = "Medical image segmentation, Imperfect dataset, Scarce annotations, Noisy annotations, Unreliable annotations, Sparse annotations, And weak annotations",
abstract = "The medical imaging literature has witnessed remarkable progress in high-performing segmentation models based on convolutional neural networks. Despite the new performance highs, the recent advanced segmentation models still require large, representative, and high quality annotated datasets. However, rarely do we have a perfect training dataset, particularly in the field of medical imaging, where data and annotations are both expensive to acquire. Recently, a large body of research has studied the problem of medical image segmentation with imperfect datasets, tackling two major dataset limitations: scarce annotations where only limited annotated data is available for training, and weak annotations where the training data has only sparse annotations, noisy annotations, or image-level annotations. In this article, we provide a detailed review of the solutions above, summarizing both the technical novelties and empirical results. We further compare the benefits and requirements of the surveyed methodologies and provide our recommended solutions. We hope this survey article increases the community awareness of the techniques that are available to handle imperfect medical image segmentation datasets."
}

@InProceedings{10.1007/978-3-030-59719-1_44,
author="Zheng, Hao
and Zhuang, Zhiguo
and Qin, Yulei
and Gu, Yun
and Yang, Jie
and Yang, Guang-Zhong",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="Weakly Supervised Deep Learning for Breast Cancer Segmentation with Coarse Annotations",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="450--459",
abstract="Cancer lesion segmentation plays a vital role in breast cancer diagnosis and treatment planning. As creating labels for large medical image datasets can be time-consuming, laborious and error prone, a framework is proposed in this paper by using coarse annotations generated from boundary scribbles for training deep convolutional neural networks. These coarse annotations include locations of lesions but are lack of accurate information about boundaries. To mitigate the negative impact of annotation errors, we propose an adaptive weighted constrained loss that can change the weight of the task-specific penalty term according to the learning process. To impose further supervision about the boundaries, uncertainty-based boundary maps are generated, which can provide better descriptions for the blurry boundaries. Validation on a dataset containing 154 MRI scans has shown an average Dice coefficient of {\$}{\$}82.25{\backslash}{\%}{\$}{\$}82.25{\%}, which is comparable to results from fine annotations, demonstrating the efficacy of the proposed approach.",
isbn="978-3-030-59719-1"
}

@ARTICLE{7412749,
  author={M. {Kallenberg} and K. {Petersen} and M. {Nielsen} and A. Y. {Ng} and P. {Diao} and C. {Igel} and C. M. {Vachon} and K. {Holland} and R. R. {Winkel} and N. {Karssemeijer} and M. {Lillholm}},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Unsupervised Deep Learning Applied to Breast Density Segmentation and Mammographic Risk Scoring}, 
  year={2016},
  volume={35},
  number={5},
  pages={1322-1331},
  doi={10.1109/TMI.2016.2532122}
}

@article {Dabbous397,
	author = {Dabbous, Firas M. and Dolecek, Therese A. and Berbaum, Michael L. and Friedewald, Sarah M. and Summerfelt, Wm. Thomas and Hoskins, Kent and Rauscher, Garth H.},
	title = {Impact of a False-Positive Screening Mammogram on Subsequent Screening Behavior and Stage at Breast Cancer Diagnosis},
	volume = {26},
	number = {3},
	pages = {397--403},
	year = {2017},
	doi = {10.1158/1055-9965.EPI-16-0524},
	publisher = {American Association for Cancer Research},
	abstract = {Background: Experiencing a false positive (FP) screening mammogram is economically, physically, and emotionally burdensome, which may affect future screening behavior by delaying the next scheduled mammogram or by avoiding screening altogether. We sought to examine the impact of a FP screening mammogram on the subsequent screening mammography behavior.Methods: Delay in obtaining subsequent screening was defined as any mammogram performed more than 12 months from index mammogram. The Kaplan{\textendash}Meier (product limit) estimator and Cox proportional hazards model were used to estimate the unadjusted delay and the hazard ratio (HR) of delay of the subsequent screening mammogram within the next 36 months from the index mammogram date.Results: A total of 650,232 true negative (TN) and 90,918 FP mammograms from 261,767 women were included. The likelihood of a subsequent mammogram was higher in women experiencing a TN result than women experiencing a FP result (85.0\% vs. 77.9\%, P \&lt; 0.001). The median delay in returning to screening was higher for FP versus TN (13 months vs. 3 months, P \&lt; 0.001). Women with TN result were 36\% more likely to return to screening in the next 36 months compared with women with a FP result HR = 1.36 (95\% CI, 1.35{\textendash}1.37). Experiencing a FP mammogram increases the risk of late stage at diagnosis compared with prior TN mammogram (P \&lt; 0.001).Conclusions: Women with a FP mammogram were more likely to delay their subsequent screening compared with women with a TN mammogram.Impact: A prior FP experience may subsequently increase the 4-year cumulative risk of late stage at diagnosis. Cancer Epidemiol Biomarkers Prev; 26(3); 397{\textendash}403. {\textcopyright}2017 AACR.},
	issn = {1055-9965},
	URL = {https://cebp.aacrjournals.org/content/26/3/397},
	eprint = {https://cebp.aacrjournals.org/content/26/3/397.full.pdf},
	journal = {Cancer Epidemiology and Prevention Biomarkers}
}

@article{CHOUGRAD201819,
title = "Deep Convolutional Neural Networks for breast cancer screening",
journal = "Computer Methods and Programs in Biomedicine",
volume = "157",
pages = "19 - 30",
year = "2018",
issn = "0169-2607",
doi = "https://doi.org/10.1016/j.cmpb.2018.01.011",
url = "http://www.sciencedirect.com/science/article/pii/S0169260717301451",
author = "Hiba Chougrad and Hamid Zouaki and Omar Alheyane",
keywords = "Deep learning, Convolutional Neural Network, Transfer learning, Computer-aided Diagnosis, Breast cancer, Breast mass lesion classification",
abstract = "Background and objective
Radiologists often have a hard time classifying mammography mass lesions which leads to unnecessary breast biopsies to remove suspicions and this ends up adding exorbitant expenses to an already burdened patient and health care system.
Methods
In this paper we developed a Computer-aided Diagnosis (CAD) system based on deep Convolutional Neural Networks (CNN) that aims to help the radiologist classify mammography mass lesions. Deep learning usually requires large datasets to train networks of a certain depth from scratch. Transfer learning is an effective method to deal with relatively small datasets as in the case of medical images, although it can be tricky as we can easily start overfitting.
Results
In this work, we explore the importance of transfer learning and we experimentally determine the best fine-tuning strategy to adopt when training a CNN model. We were able to successfully fine-tune some of the recent, most powerful CNNs and achieved better results compared to other state-of-the-art methods which classified the same public datasets. For instance we achieved 97.35% accuracy and 0.98 AUC on the DDSM database, 95.50% accuracy and 0.97 AUC on the INbreast database and 96.67% accuracy and 0.96 AUC on the BCDR database. Furthermore, after pre-processing and normalizing all the extracted Regions of Interest (ROIs) from the full mammograms, we merged all the datasets to build one large set of images and used it to fine-tune our CNNs. The CNN model which achieved the best results, a 98.94% accuracy, was used as a baseline to build the Breast Cancer Screening Framework. To evaluate the proposed CAD system and its efficiency to classify new images, we tested it on an independent database (MIAS) and got 98.23% accuracy and 0.99 AUC.
Conclusion
The results obtained demonstrate that the proposed framework is performant and can indeed be used to predict if the mass lesions are benign or malignant."
}

@ARTICLE{8861376,
  author={N. {Wu} and J. {Phang} and J. {Park} and Y. {Shen} and Z. {Huang} and M. {Zorin} and S. {Jastrzebski} and T. {Fevry} and J. {Katsnelson} and E. {Kim} and S. {Wolfson} and U. {Parikh} and S. {Gaddam} and L. L. Y. {Lin} and K. {Ho} and J. D. {Weinstein} and B. {Reig} and Y. {Gao} and H. {Toth} and K. {Pysarenko} and A. {Lewin} and J. {Lee} and K. {Airola} and E. {Mema} and S. {Chung} and E. {Hwang} and N. {Samreen} and S. G. {Kim} and L. {Heacock} and L. {Moy} and K. {Cho} and K. J. {Geras}},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening}, 
  year={2020},
  volume={39},
  number={4},
  pages={1184-1194},
  doi={10.1109/TMI.2019.2945514}
}

@ARTICLE{8032490,
  author={G. {Carneiro} and J. {Nascimento} and A. P. {Bradley}},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Automated Analysis of Unregistered Multi-View Mammograms With Deep Learning}, 
  year={2017},
  volume={36},
  number={11},
  pages={2355-2365},
  doi={10.1109/TMI.2017.2751523}
}

@article{lee2017curated,
  title={A curated mammography data set for use in computer-aided detection and diagnosis research},
  author={Lee, Rebecca Sawyer and Gimenez, Francisco and Hoogi, Assaf and Miyake, Kanae Kawai and Gorovoy, Mia and Rubin, Daniel L},
  journal={Scientific data},
  volume={4},
  pages={170177},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{10.1145/3079765,
author = {Riegler, Michael and Pogorelov, Konstantin and Eskeland, Sigrun Losada and Schmidt, Peter Thelin and Albisser, Zeno and Johansen, Dag and Griwodz, Carsten and Halvorsen, P\r{a}l and Lange, Thomas De},
title = {From Annotation to Computer-Aided Diagnosis: Detailed Evaluation of a Medical Multimedia System},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3079765},
doi = {10.1145/3079765},
abstract = {Holistic medical multimedia systems covering end-to-end functionality from data collection to aided diagnosis are highly needed, but rare. In many hospitals, the potential value of multimedia data collected through routine examinations is not recognized. Moreover, the availability of the data is limited, as the health care personnel may not have direct access to stored data. However, medical specialists interact with multimedia content daily through their everyday work and have an increasing interest in finding ways to use it to facilitate their work processes. In this article, we present a novel, holistic multimedia system aiming to tackle automatic analysis of video from gastrointestinal (GI) endoscopy. The proposed system comprises the whole pipeline, including data collection, processing, analysis, and visualization. It combines filters using machine learning, image recognition, and extraction of global and local image features. The novelty is primarily in this holistic approach and its real-time performance, where we automate a complete algorithmic GI screening process. We built the system in a modular way to make it easily extendable to analyze various abnormalities, and we made it efficient in order to run in real time. The conducted experimental evaluation proves that the detection and localization accuracy are comparable or even better than existing systems, but it is by far leading in terms of real-time performance and efficient resource consumption.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
articleno = {26},
numpages = {26},
keywords = {Medical multimedia system, evaluation, gastrointestinal tract}
}

@article{NISHIKAWA20141320,
title = "CADe for Early Detection of Breast Cancer—Current Status and Why We Need to Continue to Explore New Approaches",
journal = "Academic Radiology",
volume = "21",
number = "10",
pages = "1320 - 1321",
year = "2014",
issn = "1076-6332",
doi = "https://doi.org/10.1016/j.acra.2014.05.018",
url = "http://www.sciencedirect.com/science/article/pii/S1076633214002232",
author = "Robert M. Nishikawa and David Gur",
keywords = "Computer-aided detection, computer-aided diagnosis, breast imaging, mammographic screening",
abstract = "The authors describe where we are in terms of using computer-aided detection (CADe) systems during clinical mammographic interpretations, what are the issues that we face, and why they believe that, despite disappointment in terms of verified added value when it comes to detection of soft tissue abnormalities, we need to continue to explore new approaches to improving CADe-alone performance levels and, more important perhaps, new approaches to optimal communication of CADe-generated information."
}

@article{SADAF2011457,
title = "Performance of computer-aided detection applied to full-field digital mammography in detection of breast cancers",
journal = "European Journal of Radiology",
volume = "77",
number = "3",
pages = "457 - 461",
year = "2011",
issn = "0720-048X",
doi = "https://doi.org/10.1016/j.ejrad.2009.08.024",
url = "http://www.sciencedirect.com/science/article/pii/S0720048X09005117",
author = "Arifa Sadaf and Pavel Crystal and Anabel Scaranelo and Thomas Helbich",
keywords = "Mammography, Computer-assisted detection, Breast cancer",
abstract = "Objective
The aim of this retrospective study was to evaluate performance of computer-aided detection (CAD) with full-field digital mammography (FFDM) in detection of breast cancers.
Materials and Methods
CAD was retrospectively applied to standard mammographic views of 127 cases with biopsy proven breast cancers detected with FFDM (Senographe 2000, GE Medical Systems). CAD sensitivity was assessed in total group of 127 cases and for subgroups based on breast density, mammographic lesion type, mammographic lesion size, histopathology and mode of presentation.
Results
Overall CAD sensitivity was 91% (115 of 127 cases). There were no statistical differences (p>0.1) in CAD detection of cancers in dense breasts 90% (53/59) versus non-dense breasts 91% (62/68). There was statistical difference (p<0.05) in CAD detection of cancers that appeared mammographically as microcalcifications only versus other mammographic manifestations. CAD detected 100% (44/44) of cancers manifesting as microcalcifications, 89% (47/53) as no-calcified masses or asymmetries, 88% (14/16) as masses with associated calcifications, and 71% (10/14) as architectural distortions. CAD sensitivity for cancers 1–10mm was 84% (38/45); 11–20mm 93% (55/59); and >20mm 97% (22/23).
Conclusion
CAD applied to FFDM showed 100% sensitivity in identifying cancers manifesting as microcalcifications only and high sensitivity 86% (71/83) for other mammographic appearances of cancer. Sensitivity is influenced by lesion size. CAD in FFDM is an adjunct helping radiologist in early detection of breast cancers."
}

@article{https://doi.org/10.1118/1.4921612,
author = {Chu, Jinghui and Min, Hang and Liu, Li and Lu, Wei},
title = {A novel computer aided breast mass detection scheme based on morphological enhancement and SLIC superpixel segmentation},
journal = {Medical Physics},
volume = {42},
number = {7},
pages = {3859-3869},
keywords = {feature extraction, image segmentation, iterative methods, mammography, medical image processing, Mammography, Numerical approximation and analysis, Segmentation, Digital computing or data processing equipment or methods, specially adapted for specific applications, Image data processing or generation, in general, mammography, computer-aided detection, morphological enhancement, SLIC, level set, support vector machine, Mammography, Cluster analysis, Testing procedures, Learning, Computer aided diagnosis, Medical image segmentation, Databases, Medical image contrast, Cancer},
doi = {https://doi.org/10.1118/1.4921612},
url = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1118/1.4921612},
eprint = {https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1118/1.4921612},
abstract = {Purpose: To develop a computer-aided detection (CAD) scheme for mass detection on digitized mammograms that achieves a high sensitivity while maintaining a low false positive (FP) rate using morphological enhancement and simple linear iterative clustering (SLIC) method. Methods: The authors developed a multiple stage method for breast mass detection. The proposed CAD scheme consists of five major components: (1) preprocessing based on morphological enhancement, which enhances mass-like patterns while removing unrelated background clutters, (2) segmentation of mass candidates based on the SLIC method, which groups mass and background tissue into different regions, (3) prescreening of suspicious regions using rule-based classification that eliminates regions unlikely to represent masses, (4) potential lesion contour refinement based on distance regularized level set evolution, and (5) FP reduction based on feature extraction and an ensemble of undersampled support vector machines. Two datasets were built to design and evaluate the system: a mass dataset containing 187 cases (386 mammograms) and a nonmass dataset containing 88 mammograms. All cases were acquired from the digital database for screening mammography (DDSM). Approximately two thirds of the available masses were used for training the system, and the remaining masses and nonmass dataset were used for testing. Results: Testing of the proposed CAD system on the mass dataset yielded a mass-based sensitivity of 98.55\%, 97.10\%, 92.75\% at 0.84, 0.63, 0.55 FP mark/image, respectively. Tested on the nonmass dataset, the scheme showed a FP rate of 0.55, 0.34, 0.30 mark/image. Conclusions: The results indicate that the system is promising in improving the performance of current CAD systems by reducing FP rate while achieving relatively high sensitivity.},
year = {2015}
}

@article{MOREIRA2012236,
title = "INbreast: Toward a Full-field Digital Mammographic Database",
journal = "Academic Radiology",
volume = "19",
number = "2",
pages = "236 - 248",
year = "2012",
issn = "1076-6332",
doi = "https://doi.org/10.1016/j.acra.2011.09.014",
url = "http://www.sciencedirect.com/science/article/pii/S107663321100451X",
author = "Inês C. Moreira and Igor Amaral and Inês Domingues and António Cardoso and Maria João Cardoso and Jaime S. Cardoso",
keywords = "Mammographic database, CAD, computer-aided detection, computer-aided diagnosis",
abstract = "Rationale and Objectives
Computer-aided detection and diagnosis (CAD) systems have been developed in the past two decades to assist radiologists in the detection and diagnosis of lesions seen on breast imaging exams, thus providing a second opinion. Mammographic databases play an important role in the development of algorithms aiming at the detection and diagnosis of mammary lesions. However, available databases often do not take into consideration all the requirements needed for research and study purposes. This article aims to present and detail a new mammographic database.
Materials and Methods
Images were acquired at a breast center located in a university hospital (Centro Hospitalar de S. João [CHSJ], Breast Centre, Porto) with the permission of the Portuguese National Committee of Data Protection and Hospital's Ethics Committee. MammoNovation Siemens full-field digital mammography, with a solid-state detector of amorphous selenium was used.
Results
The new database—INbreast—has a total of 115 cases (410 images) from which 90 cases are from women with both breasts affected (four images per case) and 25 cases are from mastectomy patients (two images per case). Several types of lesions (masses, calcifications, asymmetries, and distortions) were included. Accurate contours made by specialists are also provided in XML format.
Conclusion
The strengths of the actually presented database—INbreast—relies on the fact that it was built with full-field digital mammograms (in opposition to digitized mammograms), it presents a wide variability of cases, and is made publicly available together with precise annotations. We believe that this database can be a reference for future works centered or related to breast cancer imaging."
}

@INPROCEEDINGS{7813261,
  author={A. {Qayyum} and A. {Basit}},
  booktitle={2016 International Conference on Emerging Technologies (ICET)}, 
  title={Automatic breast segmentation and cancer detection via SVM in mammograms}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICET.2016.7813261}
}

@inproceedings{10.1145/3313831.3376219,
author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},
title = {Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376219},
doi = {10.1145/3313831.3376219},
abstract = {Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists' use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists' mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our findings in the social science literature.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {interpretability, user-centric evaluation, machine learning},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@misc{calisto2019itmedex,
  doi = {10.13140/RG.2.2.30479.43682},
  url = {http://rgdoi.net/10.13140/RG.2.2.30479.43682},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {IT-MEDEX Closing Workshop: Towards Touch-Based Medical Image Diagnosis Annotation},
  publisher = {ResearchGate},
  year = {2019}
}

@article{doi:10.1148/radiol.2016150409,
author = {Kelly, Brendan S. and Rainford, Louise A. and Darcy, Sarah P. and Kavanagh, Eoin C. and Toomey, Rachel J.},
title = {The Development of Expertise in Radiology: In Chest Radiograph Interpretation, "Expert" Search Pattern May Predate "Expert" Levels of Diagnostic Accuracy for Pneumothorax Identification},
journal = {Radiology},
volume = {280},
number = {1},
pages = {252-260},
year = {2016},
doi = {10.1148/radiol.2016150409},
note ={PMID: 27322975},
URL = {https://doi.org/10.1148/radiol.2016150409},
eprint = {https://doi.org/10.1148/radiol.2016150409}
}

@misc{lencastre2020mvf,
  doi = {10.13140/RG.2.2.28345.52322},
  url = {http://rgdoi.net/10.13140/RG.2.2.28345.52322},
  author = {Lencastre,  Hugo and Calisto,  Francisco Maria and Nascimento,  Jacinto C.},
  language = {en},
  title = {3D Module View Feature},
  publisher = {ResearchGate},
  year = {2020}
}

@techreport{calisto2015aqmgasa,
  doi = {10.13140/RG.2.1.1905.9283},
  url = {http://rgdoi.net/10.13140/RG.2.1.1905.9283},
  author = {Oliveira, Bruno and Calisto,  Francisco Maria and Borbinha, Jos\'{e} and Gomes, Lilian},
  language = {en},
  title = {Adaptive Q-Sort Matrix Generation: A Simplified Approach},
  institution  = {INESC-ID},
  year = {2015}
}

@article{wei2019medical,
  title={Medical Hyperspectral Image Classification Based on End-to-End Fusion Deep Neural Network},
  author={Wei, Xueling and Li, Wei and Zhang, Mengmeng and Li, Qingli},
  journal={IEEE Transactions on Instrumentation and Measurement},
  year={2019},
  publisher={IEEE}
}

@article{liu2019sdfn,
  title={SDFN: Segmentation-based Deep Fusion Network for Thoracic Disease Classification in Chest X-ray Images},
  author={Liu, Han and Wang, Lei and Nan, Yandong and Jin, Faguang and Wang, Qi and Pu, Jiantao},
  journal={Computerized Medical Imaging and Graphics},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{zhao2019data,
  title={Data augmentation using learned transformations for one-shot medical image segmentation},
  author={Zhao, Amy and Balakrishnan, Guha and Durand, Fredo and Guttag, John V and Dalca, Adrian V},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8543--8553},
  year={2019}
}

@inproceedings{zhou2019collaborative,
  title={Collaborative Learning of Semi-Supervised Segmentation and Classification for Medical Images},
  author={Zhou, Yi and He, Xiaodong and Huang, Lei and Liu, Li and Zhu, Fan and Cui, Shanshan and Shao, Ling},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2079--2088},
  year={2019}
}

@article{philbrick2019ril,
  title={RIL-Contour: a Medical Imaging Dataset Annotation Tool for and with Deep Learning},
  author={Philbrick, Kenneth A and Weston, Alexander D and Akkus, Zeynettin and Kline, Timothy L and Korfiatis, Panagiotis and Sakinis, Tomas and Kostandy, Petro and Boonrod, Arunnit and Zeinoddini, Atefeh and Takahashi, Naoki and others},
  journal={Journal of digital imaging},
  pages={1--11},
  year={2019},
  publisher={Springer}
}

@article{huang2019diagnosis,
  title={Diagnosis of Alzheimer’s Disease via Multi-modality 3D Convolutional Neural Network},
  author={Huang, Yechong and Xu, Jiahang and Zhou, Yuncheng and Tong, Tong and Zhuang, Xiahai and Alzheimer’s Disease Neuroimaging Initiative (ADNI and others},
  journal={Frontiers in Neuroscience},
  volume={13},
  year={2019},
  publisher={Frontiers Media SA}
}

@article{soffer2019convolutional,
  title={Convolutional neural networks for radiologic images: a radiologist’s guide},
  author={Soffer, Shelly and Ben-Cohen, Avi and Shimon, Orit and Amitai, Michal Marianne and Greenspan, Hayit and Klang, Eyal},
  journal={Radiology},
  volume={290},
  number={3},
  pages={590--606},
  year={2019},
  publisher={Radiological Society of North America}
}

@article{gotz2019mitk,
  title={MITK Phenotyping: An open-source toolchain for image-based personalized medicine with radiomics},
  author={G{\"o}tz, Michael and Nolden, Marco and Maier-Hein, Klaus},
  journal={Radiotherapy and Oncology},
  volume={131},
  pages={108--111},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{wels2019general,
  title={General purpose radiomics for multi-modal clinical research},
  author={Wels, M. G and Lades, F. and Muehlberg, A. and Suehling, M.},
  booktitle={Medical Imaging 2019: Computer-Aided Diagnosis},
  volume={10950},
  pages={1095046},
  year={2019},
  organization={Intern. Soc. for Optics and Photonics}
}

@article{smailagic2019medal,
  title={O-MedAL: Online Active Deep Learning for Medical Image Analysis},
  author={Smailagic, Asim and Costa, Pedro and Gaudio, Alex and Khandelwal, Kartik and Mirshekari, Mostafa and Fagert, Jonathon and Walawalkar, Devesh and Xu, Susu and Galdran, Adrian and Zhang, Pei and others},
  journal={arXiv preprint arXiv:1908.10508},
  year={2019}
}

@article{KOUROU20158,
title = "Machine learning applications in cancer prognosis and prediction",
journal = "Computational and Structural Biotechnology Journal",
volume = "13",
pages = "8 - 17",
year = "2015",
issn = "2001-0370",
doi = "https://doi.org/10.1016/j.csbj.2014.11.005",
url = "http://www.sciencedirect.com/science/article/pii/S2001037014000464",
author = "Konstantina Kourou and Themis P. Exarchos and Konstantinos P. Exarchos and Michalis V. Karamouzis and Dimitrios I. Fotiadis",
keywords = "Machine learning, Cancer susceptibility, Predictive models, Cancer recurrence, Cancer survival",
abstract = "Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diagnosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent clinical management of patients. The importance of classifying cancer patients into high or low risk groups has led many research teams, from the biomedical and the bioinformatics field, to study the application of machine learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and treatment of cancerous conditions. In addition, the ability of ML tools to detect key features from complex datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied in cancer research for the development of predictive models, resulting in effective and accurate decision making. Even though it is evident that the use of ML methods can improve our understanding of cancer progression, an appropriate level of validation is needed in order for these methods to be considered in the everyday clinical practice. In this work, we present a review of recent ML approaches employed in the modeling of cancer progression. The predictive models discussed here are based on various supervised ML techniques as well as on different input features and data samples. Given the growing trend on the application of ML methods in cancer research, we present here the most recent publications that employ these techniques as an aim to model cancer risk or patient outcomes."
}

@article{https://doi.org/10.1111/cen.12731,
author = {Puig Domingo, Manuel},
title = {Treatment of acromegaly in the era of personalized and predictive medicine},
journal = {Clinical Endocrinology},
volume = {83},
number = {1},
pages = {3-14},
doi = {https://doi.org/10.1111/cen.12731},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cen.12731},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cen.12731},
abstract = {Summary Acromegaly is a chronic disorder usually diagnosed late in the disease evolution. Such delayed diagnosis, together with the inability to achieve the treatment goals of normalizing biochemical disease markers and controlling tumour mass may result in substantial morbidity and mortality. Somatostatin analogues (SSA) are accepted as first-line medical therapy or as second-line therapy in patients undergoing unsuccessful surgery and are considered a cornerstone in the treatment of acromegaly. However, because a high percentage of patients experience SSA medical treatment failure, the identification of biomarkers associated with a successful or unsuccessful response to all classes of medical therapy would help in the choice of treatment and potentially allow for a quicker normalization of biochemical parameters. The current treatment algorithms for acromegaly are based upon a “trial-and-error” approach with additional treatment options provided when disease is not controlled. In many other diseases, therapeutic algorithms have been evolving towards personalized treatment with the medication that best matches individual disease characteristics, using biomarkers that identify therapeutic response. Additionally, a personalized approach to complementary treatment of comorbidities present in the acromegalic patient is also required. This review will discuss the development of a potential treatment algorithm for acromegaly addressing the biochemical control of the disease as well of its associated comorbidities, under a personalized approach based upon markers of prognostic and predictive significance, such as tumour size, MRI adenoma signal, GH value after acute octreotide test, granular adenoma pattern, Ki-67, somatostatin receptor phenotype, aryl hydrocarbon-interacting protein expression, gsp mutations, RAF kinase activity, E-cadherin and beta-arrestin-1.},
year = {2015}
}

@article{antonanzas2015some,
  title={Some economics on personalized and predictive medicine},
  author={Anto{\~n}anzas, F and Ju{\'a}rez-Castell{\'o}, CA and Rodr{\'\i}guez-Ibeas, R},
  journal={The European journal of health economics},
  volume={16},
  number={9},
  pages={985--994},
  year={2015},
  publisher={Springer}
}

@article{hood2011predictive,
  title={Predictive, personalized, preventive, participatory (P4) cancer medicine},
  author={Hood, Leroy and Friend, Stephen H},
  journal={Nature reviews Clinical oncology},
  volume={8},
  number={3},
  pages={184--187},
  year={2011},
  publisher={Nature Publishing Group}
}

@inproceedings{10.1145/3313831.3376710,
author = {Laschke, Matthias and Braun, Christoph and Neuhaus, Robin and Hassenzahl, Marc},
title = {Meaningful Technology at Work - A Reflective Design Case of Improving Radiologists' Wellbeing Through Medical Technology},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376710},
doi = {10.1145/3313831.3376710},
abstract = {In radiology, medical technology providers (MTP) focus mainly on technology-related issues, such as image quality or efficiency of reporting. Broader notions of radiology as "meaningful work" are largely seen as out of scope for an MTP. The present paper challenges this. In a real-world case with a large MTP, we showed that medical technology could be designed more holistically to explicitly improve radiologists' wellbeing. We first gathered work practices experienced as especially conducive to wellbeing. From there, we distilled ideal practices to increase wellbeing and turned them into two software applications. The MTP's initial skepticism dissolved, while radiologists unanimously emphasized wellbeing and demonstrated how they work towards improving it. Based on our insights, the applications resonated well among the radiologists involved, the healthcare provider, and other customers of the MTP. We close with a critical reflection of the challenges and opportunities of designing wellbeing-driven technology in the work domain.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {job design, wellbeing-driven design, technology at work, practice-based},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@InProceedings{10.1007/978-3-030-59725-2_21,
author="Sargent, Dustin
and Park, Sun Young
and Jog, Amod
and Mohamed, Aly
and Richmond, David",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="Multi-site Evaluation of a Study-Level Classifier for Mammography Using Deep Learning",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="211--219",
abstract="We present a computer-aided diagnosis algorithm for mammography trained and validated on studies acquired from six clinical sites. We hold out the full dataset from a seventh hospital for testing to assess the algorithm's ability to generalize to new sites. Our classifiers are convolutional neural networks that take multiple input images from a mammography study and produce classifications for the study. The studies are globally labeled as normal, biopsy benign, high risk or biopsy malignant. We report on experimental results from several network variants, including study-level and breast-level models, single- and multiple-output models, and a novel model architecture that incorporates prior studies. Each model variation includes an image-level classifier that is pre-trained with per-image labels and is used as a feature extractor in our study-level models. Our best study-level model achieves 0.85 area under the ROC curve for normal vs malignant classification on the held-out test site. In comparison with other recent work, we achieve a similar level of classification sensitivity and specificity on a dataset with greater site and vendor variation. Additionally, our test performance is demonstrated on a held-out site to more accurately assess how the model would perform when deployed in the field.",
isbn="978-3-030-59725-2"
}

@ARTICLE{10.3389/fphy.2018.00051,
  
AUTHOR={Papp, Laszlo and Spielvogel, Clemens P. and Rausch, Ivo and Hacker, Marcus and Beyer, Thomas},   
	 
TITLE={Personalizing Medicine Through Hybrid Imaging and Medical Big Data Analysis},      
	
JOURNAL={Frontiers in Physics},      
	
VOLUME={6},      

PAGES={51},     
	
YEAR={2018},      
	  
URL={https://www.frontiersin.org/article/10.3389/fphy.2018.00051},       
	
DOI={10.3389/fphy.2018.00051},      
	
ISSN={2296-424X},   
   
ABSTRACT={Medical imaging has evolved from a pure visualization tool to representing a primary source of analytic approaches toward in vivo disease characterization. Hybrid imaging is an integral part of this approach, as it provides complementary visual and quantitative information in the form of morphological and functional insights into the living body. As such, non-invasive imaging modalities no longer provide images only, but data, as stated recently by pioneers in the field. Today, such information, together with other, non-imaging medical data creates highly heterogeneous data sets that underpin the concept of medical big data. While the exponential growth of medical big data challenges their processing, they inherently contain information that benefits a patient-centric personalized healthcare. Novel machine learning approaches combined with high-performance distributed cloud computing technologies help explore medical big data. Such exploration and subsequent generation of knowledge require a profound understanding of the technical challenges. These challenges increase in complexity when employing hybrid, aka dual- or even multi-modality image data as input to big data repositories. This paper provides a general insight into medical big data analysis in light of the use of hybrid imaging information. First, hybrid imaging is introduced (see further contributions to this special Research Topic), also in the context of medical big data, then the technological background of machine learning as well as state-of-the-art distributed cloud computing technologies are presented, followed by the discussion of data preservation and data sharing trends. Joint data exploration endeavors in the context of in vivo radiomics and hybrid imaging will be presented. Standardization challenges of imaging protocol, delineation, feature engineering, and machine learning evaluation will be detailed. Last, the paper will provide an outlook into the future role of hybrid imaging in view of personalized medicine, whereby a focus will be given to the derivation of prediction models as part of clinical decision support systems, to which machine learning approaches and hybrid imaging can be anchored.}
}

@article{JALALIAN2013420,
title = "Computer-aided detection/diagnosis of breast cancer in mammography and ultrasound: a review",
journal = "Clinical Imaging",
volume = "37",
number = "3",
pages = "420 - 426",
year = "2013",
issn = "0899-7071",
doi = "https://doi.org/10.1016/j.clinimag.2012.09.024",
url = "http://www.sciencedirect.com/science/article/pii/S0899707112002938",
author = "Afsaneh Jalalian and Syamsiah B.T. Mashohor and Hajjah Rozi Mahmud and M. Iqbal B. Saripan and Abdul Rahman B. Ramli and Babak Karasfi",
keywords = "Computer-aided detection, Computer-aided diagnosis, Breast cancer, Mammography, Ultrasound",
abstract = "Breast cancer is the most common form of cancer among women worldwide. Early detection of breast cancer can increase treatment options and patients' survivability. Mammography is the gold standard for breast imaging and cancer detection. However, due to some limitations of this modality such as low sensitivity especially in dense breasts, other modalities like ultrasound and magnetic resonance imaging are often suggested to achieve additional information. Recently, computer-aided detection or diagnosis (CAD) systems have been developed to help radiologists in order to increase diagnosis accuracy. Generally, a CAD system consists of four stages: (a) preprocessing, (b) segmentation of regions of interest, (c) feature extraction and selection, and finally (d) classification. This paper presents the approaches which are applied to develop CAD systems on mammography and ultrasound images. The performance evaluation metrics of CAD systems are also reviewed."
}

@InProceedings{10.1007/978-3-319-59050-9_28,
author="Li, Wenqi
and Wang, Guotai
and Fidon, Lucas
and Ourselin, Sebastien
and Cardoso, M. Jorge
and Vercauteren, Tom",
editor="Niethammer, Marc
and Styner, Martin
and Aylward, Stephen
and Zhu, Hongtu
and Oguz, Ipek
and Yap, Pew-Thian
and Shen, Dinggang",
title="On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task",
booktitle="Information Processing in Medical Imaging",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="348--360",
abstract="Deep convolutional neural networks are powerful tools for learning visual representations from images. However, designing efficient deep architectures to analyse volumetric medical images remains challenging. This work investigates efficient and flexible elements of modern convolutional networks such as dilated convolution and residual connection. With these essential building blocks, we propose a high-resolution, compact convolutional network for volumetric image segmentation. To illustrate its efficiency of learning 3D representation from large-scale image data, the proposed network is validated with the challenging task of parcellating 155 neuroanatomical structures from brain MR images. Our experiments show that the proposed network architecture compares favourably with state-of-the-art volumetric segmentation networks while being an order of magnitude more compact. We consider the brain parcellation task as a pretext task for volumetric image segmentation; our trained network potentially provides a good starting point for transfer learning. Additionally, we show the feasibility of voxel-level uncertainty estimation using a sampling approximation through dropout.",
isbn="978-3-319-59050-9"
}

@misc{nadia2020maivell,
  doi = {10.13140/RG.2.2.26143.51365},
  url = {http://rgdoi.net/10.13140/RG.2.2.26143.51365},
  author = {Mour{\~a}o, N{\'a}dia Liladar and Calisto,  Francisco Maria and Nascimento,  Jacinto C.},
  language = {en},
  title = {MIMBCD-UI: AI Visual Explanation - Label Lesion},
  publisher = {ResearchGate},
  year = {2020}
}

@article{urban2017lesiontracker,
  title={LesionTracker: Extensible open-source zero-footprint web viewer for cancer imaging research and clinical trials},
  author={Urban, Trinity and Ziegler, Erik and Lewis, Rob and Hafey, Chris and Sadow, Cheryl and Van den Abbeele, Annick D and Harris, Gordon J},
  journal={Cancer research},
  volume={77},
  number={21},
  pages={e119--e122},
  year={2017},
  publisher={AACR}
}

@Article{Jodogne2018,
  author="Jodogne, S{\'e}bastien",
  title="The {O}rthanc Ecosystem for Medical Imaging",
  journal="Journal of Digital Imaging",
  year="2018",
  month="Jun",
  day="01",
  volume="31",
  number="3",
  pages="341--352",
  issn="1618-727X",
  doi="10.1007/s10278-018-0082-y",
  url="https://doi.org/10.1007/s10278-018-0082-y"
}

@inproceedings{6556444,
author={S. {Jodogne} and C. {Bernard} and M. {Devillers} and E. {Lenaerts} and P. {Coucke}},
booktitle={2013 IEEE 10th International Symposium on Biomedical Imaging},
title={Orthanc - A lightweight, restful DICOM server for healthcare and medical research},
year={2013},
volume={},
number={},
pages={190-193},
abstract={Is this paper, the Orthanc open-source software is introduced. Orthanc is a lightweight, yet powerful standalone DICOM store for healthcare and medical research. Multiple instances of Orthanc can easily be deployed in the hospital network or even in the same computer, which simplifies the interconnection between the DICOM modalities and the data management of medical images. Orthanc is unique with respect to the fact that it provides a modern RESTful API: Orthanc can be driven from any computer language to automate clinical processes. Finally, Orthanc comes bundled with an embedded Web interface that allows the end-users to browse and interact with the content of the DICOM store.},
keywords={application program interfaces;embedded systems;graphical user interfaces;health care;Internet;medical computing;public domain software;Orthanc open-source software;DICOM server;healthcare;medical research;RESTful API;RESTful Application Programming Interface;DICOM modalities;data management;medical images;embedded Web interface;DICOM store;DICOM;Hospitals;Computers;Servers;Software;Protocols;DICOM Store;Scripting;REST},
doi={10.1109/ISBI.2013.6556444},
ISSN={1945-8452},
month={April},}

@misc{calisto2019micpuw,
  doi = {10.13140/RG.2.2.27402.93126},
  url = {http://rgdoi.net/10.13140/RG.2.2.27402.93126},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Medical Imaging Cornerstone Prototype User Workflow},
  publisher = {ResearchGate},
  year = {2019}
}

@article{HOSTETTER2018811,
title = "Integration of a Zero-footprint Cloud-based Picture Archiving and Communication System with Customizable Forms for Radiology Research and Education",
journal = "Academic Radiology",
volume = "25",
number = "6",
pages = "811 - 818",
year = "2018",
note = "Education Issue",
issn = "1076-6332",
doi = "https://doi.org/10.1016/j.acra.2018.01.031",
url = "http://www.sciencedirect.com/science/article/pii/S1076633218300710",
author = "Jason Hostetter and Nishanth Khanna and Jacob C. Mandell",
keywords = "Cloud-based PACS, web-based forms, educational case file, multireader study",
abstract = "Rationale and Objectives
The purpose of this study was to integrate web-based forms with a zero-footprint cloud-based Picture Archiving and Communication Systems (PACS) to create a tool of potential benefit to radiology research and education.
Materials and Methods
Web-based forms were created with a front-end and back-end architecture utilizing common programming languages including Vue.js, Node.js and MongoDB, and integrated into an existing zero-footprint cloud-based PACS.
Results
The web-based forms application can be accessed in any modern internet browser on desktop or mobile devices and allows the creation of customizable forms consisting of a variety of questions types. Each form can be linked to an individual DICOM examination or a collection of DICOM examinations.
Conclusions
Several uses are demonstrated through a series of case studies, including implementation of a research platform for multi-reader multi-case (MRMC) studies and other imaging research, and creation of an online Objective Structure Clinical Examination (OSCE) and an educational case file."
}

@misc{https://doi.org/10.13140/rg.2.2.13834.62405,
  doi = {10.13140/RG.2.2.13834.62405},
  url = {http://rgdoi.net/10.13140/RG.2.2.13834.62405},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Medical Imaging Solution Components Schematic Demonstration},
  publisher = {ResearchGate},
  year = {2020}
}

@inproceedings{10.1117/12.2513004,
author = {A. Sedghi and S. Hamidi and A. Mehrtash and E. Ziegler and C. Tempany and S. Pieper and T.  Kapur and P. Mousavi},
title = {{Tesseract-medical imaging: open-source browser-based platform for artificial intelligence deployment in medical imaging}},
volume = {10951},
booktitle = {Medical Imaging 2019: Image-Guided Procedures, Robotic Interventions, and Modeling},
editor = {B. Fei and C. A. Linte},
organization = {Intern. Soc. for Optics and Photonics},
publisher = {SPIE},
pages = {446 -- 451},
abstract = {Artificial Intelligence (AI) is increasingly becoming a tool to enhance various medical image analysis tasks with accuracies comparable to expert clinicians. Computer assisted detection and diagnosis, and image segmentation and registration have significantly benefited from AI. However, integration of AI into the clinical workflow has been slow due to requirements for libraries that are specific to each model, and also environments that are specific to clinical centers. These challenges demonstrate the need for an AI-based solution that can be integrated into any environment with minimum hardware and software overhead. Tesseract-Medical Imaging (Tesseract-MI) is an open-source, web-based platform which enables deployment of AI models while simultaneously providing standard image viewing and reporting schemes. The goal of Tesseract-MI is to augment 3D medical imaging and provide a 4<sup>th</sup> dimension (AI) when requested by a user. As a case study, we demonstrate the utility of our platform and present ProstateCancer.ai, a web application for identification of clinically significant prostate cancer in MRI.},
keywords = {Artificial Intelligence, Deployment , Clinical workflow, Integration, Deep learning, Platform, Viewer, Prostate Cancer},
year = {2019},
doi = {10.1117/12.2513004},
URL = {https://doi.org/10.1117/12.2513004}
}

@misc{https://doi.org/10.13140/rg.2.2.33967.28323,
  doi = {10.13140/RG.2.2.33967.28323},
  url = {http://rgdoi.net/10.13140/RG.2.2.33967.28323},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Annotations File Generation Schematic Diagram},
  publisher = {ResearchGate},
  year = {2020}
}

@misc{lencastre2020msia,
  doi = {10.13140/RG.2.2.16601.47209},
  url = {http://rgdoi.net/10.13140/RG.2.2.16601.47209},
  author = {Lencastre,  Hugo and Calisto,  Francisco Maria and Nascimento,  Jacinto C.},
  language = {en},
  title = {MIMBCD-UI: Scalable Interactions Architecture},
  publisher = {ResearchGate},
  year = {2020}
}

@article{vayena2018machine,
  title={Machine learning in medicine: addressing ethical challenges},
  author={Vayena, Effy and Blasimme, Alessandro and Cohen, I Glenn},
  journal={PLoS medicine},
  volume={15},
  number={11},
  year={2018},
  publisher={Public Library of Science}
}

@article{ghahramani2015probabilistic,
  title={Probabilistic machine learning and artificial intelligence},
  author={Ghahramani, Zoubin},
  journal={Nature},
  volume={521},
  number={7553},
  pages={452--459},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{choy2018current,
  title={Current applications and future impact of machine learning in radiology},
  author={Choy, Garry and Khalilzadeh, Omid and Michalski, Mark and Do, Synho and Samir, Anthony E and Pianykh, Oleg S and Geis, J Raymond and Pandharipande, Pari V and Brink, James A and Dreyer, Keith J},
  journal={Radiology},
  volume={288},
  number={2},
  pages={318--328},
  year={2018},
  publisher={Radiological Society of North America}
}

@article{hosny2018artificial,
  title={Artificial intelligence in radiology},
  author={Hosny, Ahmed and Parmar, Chintan and Quackenbush, John and Schwartz, Lawrence H and Aerts, Hugo JWL},
  journal={Nature Reviews Cancer},
  volume={18},
  number={8},
  pages={500},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{kooi2017large,
  title={Large scale deep learning for computer aided detection of mammographic lesions},
  author={Kooi, Thijs and Litjens, Geert and Van Ginneken, Bram and Gubern-M{\'e}rida, Albert and S{\'a}nchez, Clara I and Mann, Ritse and den Heeten, Ard and Karssemeijer, Nico},
  journal={Medical image analysis},
  volume={35},
  pages={303--312},
  year={2017},
  publisher={Elsevier}
}

@article{graffy2019automated,
  title={Automated segmentation and quantification of aortic calcification at abdominal CT: application of a deep learning-based algorithm to a longitudinal screening cohort},
  author={Graffy, Peter M and Liu, Jiamin and O’Connor, Stacy and Summers, Ronald M and Pickhardt, Perry J},
  journal={Abdominal Radiology},
  pages={1--8},
  year={2019},
  publisher={Springer}
}

@article{lakhani2017deep,
  title={Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks},
  author={Lakhani, Paras and Sundaram, Baskaran},
  journal={Radiology},
  volume={284},
  number={2},
  pages={574--582},
  year={2017},
  publisher={Radiological Society of North America}
}

@article{liang2019deep,
  title={Deep-learning-based detection and segmentation of organs at risk in nasopharyngeal carcinoma computed tomographic images for radiotherapy planning},
  author={Liang, Shujun and Tang, Fan and Huang, Xia and Yang, Kaifan and Zhong, Tao and Hu, Runyue and Liu, Shangqing and Yuan, Xinrui and Zhang, Yu},
  journal={European radiology},
  volume={29},
  number={4},
  pages={1961--1967},
  year={2019},
  publisher={Springer}
}

@Article{MAICAS2019101562,
  author    = {Gabriel Maicas and Andrew P. Bradley and Jacinto C. Nascimento and Ian Reid and Gustavo Carneiro},
  title     = {Pre and post-hoc diagnosis and interpretation of malignancy from breast DCE-MRI},
  doi       = {https://doi.org/10.1016/j.media.2019.101562},
  issn      = {1361-8415},
  pages     = {101562},
  url       = {http://www.sciencedirect.com/science/article/pii/S1361841518306893},
  volume    = {58},
  abstract  = {We propose a new method for breast cancer screening from DCE-MRI based on a post-hoc approach that is trained using weakly annotated data (i.e., labels are available only at the image level without any lesion delineation). Our proposed post-hoc method automatically diagnosis the whole volume and, for positive cases, it localizes the malignant lesions that led to such diagnosis. Conversely, traditional approaches follow a pre-hoc approach that initially localises suspicious areas that are subsequently classified to establish the breast malignancy – this approach is trained using strongly annotated data (i.e., it needs a delineation and classification of all lesions in an image). We also aim to establish the advantages and disadvantages of both approaches when applied to breast screening from DCE-MRI. Relying on experiments on a breast DCE-MRI dataset that contains scans of 117 patients, our results show that the post-hoc method is more accurate for diagnosing the whole volume per patient, achieving an AUC of 0.91, while the pre-hoc method achieves an AUC of 0.81. However, the performance for localising the malignant lesions remains challenging for the post-hoc method due to the weakly labelled dataset employed during training.},
  journal   = {Medical Image Analysis},
  keywords  = {Magnetic resonance imaging, Breast screening, Meta-learning, Few-shot learning, Weakly supervised learning, Strongly supervised learning, Model interpretation, Lesion detection, Deep reinforcement learning},
  publisher = {Elsevier},
  year      = {2019},
}

@Conference{calisto2019midaaiarfuv,
  author    = {Calisto, Francisco Maria and Lencastre, Hugo and Nunes, Nuno Jardim and Nascimento, Jacinto C.},
  booktitle = {Keep In Touch 2019},
  title     = {Medical Imaging Diagnosis Assistant: AI-Assisted Radiomics Framework User Validation},
  doi       = {10.13140/RG.2.2.33421.59360},
  language  = {en},
  location  = {Lisbon, Portugal},
  pages     = {1--2},
  publisher = {Instituto Superior T\'{e}cnico},
  series    = {KIT '19},
  url       = {http://rgdoi.net/10.13140/RG.2.2.33421.59360},
  address   = {Avenida Rovisco Pais 1, 1049-001 Lisboa - Portugal (EU)},
  journal   = {University of Lisbon},
  year      = {2019},
}

@article{Ruddle:2016:DEI:2872314.2834117,
 author = {Ruddle, Roy A. and Thomas, Rhys G. and Randell, Rebecca and Quirke, Philip and Treanor, Darren},
 title = {The Design and Evaluation of Interfaces for Navigating Gigapixel Images in Digital Pathology},
 journal = {ACM Trans. Comput.-Hum. Interact.},
 issue_date = {February 2016},
 volume = {23},
 number = {1},
 month = jan,
 year = {2016},
 issn = {1073-0516},
 pages = {5:1--5:29},
 articleno = {5},
 numpages = {29},
 url = {http://doi.acm.org/10.1145/2834117},
 doi = {10.1145/2834117},
 acmid = {2834117},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Gigapixel images, navigation, overview+detail, pathology, zoomable user interface},
}

@article{Park:2015:TOA:2737795.2656213,
 author = {Park, Sun Young and Chen, Yunan and Rudkin, Scott},
 title = {Technological and Organizational Adaptation of EMR Implementation in an Emergency Department},
 journal = {ACM Trans. Comput.-Hum. Interact.},
 issue_date = {March 2015},
 volume = {22},
 number = {1},
 month = feb,
 year = {2015},
 issn = {1073-0516},
 pages = {1:1--1:24},
 articleno = {1},
 numpages = {24},
 url = {http://doi.acm.org/10.1145/2656213},
 doi = {10.1145/2656213},
 acmid = {2656213},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Electronic Medical Record (EMR), adaptation, clinical practices, design, implementations, workaround},
}

@article{aerts2017data,
  title={{Data science in radiology: a path forward}},
  author={Aerts, Hugo JWL},
  journal={Clinical Cancer Research},
  pages={2804},
  year={2017},
  publisher={AACR},
  number={3},
  volume={24}
}

@article{ker2018deep,
  title={{Deep learning applications in medical image analysis}},
  author={Ker, Justin and Wang, Lipo and Rao, Jai and Lim, Tchoyoson},
  journal={Access},
  volume={6},
  pages={9375--9389},
  year={2018},
  publisher={IEEE}
}

@inproceedings{Sultanum:2018:MTP:3173574.3173996,
 author={Sultanum, Nicole and Brudno, Michael and Wigdor, Daniel and Chevalier, Fanny},
 title={{More Text Please! Understanding and Supporting the Use of Visualization for Clinical Text Overview}},
 booktitle={Conf. Human Factors in Computing Systems (CHI)},
 year={2018},
 pages={1--13},
 volume={422},
 doi={10.1145/3173574.3173996},
 publisher={ACM},
 address={New York, NY, USA},
}

@article{desantis2016breast,
  title={Breast cancer statistics, 2015: Convergence of incidence rates between black and white women},
  author={DeSantis, Carol E and Fedewa, Stacey A and Goding Sauer, Ann and Kramer, Joan L and Smith, Robert A and Jemal, Ahmedin},
  journal={CA: a cancer journal for clinicians},
  volume={66},
  number={1},
  pages={31--42},
  year={2016},
  publisher={Wiley Online Library}
}

@article{torre2015global,
  title={Global cancer statistics, 2012},
  author={Torre, Lindsey A and Bray, Freddie and Siegel, Rebecca L and Ferlay, Jacques and Lortet-Tieulent, Joannie and Jemal, Ahmedin},
  journal={CA: a cancer journal for clinicians},
  volume={65},
  number={2},
  pages={87--108},
  year={2015},
  publisher={Wiley Online Library}
}

@article{welch2016breast,
  title={Breast-cancer tumor size, overdiagnosis, and mammography screening effectiveness},
  author={Welch, H Gilbert and Prorok, Philip C and O’Malley, A James and Kramer, Barnett S},
  journal={New England Journal of Medicine},
  volume={375},
  number={15},
  pages={1438--1447},
  year={2016},
  publisher={Mass Medical Soc}
}

@article{saadatmand2015influence,
  title={Influence of tumour stage at breast cancer detection on survival in modern times: population based study in 173 797 patients},
  author={Saadatmand, Sepideh and Bretveld, Reini and Siesling, Sabine and Tilanus-Linthorst, Madeleine MA},
  journal={Bmj},
  volume={351},
  pages={h4901},
  year={2015},
  publisher={British Medical Journal Publishing Group}
}

@article{Sarcevic:2012:TET:2240156.2240161,
 author = {Sarcevic, Aleksandra and Marsic, Ivan and Burd, Randal S.},
 title = {Teamwork Errors in Trauma Resuscitation},
 journal = {ACM Trans. Comput.-Hum. Interact.},
 issue_date = {July 2012},
 volume = {19},
 number = {2},
 month = jul,
 year = {2012},
 issn = {1073-0516},
 pages = {13:1--13:30},
 articleno = {13},
 numpages = {30},
 url = {http://doi.acm.org/10.1145/2240156.2240161},
 doi = {10.1145/2240156.2240161},
 acmid = {2240161},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Team errors, collocated teams, distributed cognition, healthcare, medical error, system requirements, trauma resuscitation},
}

@article{Lim:2019:DDI:3319806.3301427,
 author = {Lim, Bohyeon and Rogers, Yvonne and Sebire, Neil},
 title = {Designing to Distract: Can Interactive Technologies Reduce Visitor Anxiety in a Children\&\#x0027;s Hospital Setting\&\#x003F;},
 journal = {ACM Trans. Comput.-Hum. Interact.},
 issue_date = {April 2019},
 volume = {26},
 number = {2},
 month = apr,
 year = {2019},
 issn = {1073-0516},
 pages = {9:1--9:19},
 articleno = {9},
 numpages = {19},
 url = {http://doi.acm.org/10.1145/3301427},
 doi = {10.1145/3301427},
 acmid = {3301427},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Children\&\#x0027;s hospital, Distraction principle, Interactive floor displays, Public displays, Reception area},
}

@article{shin2016deep,
  title={Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning},
  author={Shin, Hoo-Chang and Roth, Holger R and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M},
  journal={IEEE transactions on medical imaging},
  volume={35},
  number={5},
  pages={1285--1298},
  year={2016},
  publisher={IEEE}
}

@article{carneiro2017automated,
  title={{Automated Analysis of Unregistered Multi-View Mammograms With Deep Learning}},
  author={Carneiro, Gustavo and Nascimento, Jacinto and Bradley, Andrew P},
  journal={Transactions on Medical Imaging},
  volume={36},
  number={11},
  pages={2355--2365},
  year={2017},
  publisher={IEEE}
}

@article{wang2016discrimination,
  title={Discrimination of breast cancer with microcalcifications on mammography by deep learning},
  author={Wang, Jinhua and Yang, Xi and Cai, Hongmin and Tan, Wanchang and Jin, Cangzheng and Li, Li},
  journal={Scientific reports},
  volume={6},
  pages={27327},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{becker2017deep,
  title={Deep learning in mammography: diagnostic accuracy of a multipurpose image analysis software in the detection of breast cancer},
  author={Becker, Anton S and Marcon, Magda and Ghafoor, Soleen and Wurnig, Moritz C and Frauenfelder, Thomas and Boss, Andreas},
  journal={Investigative radiology},
  volume={52},
  number={7},
  pages={434--440},
  year={2017},
  publisher={LWW}
}

@article{khan2019novel,
  title={A novel deep learning based framework for the detection and classification of breast cancer using transfer learning},
  author={Khan, SanaUllah and Islam, Naveed and Jan, Zahoor and Din, Ikram Ud and Rodrigues, Joel JP C},
  journal={Pattern Recognition Letters},
  volume={125},
  pages={1--6},
  year={2019},
  publisher={Elsevier}
}

@article{topol2019high,
  title={High-performance medicine: the convergence of human and artificial intelligence},
  author={Topol, Eric J},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={44},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{gale2017detecting,
  title={Detecting hip fractures with radiologist-level performance using deep neural networks},
  author={Gale, William and Oakden-Rayner, Luke and Carneiro, Gustavo and Bradley, Andrew P and Palmer, Lyle J},
  journal={arXiv preprint arXiv:1711.06504},
  year={2017}
}

@article{greenspan2016guest,
  title={{Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique}},
  author={Greenspan, Hayit and van Ginneken, Bram and Summers, Ronald M},
  journal={Trans. Medical Imaging},
  volume={35},
  number={5},
  pages={1153--1159},
  year={2016},
  publisher={IEEE}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{waite2017tired,
  title={Tired in the reading room: the influence of fatigue in radiology},
  author={Waite, Stephen and Kolla, Srinivas and Jeudy, Jean and Legasto, Alan and Macknik, Stephen L and Martinez-Conde, Susana and Krupinski, Elizabeth A and Reede, Deborah L},
  journal={Journal of the American College of Radiology},
  volume={14},
  number={2},
  pages={191--197},
  year={2017},
  publisher={Elsevier}
}

@article{chatelain2018evaluation,
  title={Evaluation of Gaze Tracking Calibration for Longitudinal Biomedical Imaging Studies},
  author={Chatelain, Pierre and Sharma, Harshita and Drukker, Lior and Papageorghiou, Aris T and Noble, J Alison},
  journal={IEEE transactions on cybernetics},
  number={99},
  pages={1--11},
  year={2018},
  publisher={IEEE}
}

@article{miglioretti2007radiologist,
  title={Radiologist characteristics associated with interpretive performance of diagnostic mammography},
  author={Miglioretti, Diana L and Smith-Bindman, Rebecca and Abraham, Linn and Brenner, R James and Carney, Patricia A and Bowles, Erin J Aiello and Buist, Diana SM and Elmore, Joann G},
  journal={Journal of the National Cancer Institute},
  volume={99},
  number={24},
  pages={1854--1863},
  year={2007},
  publisher={Oxford University Press}
}

@article{rosset2004osirix,
  title={{OsiriX: an open-source software for navigating in multidimensional DICOM images}},
  author={Rosset, Antoine and Spadola, Luca and Ratib, Osman},
  journal={Journal of Digital Imaging},
  volume={17},
  number={3},
  pages={205--216},
  year={2004},
  publisher={Springer}
}

@article{wolf2005medical,
  title={{The medical imaging interaction toolkit}},
  author={Wolf, Ivo and Vetter, Marcus and Wegner, Ingmar and B{\"o}ttger, Thomas and Nolden, Marco and Sch{\"o}binger, Max and Hastenteufel, Mark and Kunert, Tobias and Meinzer, Hans-Peter},
  journal={Medical Image Analysis},
  volume={9},
  number={6},
  pages={594--604},
  year={2005},
  publisher={Elsevier}
}

@article{weese2016four,
  title={{Four challenges in medical image analysis from an industrial perspective}},
  author={Weese, J{\"u}rgen and Lorenz, Cristian},
  journal={Medical Image Analysis},
  volume={33},
  pages={44--49},
  year={2016},
  publisher={Elsevier}
}

@article{heinrich2012mind,
  title={{MIND: Modality independent neighbourhood descriptor for multi-modal deformable registration}},
  author={Heinrich, Mattias P and Jenkinson, Mark and Bhushan, Manav and Matin, Tahreema and Gleeson, Fergus V and Brady, Michael and Schnabel, Julia A},
  journal={Medical Image Analysis},
  volume={16},
  number={7},
  pages={1423--1435},
  year={2012},
  publisher={Elsevier}
}

@article{sorace2018distinguishing,
  title={Distinguishing benign and malignant breast tumors: preliminary comparison of kinetic modeling approaches using multi-institutional dynamic contrast-enhanced MRI data from the International Breast MR Consortium 6883 trial},
  author={Sorace, Anna G and Partridge, Savannah C and Li, Xia and Virostko, Jack and Barnes, Stephanie L and Hippe, Daniel S and Huang, Wei and Yankeelov, Thomas E},
  journal={Journal of Medical Imaging},
  volume={5},
  number={1},
  pages={011019},
  year={2018},
  publisher={International Society for Optics and Photonics}
}

@article{mohamed2018deep,
  title={A deep learning method for classifying mammographic breast density categories},
  author={Mohamed, Aly A and Berg, Wendie A and Peng, Hong and Luo, Yahong and Jankowitz, Rachel C and Wu, Shandong},
  journal={Medical physics},
  volume={45},
  number={1},
  pages={314--321},
  year={2018},
  publisher={Wiley Online Library}
}

@inproceedings{aghaei2018association,
  title={{Association between background parenchymal enhancement of breast MRI and BIRADS rating change in the subsequent screening}},
  author={Aghaei, Faranak and Mirniaharikandehei, Seyedehnafiseh and Hollingsworth, Alan B and Stoug, Rebecca G and Pearce, Melanie and Liu, Hong and Zheng, Bin},
  booktitle={Medical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications},
  volume={10579},
  pages={1--8},
  year={2018},
  publisher={SPIE},
  address={Houston, Texas, United States}
}

@inproceedings{10.1145/3308558.3314123,
author = {Paranyushkin, Dmitry},
title = {InfraNodus: Generating Insight Using Text Network Analysis},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3314123},
doi = {10.1145/3308558.3314123},
abstract = {In this paper we present a web-based open source tool and a method for generating insight from any text or discourse using text network analysis. The tool (InfraNodus) can be used by researchers and writers to organize and to better understand their notes, to measure the level of bias in discourse, and to identify the parts of the discourse where there is a potential for insight and new ideas. The method is based on text network analysis algorithm, which represents any text as a network and identifies the most influential words in a discourse based on the terms' co-occurrence. Graph community detection algorithm is then applied in order to identify the different topical clusters, which represent the main topics in the text as well as the relations between them. The community structure is used in conjunction with other measures to identify the level of bias or cognitive diversity of the discourse. Finally, the structural gaps in the graph can indicate the parts of the discourse where the connections are lacking, therefore highlighting the areas where there's a potential for new ideas. The tool can be used as stand-alone software by end users as well as implemented via an API into other tools. Another interesting application is in the field of recommendation systems: structural gaps could indicate potentially interesting non-trivial connections to any connected datasets.},
booktitle = {The World Wide Web Conference},
pages = {3584–3589},
numpages = {6},
keywords = {comprehension, information interfaces, cognitive science, text mining, ideation, research, bias, graph theory, TNA, network topology, network analysis, discourse bias, mental maps, creativity, topic modelling, insight, text network analysis},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{mullie2019coreslicer,
  title={CoreSlicer: a web toolkit for analytic morphomics},
  author={Mullie, Louis and Afilalo, Jonathan},
  journal={BMC medical imaging},
  volume={19},
  number={1},
  pages={15},
  year={2019},
  publisher={Springer}
}

@techreport{https://doi.org/10.13140/rg.2.2.16566.14403/1,
  doi = {10.13140/RG.2.2.16566.14403/1},
  url = {http://rgdoi.net/10.13140/RG.2.2.16566.14403/1},
  author = {Calisto,  Francisco Maria},
  language = {en},
  title = {Assistant Introduction: User Testing Guide For A Comparison Between Multi-Modality and AI-Assisted Systems},
  publisher = {Madeira Interactive Technologies Institute},
  year = {2019},
  institution={Instituto Superior T\'{e}cnico}
}

@inproceedings{Tyllinen:2016:WNN:2858036.2858570,
 author = {Tyllinen, Mari and Kaipio, Johanna and L\"{a}\"{a}veri, Tinja and Nieminen, Marko H.T.},
 title = {We Need Numbers!: Heuristic Evaluation During Demonstrations (HED) for Measuring Usability in IT System Procurement},
 booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '16},
 year = {2016},
 isbn = {978-1-4503-3362-7},
 location = {San Jose, California, USA},
 pages = {4129--4141},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/2858036.2858570},
 doi = {10.1145/2858036.2858570},
 acmid = {2858570},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {electronic health record, healthcare and social welfare information system, measuring usability, public procurement, summative evaluation, usability evaluation},
}

@article{ramkumar2017using,
  title={{Using GOMS and NASA-TLX to Evaluate Human--Computer Interaction Process in Interactive Segmentation}},
  author={Ramkumar, Anjana and Stappers, Pieter Jan and Niessen, Wiro J and Adebahr, Sonja and Schimek-Jasch, Tanja and Nestle, Ursula and Song, Yu},
  journal={International Journal of Human-Computer Interaction},
  volume={33},
  number={2},
  pages={123--134},
  year={2017},
  publisher={Taylor \& Francis}
}

@inproceedings{grier2015high,
  title={How high is high? A meta-analysis of NASA-TLX global workload scores},
  author={Grier, Rebecca A},
  booktitle={Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume={59},
  number={1},
  pages={1727--1731},
  year={2015},
  organization={SAGE Publications Sage CA: Los Angeles, CA}
}

@misc{https://doi.org/10.13140/rg.2.2.25301.06883,
  doi = {10.13140/rg.2.2.25301.06883},
  url = {http://rgdoi.net/10.13140/RG.2.2.25301.06883},
  author = {Calisto,  Francisco Maria and Nascimento,  Jacinto C.},
  language = {en},
  title = {Medical Imaging Multimodality Breast Cancer Diagnosis User Interface: SUS Survey Template File},
  publisher = {ResearchGate},
  month = {April},
  year = {2018}
}

@misc{https://doi.org/10.13140/rg.2.2.26978.79044,
  doi = {10.13140/rg.2.2.26978.79044},
  url = {http://rgdoi.net/10.13140/RG.2.2.26978.79044},
  author = {Calisto,  Francisco Maria and Nascimento,  Jacinto C.},
  language = {en},
  title = {Medical Imaging Multimodality Breast Cancer Diagnosis User Interface: NASA-TLX Survey Template File},
  publisher = {ResearchGate},
  month = {April},
  year = {2018}
}

@misc{https://doi.org/10.13140/rg.2.2.36306.86725,
  doi = {10.13140/rg.2.2.36306.86725},
  url = {http://rgdoi.net/10.13140/RG.2.2.36306.86725},
  author = {Calisto,  Francisco Maria and Nascimento,  Jacinto C.},
  language = {en},
  title = {Medical Imaging Multimodality Breast Cancer Diagnosis User Interface: BIRADS Survey Template File},
  publisher = {ResearchGate},
  month = {May},
  year = {2018}
}

@misc{https://doi.org/10.13140/RG.2.2.23078.37448/1,
  doi = {10.13140/rg.2.2.23078.37448/1},
  url = {http://rgdoi.net/10.13140/RG.2.2.23078.37448/1},
  author = {Calisto,  Francisco Maria and Nunes, Nuno and Nascimento,  Jacinto C.},
  language = {en},
  title = {Medical Imaging Diagnosis Assistant: Dimensions Of Trust Scale (DOTS) Survey Template File},
  publisher = {ResearchGate},
  month = {May},
  year = {2019}
}

@article{liikkanen2017data,
  title={The data-driven design era in professional web design.},
  author={Liikkanen, Lassi A},
  journal={interactions},
  volume={24},
  number={5},
  pages={52--57},
  year={2017}
}

@article{lewis2018system,
  title={The system usability scale: past, present, and future},
  author={Lewis, James R},
  journal={International Journal of Human--Computer Interaction},
  volume={34},
  number={7},
  pages={577--590},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{Wobbrock:2011:ART:1978942.1978963,
 author = {Wobbrock, Jacob O. and Findlater, Leah and Gergle, Darren and Higgins, James J.},
 title = {The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only Anova Procedures},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 series = {CHI '11},
 year = {2011},
 isbn = {978-1-4503-0228-9},
 location = {Vancouver, BC, Canada},
 pages = {143--146},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1978942.1978963},
 doi = {10.1145/1978942.1978963},
 acmid = {1978963},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analysis of variance, anova, f-test, factorial analysis, nonparametric data, statistics},
}

@article{mathews2017usability,
  title={Usability evaluation of laboratory information systems},
  author={Mathews, Althea and Marc, David},
  journal={Journal of pathology informatics},
  volume={8},
  year={2017},
  publisher={Wolters Kluwer--Medknow Publications}
}

@inproceedings{10.1145/2858036.2858360,
author = {Zhang, Xiaoyi and Pina, Laura R. and Fogarty, James},
title = {Examining Unlock Journaling with Diaries and Reminders for In Situ Self-Report in Health and Wellness},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858360},
doi = {10.1145/2858036.2858360},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {5658–5664},
numpages = {7},
keywords = {personal informatics, experience sampling, self-tracking},
location = {San Jose, California, USA},
series = {CHI ’16}
}

@inproceedings{Harboe:2012:CSC:2145204.2145379,
 author = {Harboe, Gunnar and Minke, Jonas and Ilea, Ioana and Huang, Elaine M.},
 title = {Computer Support for Collaborative Data Analysis: Augmenting Paper Affinity Diagrams},
 booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
 series = {CSCW '12},
 year = {2012},
 isbn = {978-1-4503-1086-4},
 location = {Seattle, Washington, USA},
 pages = {1179--1182},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2145204.2145379},
 doi = {10.1145/2145204.2145379},
 acmid = {2145379},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affinity diagram, augmented paper, augmented reality},
}

@inproceedings{Hoiseth:2013:DHG:2485760.2485770,
 author = {H{\o}iseth, Marikken and Giannakos, Michail N. and Alsos, Ole A. and Jaccheri, Letizia and Asheim, Jonas},
 title = {Designing Healthcare Games and Applications for Toddlers},
 booktitle = {Proceedings of the 12th International Conference on Interaction Design and Children},
 series = {IDC '13},
 year = {2013},
 isbn = {978-1-4503-1918-8},
 location = {New York, New York, USA},
 pages = {137--146},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2485760.2485770},
 doi = {10.1145/2485760.2485770},
 acmid = {2485770},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design considerations, healthcare games, toddlers, young children},
}

@inproceedings{Hoiseth:2013:RGD:2468356.2468436,
 author = {H{\o}iseth, Marikken and Giannakos, Michail N. and Jaccheri, Letizia},
 title = {Research-derived Guidelines for Designing Toddlers' Healthcare Games},
 booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
 series = {CHI EA '13},
 year = {2013},
 isbn = {978-1-4503-1952-2},
 location = {Paris, France},
 pages = {451--456},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2468356.2468436},
 doi = {10.1145/2468356.2468436},
 acmid = {2468436},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {affinity diagrams, design guidelines, focus groups, healthcare games, toddlers},
}

@inproceedings{10.1145/3290605.3300628,
author = {Subramonyam, Hariharan and Drucker, Steven M. and Adar, Eytan},
title = {Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300628},
doi = {10.1145/3290605.3300628},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
articleno = {Paper 398},
numpages = {13},
keywords = {augmented reality, visual analytics, affinity diagramming},
location = {Glasgow, Scotland Uk},
series = {CHI ’19},
pages={1--13}
}

@inproceedings{10.1145/2858036.2858373,
author = {Yang, Qian and Zimmerman, John and Steinfeld, Aaron and Carey, Lisa and Antaki, James F.},
title = {Investigating the Heart Pump Implant Decision Process: Opportunities for Decision Support Tools to Help},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858373},
doi = {10.1145/2858036.2858373},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {4477–4488},
numpages = {12},
keywords = {clinical decision support systems, service design, qualitative methods, decision support tools, field study},
location = {San Jose, California, USA},
series = {CHI ’16}
}

@inproceedings{10.1145/3343413.3377983,
author = {McKay, Dana and Makri, Stephann and Chang, Shanton and Buchanan, George},
title = {On Birthing Dancing Stars: The Need for Bounded Chaos in Information Interaction},
year = {2020},
isbn = {9781450368926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343413.3377983},
doi = {10.1145/3343413.3377983},
booktitle = {Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
pages = {292–302},
numpages = {11},
keywords = {serendipity, creativity, chaos, information-seeking, browsing, information encountering, information interaction, exploration},
location = {Vancouver BC, Canada},
series = {CHIIR ’20}
}

@incollection{harrington2016affinity,
  title={Affinity Diagrams},
  author={Harrington, H James},
  booktitle={The Innovation Tools Handbook, Volume 2},
  pages={45--54},
  year={2016},
  publisher={Productivity Press}
}

@inproceedings{10.1145/3173574.3173704,
author = {Yang, Qian and Banovic, Nikola and Zimmerman, John},
title = {Mapping Machine Learning Advances from HCI Research to Reveal Starting Places for Design Innovation},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173704},
doi = {10.1145/3173574.3173704},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
articleno = {Paper 130},
numpages = {11},
keywords = {machine learning, research transfer, sensitizing concept, user experience, bibliometric, data mining},
location = {Montreal QC, Canada},
series = {CHI ’18},
pages={1--11}
}

@inproceedings{Abdul:2018:TTE:3173574.3174156,
 author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
 title = {Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {582:1--582:18},
 articleno = {582},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/3173574.3174156},
 doi = {10.1145/3173574.3174156},
 acmid = {3174156},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {explainable artificial intelli-gence, explanations, intelligibility, interpretable machine learning},
}

@book{szolovits2019artificial,
  title={Artificial intelligence in medicine},
  author={Szolovits, Peter},
  year={2019},
  publisher={Routledge}
}

@article{challen2019artificial,
  title={Artificial intelligence, bias and clinical safety},
  author={Challen, Robert and Denny, Joshua and Pitt, Martin and Gompels, Luke and Edwards, Tom and Tsaneva-Atanasova, Krasimira},
  journal={BMJ Qual Saf},
  volume={28},
  number={3},
  pages={231--237},
  year={2019},
  publisher={BMJ Publishing Group Ltd}
}

@inproceedings{Alkhatib:2019:SAT:3290605.3300760,
 author = {Alkhatib, Ali and Bernstein, Michael},
 title = {Street-Level Algorithms: A Theory at the Gaps Between Policy and Decisions},
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '19},
 year = {2019},
 isbn = {978-1-4503-5970-2},
 location = {Glasgow, Scotland Uk},
 pages = {530:1--530:13},
 articleno = {530},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/3290605.3300760},
 doi = {10.1145/3290605.3300760},
 acmid = {3300760},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial intelligence, street-level algorithms, street-level bureaucracies},
}

@inproceedings{DeBackere:2015:DPR:2826165.2826229,
 author = {De Backere, Femke and Verstichel, Stijn and Van den Bergh, Jan and Elprama, Shirley A. and Ongenae, Femke and De Turck, Filip and Jacobs, An and Coninx, Karin},
 title = {Discovery of the Potential Role of Sensors in a Personal Emergency Response System: What Can We Learn from a Single Workshop?},
 booktitle = {Proceedings of the 9th International Conference on Pervasive Computing Technologies for Healthcare},
 series = {PervasiveHealth '15},
 year = {2015},
 isbn = {978-1-63190-045-7},
 location = {Istanbul, Turkey},
 pages = {330--333},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=2826165.2826229},
 acmid = {2826229},
 publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
 address = {ICST, Brussels, Belgium, Belgium},
 keywords = {care, decision, process, workshop},
}

@inproceedings{Bonham:2019:ARS:3308557.3308726,
 author = {Bonham, Matthew},
 title = {Augmented Reality Simulation Toward Improving Therapeutic Healthcare Communication Techniques},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
 series = {IUI '19},
 year = {2019},
 isbn = {978-1-4503-6673-1},
 location = {Marina del Ray, California},
 pages = {161--162},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3308557.3308726},
 doi = {10.1145/3308557.3308726},
 acmid = {3308726},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {healthcare communication, nurse training, therapeutic nursing},
}

@inproceedings{Gambino:2019:DDR:3290607.3312916,
 author = {Gambino, Andrew and Kim, Jinyoung and Sundar, S. Shyam},
 title = {Digital Doctors and Robot Receptionists: User Attributes That Predict Acceptance of Automation in Healthcare Facilities},
 booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
 series = {CHI EA '19},
 year = {2019},
 isbn = {978-1-4503-5971-9},
 location = {Glasgow, Scotland Uk},
 pages = {LBW0287:1--LBW0287:6},
 articleno = {LBW0287},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3290607.3312916},
 doi = {10.1145/3290607.3312916},
 acmid = {3312916},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automation, cognitive heuristics, health information systems, self-disclosure},
}

@inproceedings{Sonntag:2012:RMD:2166966.2167031,
 author = {Sonntag, Daniel and Schulz, Christian and Reuschling, Christian and Galarraga, Luis},
 title = {RadSpeech's Mobile Dialogue System for Radiologists},
 booktitle = {Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces},
 series = {IUI '12},
 year = {2012},
 isbn = {978-1-4503-1048-2},
 location = {Lisbon, Portugal},
 pages = {317--318},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2166966.2167031},
 doi = {10.1145/2166966.2167031},
 acmid = {2167031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {healthcare, mobility, speech dialogue},
}

@article{amershi2014power,
  title={Power to the people: The role of humans in interactive machine learning},
  author={Amershi, Saleema and Cakmak, Maya and Knox, William Bradley and Kulesza, Todd},
  journal={AI Magazine},
  volume={35},
  number={4},
  pages={105--120},
  year={2014}
}

@inproceedings{Eslami:2016:FIL:2858036.2858494,
 author = {Eslami, Motahhare and Karahalios, Karrie and Sandvig, Christian and Vaccaro, Kristen and Rickman, Aimee and Hamilton, Kevin and Kirlik, Alex},
 title = {First I "Like" It, then I Hide It: Folk Theories of Social Feeds},
 booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '16},
 year = {2016},
 isbn = {978-1-4503-3362-7},
 location = {San Jose, California, USA},
 pages = {2371--2382},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2858036.2858494},
 doi = {10.1145/2858036.2858494},
 acmid = {2858494},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithms, folk theories, seamful design, social media feeds},
}

@inproceedings{Oh:2018:ILY:3173574.3174223,
 author = {Oh, Changhoon and Song, Jungwoo and Choi, Jinhan and Kim, Seonghyeon and Lee, Sungwoo and Suh, Bongwon},
 title = {I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {649:1--649:13},
 articleno = {649},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/3173574.3174223},
 doi = {10.1145/3173574.3174223},
 acmid = {3174223},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial intelligence, human computer collaboration, human-ai interaction},
}

@inproceedings{Inkpen:2019:HBG:3290607.3299002,
 author = {Inkpen, Kori and Chancellor, Stevie and De Choudhury, Munmun and Veale, Michael and Baumer, Eric P. S.},
 title = {Where is the Human?: Bridging the Gap Between AI and HCI},
 booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
 series = {CHI EA '19},
 year = {2019},
 isbn = {978-1-4503-5971-9},
 location = {Glasgow, Scotland Uk},
 pages = {W09:1--W09:9},
 articleno = {W09},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3290607.3299002},
 doi = {10.1145/3290607.3299002},
 acmid = {3299002},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial interlligence, human computer interaction, machine learning},
}

@article{sundaram2018predicting,
  title={Predicting the clinical impact of human mutation with deep neural networks},
  author={Sundaram, Laksshman and Gao, Hong and Padigepati, Samskruthi Reddy and McRae, Jeremy F and Li, Yanjun and Kosmicki, Jack A and Fritzilas, Nondas and Hakenberg, J{\"o}rg and Dutta, Anindita and Shon, John and others},
  journal={Nature genetics},
  volume={50},
  number={8},
  pages={1161},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{bickmore2018patient,
  title={Patient and consumer safety risks when using conversational assistants for medical information: An observational study of Siri, Alexa, and Google Assistant},
  author={Bickmore, Timothy W and Trinh, Ha and Olafsson, Stefan and O'Leary, Teresa K and Asadi, Reza and Rickles, Nathaniel M and Cruz, Ricardo},
  journal={Journal of medical Internet research},
  volume={20},
  number={9},
  pages={e11510},
  year={2018},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@article{madani2018deep,
  title={Deep echocardiography: data-efficient supervised and semi-supervised deep learning towards automated diagnosis of cardiac disease},
  author={Madani, Ali and Ong, Jia Rui and Tibrewal, Anshul and Mofrad, Mohammad RK},
  journal={npj Digital Medicine},
  volume={1},
  number={1},
  pages={59},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{coudray2018classification,
  title={Classification and mutation prediction from non--small cell lung cancer histopathology images using deep learning},
  author={Coudray, Nicolas and Ocampo, Paolo Santiago and Sakellaropoulos, Theodore and Narula, Navneet and Snuderl, Matija and Feny{\"o}, David and Moreira, Andre L and Razavian, Narges and Tsirigos, Aristotelis},
  journal={Nature medicine},
  volume={24},
  number={10},
  pages={1559},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{ahmad2018death,
  title={Death vs. Data Science: Predicting End of Life},
  author={Ahmad, Muhammad A and Eckert, Carly and McKelvey, Greg and Zolfagar, Kiyana and Zahid, Anam and Teredesai, Ankur},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{hoff2015trust,
  title={Trust in automation: Integrating empirical evidence on factors that influence trust},
  author={Hoff, Kevin Anthony and Bashir, Masooda},
  journal={Human Factors},
  volume={57},
  number={3},
  pages={407--434},
  year={2015},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@incollection{benrimoh2018aifred,
  title={Aifred Health, a Deep Learning Powered Clinical Decision Support System for Mental Health},
  author={Benrimoh, David and Fratila, Robert and Israel, Sonia and Perlman, Kelly and Mirchi, Nykan and Desai, Sneha and Rosenfeld, Ariel and Knappe, Sabrina and Behrmann, Jason and Rollins, Colleen and others},
  booktitle={The NIPS'17 Competition: Building Intelligent Systems},
  pages={251--287},
  year={2018},
  publisher={Springer}
}

@article{ghosh2019artificial,
  title={Artificial Intelligence Using Open Source BI-RADS Data Exemplifying Potential Future Use},
  author={Ghosh, Adarsh},
  journal={Journal of the American College of Radiology},
  volume={16},
  number={1},
  pages={64--72},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{chen2019learning,
  title={Learning Active Contour Models for Medical Image Segmentation},
  author={Chen, Xu and Williams, Bryan M and Vallabhaneni, Srinivasa R and Czanner, Gabriela and Williams, Rachel and Zheng, Yalin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={11632--11640},
  year={2019}
}

@article{burr2018analysis,
  title={An Analysis of the Interaction Between Intelligent Software Agents and Human Users},
  author={Burr, Christopher and Cristianini, Nello and Ladyman, James},
  journal={Minds and machines},
  volume={28},
  number={4},
  pages={735--774},
  year={2018},
  publisher={Springer}
}

@article{johnson2016face,
  title={Face-to-face interaction with pedagogical agents, twenty years later},
  author={Johnson, W Lewis and Lester, James C},
  journal={International Journal of Artificial Intelligence in Education},
  volume={26},
  number={1},
  pages={25--36},
  year={2016},
  publisher={Springer}
}

@article{mou2017media,
  title={The media inequality: Comparing the initial human-human and human-AI social interactions},
  author={Mou, Yi and Xu, Kun},
  journal={Computers in Human Behavior},
  volume={72},
  pages={432--440},
  year={2017},
  publisher={Elsevier}
}

@article{miller2019intrinsically,
  title={The intrinsically linked future for human and Artificial Intelligence interaction},
  author={Miller, Anthony},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={38},
  year={2019},
  publisher={Springer}
}

@inproceedings{chattopadhyay2017evaluating,
  title={Evaluating visual conversational agents via cooperative human-ai games},
  author={Chattopadhyay, Prithvijit and Yadav, Deshraj and Prabhu, Viraj and Chandrasekaran, Arjun and Das, Abhishek and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
  booktitle={Fifth AAAI Conference on Human Computation and Crowdsourcing},
  year={2017}
}

@article{azuaje2019artificial,
  title={Artificial intelligence for precision oncology: beyond patient stratification},
  author={Azuaje, Francisco},
  journal={NPJ precision oncology},
  volume={3},
  number={1},
  pages={6},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{chin2018clinical,
  title={Clinical judgement in the era of big data and predictive analytics},
  author={Chin-Yee, Benjamin and Upshur, Ross},
  journal={Journal of evaluation in clinical practice},
  volume={24},
  number={3},
  pages={638--645},
  year={2018},
  publisher={Wiley Online Library}
}

@article{lau2018dataset,
  title={A dataset of clinically generated visual questions and answers about radiology images},
  author={Lau, Jason J and Gayen, Soumya and Abacha, Asma Ben and Demner-Fushman, Dina},
  journal={Scientific data},
  volume={5},
  pages={180251},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{price2018big,
  title={Big data and black-box medical algorithms},
  author={Price, W Nicholson},
  journal={Science translational medicine},
  volume={10},
  number={471},
  pages={eaao5333},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{fails2003interactive,
  title={Interactive machine learning},
  author={Fails, Jerry Alan and Olsen Jr, Dan R},
  booktitle={Proceedings of the 8th international conference on Intelligent user interfaces},
  pages={39--45},
  year={2003},
  organization={ACM}
}

@article{holzinger2016interactive,
  title={Interactive machine learning for health informatics: when do we need the human-in-the-loop?},
  author={Holzinger, Andreas},
  journal={Brain Informatics},
  volume={3},
  number={2},
  pages={119--131},
  year={2016},
  publisher={Springer}
}

@inproceedings{aha2017ai,
  title={The AI rebellion: Changing the narrative},
  author={Aha, David W and Coman, Alexandra},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@article{holzinger2019interactive,
  title={Interactive machine learning: experimental evidence for the human in the algorithmic loop},
  author={Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Cri{\c{s}}an, Gloria Cerasela and Pintea, Camelia-M and Palade, Vasile},
  journal={Applied Intelligence},
  volume={49},
  number={7},
  pages={2401--2414},
  year={2019},
  publisher={Springer}
}

@article{seref2019performance,
  title={Performance of Na{\"\i}ve and Complement Na{\"\i}ve Bayes Algorithms Based on Accuracy, Precision and Recall Performance Evaluation Criterions},
  author={Seref, Berna and Bostanci, Erkan},
  journal={International Journal of Computing},
  volume={8},
  number={5},
  pages={75--92},
  year={2019}
}

@inproceedings{Dove:2017:UDI:3025453.3025739,
 author = {Dove, Graham and Halskov, Kim and Forlizzi, Jodi and Zimmerman, John},
 title = {UX Design Innovation: Challenges for Working with Machine Learning As a Design Material},
 booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '17},
 year = {2017},
 isbn = {978-1-4503-4655-9},
 location = {Denver, Colorado, USA},
 pages = {278--288},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3025453.3025739},
 doi = {10.1145/3025453.3025739},
 acmid = {3025739},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design material, interaction design, machine learning, ux practice},
}

@article{boughey2016identification,
  title={Identification and resection of the clipped node decreases the false negative rate of sentinel lymph node surgery in patients presenting with node positive breast cancer (T0-T4, N1-2) who receive neoadjuvant chemotherapy--results from ACOSOG Z1071 (Alliance)},
  author={Boughey, Judy C and Ballman, Karla V and Le-Petross, Huong T and McCall, Linda M and Mittendorf, Elizabeth A and Ahrendt, Gretchen M and Wilke, Lee G and Taback, Bret and Feliberti, Eric C and Hunt, Kelly K},
  journal={Annals of surgery},
  volume={263},
  number={4},
  pages={802},
  year={2016},
  publisher={NIH Public Access}
}

@article{dialani2015role,
  title={Role of imaging in neoadjuvant therapy for breast cancer},
  author={Dialani, Vandana and Chadashvili, Tamuna and Slanetz, Priscilla J},
  journal={Annals of surgical oncology},
  volume={22},
  number={5},
  pages={1416--1424},
  year={2015},
  publisher={Springer}
}

@book{farrell2016nodejs,
  title={Nodejs Winner},
  author={Farrell, Eve},
  year={2016},
  publisher={CreateSpace Independent Publishing Platform}
}

@article{drnasin2017javascript,
  title={JavaScript access to DICOM network and objects in web browser},
  author={Drnasin, Ivan and Grgi{\'c}, Mislav and Gogi{\'c}, Goran},
  journal={Journal of digital imaging},
  volume={30},
  number={5},
  pages={537--546},
  year={2017},
  publisher={Springer}
}

@inproceedings{gustin2017empowerment,
  title={Empowerment of diabetic patients through mHealth technologies and education: development of a pilot self-management application},
  author={Gustin, Guillaume and Macq, Beno{\^\i}t and Gruson, Damien and Kieffer, Suzanne},
  booktitle={13th International Conference on Medical Information Processing and Analysis},
  volume={10572},
  pages={105720L},
  year={2017},
  organization={International Society for Optics and Photonics}
}

@article{depeursinge2011mobile,
  title={Mobile medical visual information retrieval},
  author={Depeursinge, Adrien and Duc, Samuel and Eggel, Ivan and Muller, Henning},
  journal={IEEE Transactions on information technology in biomedicine},
  volume={16},
  number={1},
  pages={53--61},
  year={2011},
  publisher={IEEE}
}

@inproceedings{zhang2015dicom,
  title={DICOM index tracker enterprise: advanced system for enterprise-wide quality assurance and patient safety monitoring},
  author={Zhang, Min and Pavlicek, William and Panda, Anshuman and Langer, Steve G and Morin, Richard and Fetterly, Kenneth A and Paden, Robert and Hanson, James and Wu, Lin-Wei and Wu, Teresa},
  booktitle={Medical Imaging 2015: PACS and Imaging Informatics: Next Generation and Innovations},
  volume={9418},
  pages={94180L},
  year={2015},
  organization={International Society for Optics and Photonics}
}

@inproceedings{NEURIPS2019_bdbca288,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {8026--8037},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{sidenko2018eye,
  title={Eye-tracking technology for the analysis of dynamic data},
  author={Sidenko, Ievgen and Filina, Korinna and Kondratenko, Galyna and Chabanovskyi, Danyl and Kondratenko, Yuriy},
  booktitle={2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)},
  pages={479--484},
  year={2018},
  organization={IEEE}
}

@article{schoorman2016perspective,
  title={Perspective: Empowerment in veterinary clinics: the role of trust in delegation},
  author={Schoorman, F David and Mayer, Roger C and Davis, James H},
  journal={Journal of Trust Research},
  volume={6},
  number={1},
  pages={91--95},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{basheer2015certainty,
  title={Certainty, trust and evidence: Towards an integrative model of confidence in multi-agent systems},
  author={Basheer, Ghusoon Salim and Ahmad, Mohd Sharifuddin and Tang, Alicia YC and Graf, Sabine},
  journal={Computers in Human Behavior},
  volume={45},
  pages={307--315},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{khalid2016prediction,
  title={Prediction of trust in scripted dialogs using neuro-fuzzy method},
  author={Khalid, HM and Liew, WS and Helander, MG and Loo, CK},
  booktitle={2016 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
  pages={1558--1562},
  year={2016},
  organization={IEEE}
}

@inproceedings{10.1145/2898375.2898385,
author = {Pearson, Carl J. and Welk, Allaire K. and Boettcher, William A. and Mayer, Roger C. and Streck, Sean and Simons-Rudolph, Joseph M. and Mayhorn, Christopher B.},
title = {Differences in Trust between Human and Automated Decision Aids},
year = {2016},
isbn = {9781450342773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2898375.2898385},
doi = {10.1145/2898375.2898385},
abstract = {Humans can easily find themselves in high cost situations where they must choose between suggestions made by an automated decision aid and a conflicting human decision aid. Previous research indicates that humans often rely on automation or other humans, but not both simultaneously. Expanding on previous work conducted by Lyons and Stokes (2012), the current experiment measures how trust in automated or human decision aids differs along with perceived risk and workload. The simulated task required 126 participants to choose the safest route for a military convoy; they were presented with conflicting information from an automated tool and a human. Results demonstrated that as workload increased, trust in automation decreased. As the perceived risk increased, trust in the human decision aid increased. Individual differences in dispositional trust correlated with an increased trust in both decision aids. These findings can be used to inform training programs for operators who may receive information from human and automated sources. Examples of this context include: air traffic control, aviation, and signals intelligence.},
booktitle = {Proceedings of the Symposium and Bootcamp on the Science of Security},
pages = {95–98},
numpages = {4},
keywords = {decision-making, workload, reliance, strain, risk, trust, automation},
location = {Pittsburgh, Pennsylvania},
series = {HotSos '16}
}

@InProceedings{maicas2018training,
author="Maicas, Gabriel
and Bradley, Andrew P.
and Nascimento, Jacinto C.
and Reid, Ian
and Carneiro, Gustavo",
editor="Frangi, Alejandro F.
and Schnabel, Julia A.
and Davatzikos, Christos
and Alberola-L{\'o}pez, Carlos
and Fichtinger, Gabor",
title="Training Medical Image Analysis Systems like Radiologists ",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="546--554",
abstract="The training of medical image analysis systems using machine learning approaches follows a common script: collect and annotate a large dataset, train the classifier on the training set, and test it on a hold-out test set. This process bears no direct resemblance with radiologist training, which is based on solving a series of tasks of increasing difficulty, where each task involves the use of significantly smaller datasets than those used in machine learning. In this paper, we propose a novel training approach inspired by how radiologists are trained. In particular, we explore the use of meta-training that models a classifier based on a series of tasks. Tasks are selected using teacher-student curriculum learning, where each task consists of simple classification problems containing small training sets. We hypothesize that our proposed meta-training approach can be used to pre-train medical image analysis models. This hypothesis is tested on the automatic breast screening classification from DCE-MRI trained with weakly labeled datasets. The classification performance achieved by our approach is shown to be the best in the field for that application, compared to state of art baseline approaches: DenseNet, multiple instance learning and multi-task learning.",
isbn="978-3-030-00928-1"
}

@article{doi:10.1002/ijc.27711,
author = {Bray, Freddie and Ren, Jian-Song and Masuyer, Eric and Ferlay, Jacques},
title = {Global estimates of cancer prevalence for 27 sites in the adult population in 2008},
journal = {International Journal of Cancer},
volume = {132},
number = {5},
pages = {1133-1145},
keywords = {prevalence, neoplasms, healthcare facilities, development, survival},
doi = {10.1002/ijc.27711},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ijc.27711},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ijc.27711},
abstract = {Abstract Recent estimates of global cancer incidence and survival were used to update previous figures of limited duration prevalence to the year 2008. The number of patients with cancer diagnosed between 2004 and 2008 who were still alive at the end of 2008 in the adult population is described by world region, country and the human development index. The 5-year global cancer prevalence is estimated to be 28.8 million in 2008. Close to half of the prevalence burden is in areas of very high human development that comprise only one-sixth of the world's population. Breast cancer continues to be the most prevalent cancer in the vast majority of countries globally; cervix cancer is the most prevalent cancer in much of Sub-Saharan Africa and Southern Asia and prostate cancer dominates in North America, Oceania and Northern and Western Europe. Stomach cancer is the most prevalent cancer in Eastern Asia (including China); oral cancer ranks as the most prevalent cancer in Indian men and Kaposi sarcoma has the highest 5-year prevalence among men in 11 countries in Sub-Saharan Africa. The methods used to estimate point prevalence appears to give reasonable results at the global level. The figures highlight the need for long-term care targeted at managing patients with certain very frequently diagnosed cancer forms. To be of greater relevance to cancer planning, the estimation of other time-based measures of global prevalence is warranted.},
year = {2013}
}

@article{chkotua2017peer,
  title={Peer Reviewed: Mammography Use in Portugal: National Health Survey 2014},
  author={Chkotua, Sofia and Peleteiro, B{\'a}rbara},
  journal={Preventing Chronic Disease},
  volume={14},
  year={2017},
  publisher={Centers for Disease Control and Prevention},
  pages={1--2},
  numpages={2},
  doi={http://dx.doi.org/10.5888/pcd14.170054}
}

@article{chaurasia2017novel,
  title={A novel approach for breast cancer detection using data mining techniques},
  author={Chaurasia, Vikas and Pal, Saurabh},
  journal={International Journal of Innovative Research in Computer and Communication Engineering (An ISO 3297: 2007 Certified Organization) Vol},
  volume={2},
  year={2017}
}

@article{hengstler2016applied,
  title={Applied artificial intelligence and trust—The case of autonomous vehicles and medical assistance devices},
  author={Hengstler, Monika and Enkel, Ellen and Duelli, Selina},
  journal={Technological Forecasting and Social Change},
  volume={105},
  pages={105--120},
  year={2016},
  publisher={Elsevier}
}

@article{GOTTAPU2018179,
title = "DenseNet for Anatomical Brain Segmentation",
journal = "Procedia Computer Science",
volume = "140",
pages = "179 - 185",
year = "2018",
note = "Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2018.10.327",
url = "http://www.sciencedirect.com/science/article/pii/S1877050918320003",
author = "Ram Deepak Gottapu and Cihan H Dagli",
keywords = "convolutional neural network (CNN), Dense Net, segmentation",
abstract = "Automated segmentation in brain magnetic resonance image (MRI) plays an important role in the analysis of many diseases and conditions. In this paper, we present a new architecture to perform MR image brain segmentation (MRI) into a number of classes based on type of tissue. Recent work has shown that convolutional neural networks (DenseNet) can be substantially more accurate with less number of parameters if each layer in the network is connected with every other layer in a feed forward fashion. We embrace this idea and generate new architecture that can assign each pixel/voxel in an MR image of the brain to its corresponding anatomical region. To benchmark our model, we used the dataset provided by the IBSR 2(Internet Brain Segmentation Repository), which consists of 18 manually segmented MR images of the brain. To our knowledge, our approach is the first to use DenseNet to perform anatomical segmentation of the whole brain."
}

@article{Savage2019,
  doi = {10.1038/d41586-019-02870-4},
  url = {https://doi.org/10.1038/d41586-019-02870-4},
  year = {2019},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {573},
  number = {7775},
  pages = {S98--S99},
  author = {Neil Savage},
  title = {Digital assistants aid disease diagnosis},
  journal = {Nature}
}

@article{raja2017machine,
  title={Machine learning workflow to enhance predictions of adverse drug reactions (ADRs) through drug-gene interactions: Application to drugs for cutaneous diseases},
  author={Raja, Kalpana and Patrick, Matthew and Elder, James T and Tsoi, Lam C},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={3690},
  year={2017},
  publisher={Nature Publishing Group}
}

@Inbook{Miao2019,
author="Miao, Shun
and Liao, Rui",
title="Agent-Based Methods for Medical Image Registration",
bookTitle="Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="323--345",
abstract="Medical imaging registration is a critical step in a wide spectrum of medical applications from diagnosis to therapy and has been an extensively studied research field. Prior to the popularity of deep learningDeep learning, image registrationImage registration was commonly performed by optimizing an image matching metric as a cost function in search for the optimal registration. However, the optimization task is known to be challenging due to (1) the non-convex nature of the matching metric over the registration parameter space and (2) the lack of effective approaches for robust optimization. With the latest advance in deep learningDeep learning and artificial intelligence, the field of medical image registrationImage registration had a major paradigm shift, whereby learning-based image registrationImage registration methods are developed to employ deep neural networks to analyze images in order to estimate plausible registrations. Among the latest advances in learning-based registration methods, agent-based methods have been shown to be effective in both 3-D/3-D and 2-D/3-D registrations with significant robustness advantage over conventional optimization-based methods. In this chapter, we give an overview of agent-based methods for medical image registrationImage registration and its two applications on rigid-body 3-D/3-D and 2-D/3-D registrations.",
isbn="978-3-030-13969-8",
doi="10.1007/978-3-030-13969-8_16",
url="https://doi.org/10.1007/978-3-030-13969-8_16"
}

@inproceedings{sonntag2016persuasive,
  title={Persuasive AI Technologies for Healthcare Systems},
  author={Sonntag, Daniel},
  booktitle={2016 AAAI Fall Symposium Series},
  year={2016},
  publisher={AAAI},
  address={Westin Arlington Gateway, Arlington, Virginia, USA},
  pages={165--168},
  numpages={4}
}

@article{williams2015unified,
  title={The unified theory of acceptance and use of technology (UTAUT): a literature review},
  author={Williams, Michael D and Rana, Nripendra P and Dwivedi, Yogesh K},
  journal={Journal of enterprise information management},
  year={2015},
  publisher={Emerald Group Publishing Limited}
}

@article{venkatesh2016unified,
  title={Unified theory of acceptance and use of technology: A synthesis and the road ahead},
  author={Venkatesh, Viswanath and Thong, James YL and Xu, Xin},
  journal={Journal of the association for Information Systems},
  volume={17},
  number={5},
  pages={328--376},
  year={2016}
}

@article{rahi2018investigating,
  title={Investigating the role of unified theory of acceptance and use of technology (UTAUT) in internet banking adoption context},
  author={Rahi, S and Ghani, M and Alnaser, F and Ngah, A},
  journal={Management Science Letters},
  volume={8},
  number={3},
  pages={173--186},
  year={2018}
}

@article{crede2019questionable,
  title={Questionable research practices when using confirmatory factor analysis},
  author={Crede, Marcus and Harms, Peter},
  journal={Journal of Managerial Psychology},
  year={2019},
  publisher={Emerald Publishing Limited}
}

@book{hair2017advanced,
  title={Advanced issues in partial least squares structural equation modeling},
  author={Hair Jr, Joseph F and Sarstedt, Marko and Ringle, Christian M and Gudergan, Siegfried P},
  year={2017},
  publisher={saGe publications}
}

@article{cham2017full,
  title={Full information maximum likelihood estimation for latent variable interactions with incomplete indicators},
  author={Cham, Heining and Reshetnyak, Evgeniya and Rosenfeld, Barry and Breitbart, William},
  journal={Multivariate behavioral research},
  volume={52},
  number={1},
  pages={12--30},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{igolkina2020semopy,
  title={semopy: A Python Package for Structural Equation Modeling},
  author={Igolkina, Anna A and Meshcheryakov, Georgy},
  journal={Structural Equation Modeling: A Multidisciplinary Journal},
  pages={1--12},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{hayes2017regression,
  title={Regression-based statistical mediation and moderation analysis in clinical research: Observations, recommendations, and implementation},
  author={Hayes, Andrew F and Rockwood, Nicholas J},
  journal={Behaviour research and therapy},
  volume={98},
  pages={39--57},
  year={2017},
  publisher={Elsevier}
}

@ARTICLE{9233366,
  author={E. {Tjoa} and C. {Guan}},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI}, 
  year={2020},
  volume={},
  number={},
  pages={1-21},
  doi={10.1109/TNNLS.2020.3027314}
}

@article{andreas2020measuring,
  title={Measuring the Quality of Explanations: The System Causability Scale (SCS): Comparing Human and Machine Explanations},
  author={Andreas, Holzinger and Andr{\'e}, Carrington and Heimo, M{\"u}ller},
  journal={Kunstliche intelligenz},
  volume={34},
  number={2},
  pages={193--198},
  year={2020}
}