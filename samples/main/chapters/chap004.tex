% #############################################################################
% This is Chapter 4
% !TEX root = main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Acceptance \& Adoption}
\clearpage
% The following line allows to ref this chapter
\label{chap:chap004}

\vspace{0.5mm}

\noindent
{\it This Chapter~\ref{chap:chap004} was published in one top (Q1 in 2022) ranked journal:}

\vspace{0.5mm}

\begin{itemize}
\item {\bf Francisco Maria Calisto}, Nuno Nunes, Jacinto C. Nascimento, Modeling Adoption of Intelligent Agents in Medical Imaging, International Journal of Human-Computer Studies, Volume 168, 2022, 102922, ISSN 1071-5819, DOI: \href{https://doi.org/10.1016/j.ijhcs.2022.102922}{doi.org/10.1016/j.ijhcs.2022.102922}
\end{itemize}

TODO: Intro PhD Thesis and Chapter...

\section{Motivation}
\label{sec:chap004001}

In the rapidly advancing field of \ac{AI} research, the medical sector holds significant potential for innovation.
\ac{DL} methods have emerged as powerful tools in the clinical domain, enabling a range of advancements such as genome interpretation~\cite{sundaram2018predicting}, medical coaching~\cite{bickmore2018patient}, assistive scan readings~\cite{madani2018deep}, cancer diagnosis and identification of mutations~\cite{coudray2018classification}, and mortality prediction~\cite{ahmad2018death}.
These \ac{AI}-based systems have the capacity to support clinicians and improve patient care, particularly in cases where radiologic expertise may be limited~\cite{doi:10.1148/radiol.2020201874, doi:10.1148/radiol.2020190283}.
However, to ensure successful implementation and adoption of \ac{AI} systems in real-world clinical settings~\cite{hoff2015trust, Kocielnik:2019:YAI:3290605.3300641}, it is crucial to address issues of inconsistency and imperfection that can erode clinician trust and lead to technology abandonment~\cite{benrimoh2018aifred, CALISTO2021102607, CALISTO2022102285}.

Clinicians' trust in \ac{AI} technologies is crucial for their acceptance and use~\cite{calisto2019midaaiarfuv, https://doi.org/10.13140/rg.2.2.25412.68486}.
Understanding the factors that influence clinicians' adoption of \ac{AI} systems is essential for designing trustworthy solutions~\cite{JUNGMANN2021834, doi:10.1200/CCI.20.00148, info:doi/10.2196/12422}.
It requires addressing reliability, consistency, alignment with clinical expertise, and privacy concerns while considering moderator variables.
By identifying and addressing these factors, we can establish and maintain trust in \ac{AI} solutions, enhancing clinicians' decision-making processes and improving patient care.

In this chapter, we investigate the adoption and use of intelligent agents that support clinicians during patient diagnosis via medical imaging.
We aim to understand the factors that influence the acceptance and adoption of these \ac{AI} systems, focusing on risk, privacy, trust, and moderator variables such as gender, age, medical experience, training levels, and areas of expertise~\cite{CALISTO2021102607, Kocielnik:2019:YAI:3290605.3300641}.
Exploring these factors provides valuable insights into clinicians' adoption patterns and enables the enhancement of the design and integration of intelligent agents in healthcare settings.
Although performance improvements in intelligent agents are significant, they may not be sufficient for widespread adoption in clinical practice.
Therefore, it is crucial to explore the critical factors that impact clinicians' behavior and acceptance of intelligent agents.
By identifying these factors, we can facilitate the integration of intelligent agents and drive advancements in the healthcare sector.
Despite the promising potential of intelligent agents, there is a gap between their capabilities and their limited usage among clinicians in their daily work.
This underscores the need for research on the acceptance and adoption of intelligent agents by healthcare practitioners.

To provide a theoretical framework for our research, we adopt the \ac{UTAUT} model~\cite{venkatesh2016unified}.
The \ac{UTAUT} model has successfully explained technology acceptance in various contexts~\cite{10.2307/30036540, KIJSANAYOTIN2009404}.
Moreover, its application in the healthcare domain can expand our understanding of the introduction of \ac{AI} in specific healthcare domains~\cite{KALAVANI2018287}.
We aim to analyze clinicians' intentions to use \ac{AI} systems using the \ac{UTAUT} model and enhance its explanatory power and predictive accuracy~\cite{DEANGELI2020102412, HART201993, Zhang2022}.

In this work, we examine clinicians' perceptions and attitudes toward \ac{AI} applications in the medical imaging workflow.
The research objectives are threefold:
(1) to investigate the effects of intelligent agents on technology adoption, with a focus on security, risk, and trust;
(2) to better understand the determinants of trust and acceptance in technology use; and
(3) to improve the explanatory power of a questionnaire based on \ac{UTAUT} constructs for broader application in \ac{HCI} and \ac{AI} research communities.
We employ the \ac{UTAUT} model to analyze clinicians' intention to use an \ac{AI} system~\cite{BOOTSMAN201999, DEANGELI2020102412, HART201993, HOEHLE201635, LOOIJE2010386, MCGLYNN201733, MOORE2022102784}, with the novelty of building a sustainable model tailored to the medical imaging workflow.

The rest of this article is organized as follows.
Section~\ref{sec:chap004002} provides an overview of the current literature, highlighting the research gaps and motivations for our study.
Sections~\ref{sec:chap004003}, \ref{sec:chap004004}, and \ref{sec:chap004005} outline the research questions, hypotheses, and methods adopted, as well as present the results of our study.
The analysis, discussion, and limitations of the research are presented in Sections~\ref{sec:chap004006}, \ref{sec:chap004006002}, and \ref{sec:chap004006003}, considering the specific context of the medical imaging workflow.
Finally, Sections~\ref{sec:chap004006004} and \ref{sec:chap004010} present future directions and conclude the paper, respectively.

\section{Background}
\label{sec:chap004002}

This section presents the background regarding the main topics addressed in this paper.
The first part (Section~\ref{sec:chap004002001}) provides an overview of \acp{CDSSe} and the impact of \ac{AI} systems on the healthcare sector.
The second part (Section~\ref{sec:chap004002002}) deals with the technology measures and respective challenges involved within this medical workflow on a clinical engagement perspective.
Finally, the last part (Section~\ref{sec:chap004002003}) surveys the technology adoption scales and methodology used and extended in our study.

\subsection{Impact of AI in Medical Imaging}
\label{sec:chap004002001}

Medical imaging systems allow the end-user to diagnose several imaging modalities, such as \ac{CT}, \ac{US} or \ac{MRI}, from the retrieval of medical imaging data~\cite{WO2022071818A1, faraji2019radiologic, seifabadi2019correlation}.
Bringing those modalities together offers new possibilities for quantitative and qualitative imaging and diagnosis, but also requires specialized data handling, post-processing and novel visualization methods~\cite{Igarashi:2016:IVS:2984511.2984537, Ocegueda-Hernandez:2016:CMN:2876456.2879485, Sousa:2017:VVR:3025453.3025566}.
In the clinical domain, medical imaging tools can help experts make better decisions~\cite{Lopes:2017:UHC:3143820.3144118}, {\it e.g.}, by identifying cancer prognostics among the available multi-modal data~\cite{calisto2017mimbcdui, lopes2018interaction}.

\ac{AI} has significant potential in image processing specialties like pathology, radiology, and dermatology~\cite{EVANS2022281, MULLER202267}.
Advancements in statistical machine learning, availability of training data, and increasing computational power have fueled the growth of \ac{AI} applications in medicine~\cite{10.1145/3399715.3399744}.
However, the most powerful \ac{AI} methods face challenges in explainability and robustness~\cite{make4020026}.
To build trustworthy \ac{AI} systems, it is crucial to prioritize explainability, traceability, transparency, accountability, fairness, justice, and equity~\cite{10.1007/978-3-030-93736-2_33, 9473208}.

\acp{CDSSe} represent a transformative shift in healthcare, providing clinicians with knowledge to enhance workflow and decision-making~\cite{10.1145/3290605.3300234, edge2019clinical, hwang2019artificial}.
While \acp{CDSSe} can reduce errors and improve outcomes~\cite{Cai:2019:EEE:3301275.3302289, 10.1145/3290605.3300234}, understanding user trust and acceptance remains a challenge, as clinicians may resist systems that do not align with their mental models or provide relevant context~\cite{khairat2018reasons, kohli2018cad, yang2016investigating}.
Clinicians' resistance to change, and new tools, impacts workflow and integration of \acp{CDSSe}~\cite{calisto2017mimbcdui, 10.1145/3132272.3134111, gagnon2014electronic}.
Efficient integration of \ac{CDSSe} requires user trust and acceptance~\cite{eslami2012effects, jia2016effects, Sutton2020}.
Studying the adoption of \acp{CDSSe} is essential for understanding trust and acceptance factors in AI utilization within the healthcare sector, particularly in the medical imaging workflow.

\subsection{Challenges in Adoption of AI Systems}
\label{sec:chap004002002}

\ac{AI} systems have the potential to enhance the efficiency and accuracy of clinicians' workflow, ultimately improving patient outcomes~\cite{wallis2019artificial}.
Despite these possibilities, the widespread adoption and acceptance of \ac{AI} systems in the medical imaging workflow have encountered challenges and limitations~\cite{Fan2020}.
Understanding the factors that contribute to the limited adoption and acceptance of \ac{AI} systems is crucial for addressing barriers and promoting their successful integration into clinical practice.

Various \ac{AI} systems have been developed to support healthcare, such as breast cancer prediction~\cite{McKinney2020}, similar image search using \ac{DL} algorithms~\cite{10.1145/3290605.3300234}, and decision-making support with machine prognostics~\cite{10.1145/3290605.3300468}.
Surveys have classified and discussed \ac{AI} systems in clinical healthcare~\cite{CORONATO2020101964, LANGER2021106878, PELAU2021106855, STADIN2021106486}, addressing ethical dimensions, privacy requirements, discrimination risks, and data usage concerns~\cite{10.1145/3311957.3361858, PARK2021106795, DEANGELI2020102412}.
The engagement of clinicians in \ac{AI} application development has been proposed as an alternative approach~\cite{10.1145/3290605.3300234, CALISTO2021102607, Oh:2018:ILY:3173574.3174223, Sousa:2017:VVR:3025453.3025566}.

The rapid development of \ac{AI} tools for healthcare have raised ethical concerns and sparked debates surrounding their use.
Issues such as privacy, autonomy, and potential discrimination risks have been at the forefront of discussions~\cite{Fan2020, Sutton2020}.
In order to mitigate these concerns and ensure the successful integration of \ac{AI} systems, it is important to adopt a human-centered approach.
Participatory design methodologies, which involve clinicians in the design process, can help align \ac{AI} tools with their specific needs and workflow requirements, fostering acceptance and usability~\cite{CALISTO2021102607}.
By involving clinicians as active stakeholders, the development, and implementation of \ac{AI} systems can be guided by their insights and expertise, resulting in solutions that are more tailored and aligned with their expectations.

\subsection{Models of Technology Adoption}
\label{sec:chap004002003}

Models like \ac{TAM} and \ac{UTAUT} are commonly used to study individual technology adoption~\cite{HOEHLE201635, MOORE2022102784}.
\ac{TAM}, derived from \ac{TRA}, focuses on measuring attitudes towards technology but lacks consideration for social and organizational dynamics~\cite{RePEc:inm:ormnsc:v:35:y:1989:i:8:p:982-1003, AJZEN1991179, LEGRIS2003191}.
To address these limitations, \ac{UTAUT} integrates various user acceptance models, including \ac{TAM} and \ac{TRA}~\cite{10.2307/30036540}.
According to \ac{UTAUT}, indicators like performance expectancy, effort expectancy, social influence, and facilitating conditions influence behavioral intention and use behavior, serving as predictors of users' intention to perform a specific action.

\vspace{2.00mm}

\noindent
The next items describe the meanings of the above constructs from~\cite{10.2307/30036540}:

\vspace{0.05mm}

\begin{itemize}
\item Performance Expectancy: it is the ``[...] the degree to which an individual believes that using the system will help him or her to attain gains in job performance.'' (\cite{10.2307/30036540} p. 447);
\item Effort expectancy: it is the ``[...] degree of ease associated with the use of the system.'' (\cite{10.2307/30036540} p. 450);
\item Social Influence: it is the ``[...] the degree to which an individual perceives that important others believe he or she should use the new system.'' (\cite{10.2307/30036540} p. 451);
\item Facilitating Conditions: it is the ``[...] the degree to which an individual believes that an organizational and technical infrastructure exists to support use of the system.'' (\cite{10.2307/30036540} p. 453).
\end{itemize}

\vspace{0.05mm}

Technology adoption models, such as \ac{TAM} and \ac{UTAUT}, are widely applied and accepted in various domains~\cite{venkatesh2016unified}.
However, these models have been criticized for their deterministic approach and limited consideration of users' characteristics~\cite{https://doi.org/10.1002/mar.20823}.
To address these limitations, researchers have integrated security-related factors into the models, including perceived security, perceived risk, and trust~\cite{KHALILZADEH2017460, SHIN20091343}.
Perceived security refers to the belief in the system's security, perceived risk represents uncertainty or anxiety regarding the outcome of an action, and trust is the belief that the system will meet users' expectations~\cite{KHALILZADEH2017460, mandrik2005exploring, SHIN20091343}.
Perceived risk has been found to negatively influence adoption intention, while trust affects performance and effort expectancy~\cite{Thakur2014, Lee:2013:0301-2212:587}.
In domains like healthcare, where users face higher risks and uncertainties, trust becomes particularly significant~\cite{LU2011393, ZHOU20131085}.
Privacy, security, and trust concerns also arise in the context of e-health and mobile health technologies~\cite{6038874, Schnall2015, 10.1145/3132272.3134111}.
The \ac{UTAUT} model, along with its extensions, has been employed to study \ac{AI}-based technologies in healthcare, considering factors such as cultural differences and users' interest in technology~\cite{huang2010cultural, SOHN2020101324, info:doi/10.2196/14316}.
Building on these efforts, we extend the \ac{UTAUT} scale to measure users' attitudes toward adopting novel \ac{AI} systems in the medical imaging workflow.

\section{Research Questions \& Hypotheses}
\label{sec:chap004003}

This study proposes a questionnaire adapted from the UTAUT model, incorporating variables such as trust, perceived usefulness, ease of use, security, risk, and social influence~\cite{BOOTSMAN201999, LOOIJE2010386}.
This work aims to investigate the effects of AI on technology adoption, particularly in terms of risk, privacy, and trust, and to improve the explanatory power of the questionnaire for broader research applications across AI and HCI communities.
Figure~\ref{fig:fig073} (Section~\ref{chap:app003002} of Appendix~\ref{chap:app003}) presents the proposed acceptance/use model to understand the relationship between variables and hypotheses.

We formulated hypotheses based on the UTAUT constructs and developed a conceptual framework for our exploratory study.
Our model encompasses core determinants of AI adoption and allows for the analysis of moderator uncertainties that may impact these determinants.
UTAUT was selected as the theoretical foundation due to its empirical superiority over other models~\cite{AJZEN1991179, RePEc:inm:ormnsc:v:35:y:1989:i:8:p:982-1003}.

\subsection{Items Based on UTAUT Constructs}
\label{sec:chap004003001}

Our study takes an exploratory approach to hypothesize the significant effects on user adoption, drawing from the original UTAUT constructs.
With these hypotheses, we propose a conceptual framework for our empirical study.
Our model not only covers the core determinants of AI adoption, including intention and actual adoption, but also enables the research community to analyze moderator uncertainty that may constrain the effects of these determinants.
Based on its empirical superiority over other models~\cite{AJZEN1991179, RePEc:inm:ormnsc:v:35:y:1989:i:8:p:982-1003}, we choose UTAUT as the theoretical foundation for developing our hypotheses.

\vspace{2.25mm}

\noindent
We specifically developed the following hypotheses:

\vspace{2.25mm}

{\it Facilitating Conditions} have direct positive relations with user behavior but no effect on behavioral intentions~\cite{10.2307/30036540}.
Although other authors~\cite{KHALILZADEH2017460} are using the original UTAUT model with separate constructs, in this study we are using behavioral intention as a proxy for user behavior.

\vspace{2.25mm}

\noindent
Hence, we hypothesize that:

\vspace{2.25mm}

\noindent
{\bf H1.1.} The facilitating conditions for using AI in the clinical workflow, positively predict users' intentions to use it;

\vspace{2.25mm}

\noindent
{\bf H1.2.} The facilitating conditions ({\it e.g.}, knowledge to use the system) for using AI in the clinical workflow, has direct positive impact on effort expectancy;

\vspace{2.25mm}

{\it Social Influence} has direct impact on behavioral intention.
The underlying assumption is that clinicians tend to consult their medical community about novel diagnosis paradigms and can be influenced by perceived social pressure.
Because perceived security also involves a patient's health for misdiagnosing the case, it should be highly influenced in the proposed model.

\noindent
Based on this, we anticipate that:

\vspace{2.25mm}

\noindent
{\bf H2.1.} The social influence ({\it e.g.}, recommendations from the medical community) for using an AI system, positively predicts effort expectancy;

\vspace{2.25mm}

\noindent
{\bf H2.2.} The social influence for using an AI system, positively and directly influences perceived security;

\vspace{2.25mm}

\noindent
{\bf H2.3.} The social influence for using an AI system, has a direct positive impact on performance expectancy;

\vspace{2.25mm}

{\it Performance Expectancy} is a strong predictor of behavioral intention~\cite{KHALILZADEH2017460}.
It can be divided into utilitarian performance expectancy and hedonic performance expectancy~\cite{Yang2010}.
While enhancing task performance can increase satisfaction, the effect of hedonic motivation may be weaker, according to some authors~\cite{ESCOBARRODRIGUEZ201470, HART201993}.
In radiology, the ability of AI to provide an autonomous second reader that supports decision-making is an attractive goal, offering practical benefits that are likely to drive adoption.

\vspace{2.25mm}

\noindent
Therefore, we hypothesize that:

\vspace{2.25mm}

\noindent
{\bf H3.} Performance expectancy ({\it i.e.}, usefulness) positively affects behavioral intention to use an AI system;

{\it Effort Expectancy} is another predictor of intention to use AI systems~\cite{doi:10.5465/amr.2019.0178}.
Several authors~\cite{Fan2020, Tran2021} also find effort expectancy to affect behavioral intention significantly.
As AI provides a new paradigm shift, the perceived degree of ease associated with AI systems will likely affect behavioral intention.

\noindent
In accordance, we denote that:

\vspace{2.25mm}

\noindent
{\bf H4.} Effort expectancy ({\it i.e.}, ease of use) positively affects performance expectancy ({\it i.e.}, usefulness) to use AI systems;

\subsection{Items Based on UTAUT Extensions}
\label{sec:chap004003002}

Trust, security and risk became critical additional constructs in studies about technology adoption~\cite{KHALILZADEH2017460, https://doi.org/10.1002/mar.20823}.
Specially, in the case of sharing medical information~\cite{6038852}, as clinicians are resistant to adopt new technologies~\cite{10.1145/3132272.3134111}.

\vspace{2.25mm}

{\it Perceived Security} is expected to affect behavioral intention directly.
Because AI systems involve sensitive patient information, it should be highly influential in the proposed model~\cite{KHALILZADEH2017460}.
Moreover, perceived security is an aggregating construct, changing over time according to medical experts' community opinion and social influence.

\vspace{2.25mm}

\noindent
Therefore, we hypothesize that:

\vspace{2.25mm}

\noindent
{\bf H5.1.} Perceived security of AI systems positively and directly predicts perceived trust;

\vspace{2.25mm}

\noindent
{\bf H5.2.} Perceived security of AI systems positively and directly behavioral intention to use AI systems;

{\it Perceived Risk} is usually related to perceived security.
The more a user perceives the risk, the less secure the user's feel, leading to a negative relationship between risk and security~\cite{KHALILZADEH2017460}.
Indeed, the literature~\cite{SHIN20091343, Thakur2014} states that perceived risk has negative impact on perceived security, trust, as performance expectancy.

\vspace{2.25mm}

\noindent
Based on these literature findings, we formulate the following hypothesis:

\vspace{2.25mm}

\noindent
{\bf H6.1.} The perceived risk of using AI systems has a direct negative impact on perceived security;

\vspace{2.25mm}

\noindent
{\bf H6.2.} The perceived risk of using AI systems has a direct negative impact on perceived trust;

\vspace{2.25mm}

\noindent
{\bf H6.3.} The perceived risk of using AI systems has a direct negative impact on performance expectancy;

\vspace{2.25mm}

{\it Trust} Perceived security and trust positively affect behavioral intentions~\cite{SHIN20091343}.
As a unitary construct, the effect of trust on behavioral intention has gained notable support.
Moreover, this construct is the most significant predictor of behavioral intention~\cite{chandra2010evaluating, SHIN20091343}.
While AI-based solutions become ubiquitous, trust supersedes the importance of traditional adoption factors such as perceived usefulness.
Akin to Chandra et al.~\cite{chandra2010evaluating}, this study includes trust as a singular construct, which is likely to be critical due to the novelty of AI systems in the medical workflow and complex environments.

\vspace{2.25mm}

\noindent
Thus, we hypothesize that:

\vspace{2.25mm}

\noindent
{\bf H7.1.} Trust positively affects performance expectancy ({\it i.e.}, usefulness) to use AI systems;

\vspace{2.25mm}

\noindent
{\bf H7.2.} Trust positively affects effort expectancy ({\it i.e.}, ease of use) to use AI systems;

\subsection{Items Related to Impact}
\label{sec:chap004003003}

AI has a significant social and personal behavioral impact on the medical community all over the world~\cite{Fan2020}.
Medical diagnosis impacts significantly on peoples' health and peoples' lives, and clinicians have to take responsibility for their actions.
Therefore, clinicians may be more cautious than professionals in other fields when considering adopting new technologies to assist their work.
Although clinicians typically resist new technologies, several authors~\cite{doi:10.1148/ryai.2020190043, WAYMEL2019327} are reporting a new change of behavior for adopting AI systems in the clinical workflow, whereas these systems are increasing clinicians' performance during decision-making \cite{CALISTO2021102607}.

As introduced in Section~\ref{sec:chap004002}, the TRA inspires various technology adoption models, arguing that both attitude toward an action and subjective norms have impact on behavioral intention, affecting how people perform an action~\cite{https://doi.org/10.1002/hbe2.195}.
Adapted from TRA and TAM, the UTAUT definition of attitude through behavior is ``{\it an individual's positive feeling about performing the target behavior}''~\cite{10.2307/30036540}.
On the other hand, subjective norm refers to ``{\it a person's perception that most people who are important to them think they should or should not perform the behavior in question}''~\cite{10.2307/30036540}.

\vspace{2.25mm}

\noindent
Hence, we hypothesize that:

\vspace{2.25mm}

\noindent
{\bf H8.1.} The extent to which AI positively impacts the clinical workflow ({\it e.g.}, decreasing the medical error, time-to-diagnose, etc.) due to better decision-making of clinicians, affects the intention to follow AI guidance;

\vspace{2.25mm}

\noindent
{\bf H8.2.} The extent to which AI positively impacts the clinical workflow ({\it e.g.}, decreasing the medical error, time-to-diagnose, etc.) due to better decision-making of clinicians, directly predicts perceived trust;

\vspace{2.25mm}

Salient beliefs and evaluations determine a person's attitude toward a behavior~\cite{KHALILZADEH2017460}.
In fact, several studies report a strong relationship between trust, risk, and behavior intentions~\cite{GANSSER2021101535, 9197782, https://doi.org/10.1002/mar.20823}.
Although trust and perceived risk significantly affect behavioral intention, in some of these studies, trust negatively affects perceptions of risk.
When it comes to applications that require context awareness~\cite{10.1145/3313831.3376506}, as it involves machines to learn subjective data, clinicians have significant concerns about trusting AI.

Existing studies have not explicitly examined whether perceived risk plays a mediating role~\cite{AMEEN2021106548}.
As clinicians are likely to perceive AI guidance as being highly risky for patients, trust will likely play a significant role in behavioral intention than perceived risk.
Instead, its role will be more critical in reducing risk perceptions for misdiagnosing a patient.

\vspace{2.25mm}

\noindent
Given the wide applicability of UTAUT, we can anticipate that:

\vspace{2.25mm}

\noindent
{\bf H9.1.} The willingness to follow the AI guidance positively and directly influences perceived security;

\vspace{2.25mm}

\noindent
{\bf H9.2.} The willingness to follow the AI guidance positively affects intention to use AI systems;

\section{Methods}
\label{sec:chap004004}

Our research model includes ten constructs.
For this study, we measured each construct with multiple items (Figure~\ref{fig:fig075}).
To preserve the content validity and reliability, we measured most of our items from the extant literature~\cite{SOHN2020101324, info:doi/10.2196/14316}.
From here, we adjusted these items in our questionnaire to match the context of AI adoption among clinicians.
In compliance with recommendations of the two-stage analytical procedure, we used Confirmatory Factor Analysis (CFA) to test the measurement modelâ€™s validity and reliability~\cite{2019-07124-034}.
In addition, we used Structural Equation Modeling (SEM) as a preferable technique to regression.
It allows simultaneous analysis of all relationships through multiple regression, while also allowing for both observed and latent variables to be analyzed simultaneously and providing overall fit statistics.
We conducted CFA using the \texttt{R language} (\texttt{v 4.0.2}) employing Maximum Likelihood Estimation (MLE).
Furthermore, MLE was then followed by path analysis of the structural relationships conducted in the \texttt{R language} with SEM libraries (\texttt{lavaan v. 0.6-7} and \texttt{semTools v. 0.5-3}).
We also undertook moderation analysis in the \texttt{R language}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
\centering
\includegraphics[width=0.985\linewidth]{fig075}
\caption{The basic statistical flow steps followed by us are usually taken in the framework of SEM modeling. Before employing the SEM modeling, we used the conceptual EFA to reduce and group descriptors. Here, we identified the latent factors by using the CFA. Finally, we applied the SEM to estimate the coefficients.}
\label{fig:fig075}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Procedures}
\label{sec:chap004004001}

We sent the questionnaire via social media ({\it e.g.}, radiology groups on Facebook and LinkedIn) to 17203 participants who gave prior permission to use their data for research purposes under this international study.
The questionnaire contained i) a general explanation of the study, ii) the details of the privacy policy, iii) data treatment, and a iv) link to a Google Forms survey.
We sent the questionnaire in the middle of February 2021.
The criterion was to send the questionnaire to clinicians worldwide who are members of these social media target groups ({\it i.e.}, radiologists, physicians from any other expertise, technicians, etc.).
Overall, this procedure remained until the end of July 2021, when we analysed and discussed our data.

\subsection{Participants}
\label{sec:chap004004002}

Table~\ref{tab:tab008} (Section~\ref{chap:app003003} of Appendix~\ref{chap:app003}) is listing the sample demographics under this study.
In total, we collected data from 322 participants, corresponding to an overall participation of 1.9\%.
Regarding general demographics (N = 322), the sample consisted of a slightly higher proportion of men (59.9\%) than women (38.8\%) with 1.2\% answering ``Prefer not to say'' during the questionnaire.
For age groups, 12.1\% of participants range from 18 to 29 years old, 29.5\% from 30 to 39, 22.7\% from 40 to 49, 23.9\% from 50 to 59, 10.2\% older than 59, and 1.6 answering ``Prefer not to say'' during the questionnaire.

Other demographic data are important to this study, such as medical experience of participants (Section~\ref{chap:app003003001}), training levels (Section~\ref{chap:app003003002}), and expertise areas (Section~\ref{chap:app003003003}).
In this study, the education level of participants is also important to denote relations between demographic data.
Specifically, 34.5\% of participants are specialists, 25.2\% have at least one bachelor degree, 24.2\% have a master degree, and 14\% a PhD.
The other 2.1\% are typically students or academic associate degrees.

\section{Results}
\label{sec:chap004005}

In this study, we examined the sample characteristics of participants (Section~\ref{chap:app003004} of Appendix~\ref{chap:app003}) to gain insights into their profiles and experiences in the medical imaging domain.
From Section~\ref{chap:app003004} of Appendix~\ref{chap:app003}, Table~\ref{tab:tab008} presents an overview of these characteristics, shedding light on the typical participant in this study.
The findings reveal that the typical participant is a radiology specialist in the mid-career stage, primarily working in the public sector.
Most clinicians engage in the analysis of medical imaging exams regularly, with a majority analyzing patients daily.
Furthermore, participants demonstrate varying familiarity with \ac{AI} systems, ranging from awareness without usage to regular utilization.
These insights into the participants' profiles and experiences provide valuable context for understanding their perspectives and behaviors in adopting \ac{AI} technology in healthcare settings.
For a comprehensive summary of all these results, please refer to Section~\ref{chap:app003004006} of Appendix~\ref{chap:app003}.

\subsection{Checking Assumptions}
\label{sec:chap004005001}

The assumptions of the \ac{SEM} were met in this study (Section~\ref{chap:app003004001} of Appendix~\ref{chap:app003004}), with normal distributions observed for all variables~\cite{murtagh2012multivariate}.
The overall model fit was good~\cite{Bagozzi2012, doi:10.1504/IJMDA.2017.087624, murtagh2012multivariate, SUMAK2016602}, as indicated by \acp{GFI} of 0.824 surpassing the recommended thresholds.
While some indices were slightly below the recommended levels, the literature suggests that sample size can impact these fit indices~\cite{KHALILZADEH2017460, doi:10.1080/00273171.2019.1602503, ZHOU2010760}.
Notably, the ratio of the test statistic ($\chi^2$) to the degree of freedom ($df$) was within the recommended range ($\chi^2$ / $df$ = 2.587 $<$ 3.0), and both \ac{RMSEA} and \ac{SRMR} of 0.083 indicated an acceptable model fit~\cite{ZHOU2010760}.
Despite a significant $\chi^2$ value due to the large sample size, the Hoelter index confirmed that the sample adequately supported the sophisticated model~\cite{https://doi.org/10.1002/nur.20088}.

\subsection{Measurement Model}
\label{sec:chap004005002}

Convergent and discriminant validity were examined to ensure the robustness of the measurement model~\cite{10.3389/fpubh.2018.00149, Henseler2015}.
All item loadings were significant (p-value = 0.000), indicating good convergent validity, with most loadings above 0.8~\cite{info:doi/10.2196/14316}.
Each item significantly loaded on its respective construct, demonstrating convergent validity.
Discriminant validity was confirmed by comparing shared variance and \ac{AVE}, with shared variance lower than \ac{AVE}.
The measurement model exhibited good reliability, with a \ac{CR} of 0.95~\cite{Bagozzi2012, doi:10.1504/IJMDA.2017.087624, murtagh2012multivariate, SUMAK2016602}.
These findings confirm the reliability, convergent validity, and discriminant validity of the measurement model.
Moderating effects on other variables, particularly demographics (Section~\ref{sec:chap004005005}), warrant further investigation.
More details are provided in Section~\ref{chap:app003004002} of Appendix~\ref{chap:app003}, with Table~\ref{tab:tab009} presenting the \ac{CFA} results.

\subsection{Parallel Analysis}
\label{sec:chap004005003}

Parallel analysis was conducted to determine the appropriate number of factors to retain in the measurement model~\cite{doi:10.1080/10705511.2019.1615835}.
More details on the parallel analysis results will be described in Section~\ref{chap:app003004003} of Appendix~\ref{chap:app003}.
By simulating eigenvalues based on the number of items and sample size, the parallel analysis compares these simulated eigenvalues with the eigenvalues obtained from the study (Figure~\ref{fig:fig093}).
Factors with eigenvalues that overlap with the cutoff line for retention indicate potential factors to consider.
The scree plot in Figure~\ref{fig:fig093} shows notable changes in effect size, suggesting a breakpoint for the number of factors.
However, the decision to retain factors is not solely based on these changes, but also considers the line limits of the parallel analysis.
The difference in eigenvalues becomes more significant when comparing the fifth and sixth components, as well as the sixth and seventh components.
Components eight and nine are mainly below the line, highlighting the arbitrary nature of item retention for factors or overall models~\cite{doi:10.1207/S15328031US0201}.
Therefore, the analysis suggests retaining factors up to the seventh component, resulting in a total of seven factors in the measurement model.

\subsection{Structural Model}
\label{sec:chap004005004}

The satisfactory fit of the measurement model allowed us to employ \ac{SEM} for data analysis.
\ac{SEM} evaluates the extent to which the collected data align with the theoretical model's measurement~\cite{doi:10.1080/10705511.2017.1401932}.
In this study, we extended the proposed research model by introducing two new constructs:
(1) Clinical Workflow Impact; and
(2) \ac{AI} Guidance.
These additions enabled us to explore the interactions between these constructs and existing variables within the model.
We assessed the structural relationships by estimating the hypothesized causal paths, with the majority of hypotheses supported (except for {\bf H6.1}, {\bf H6.3}, and {\bf H7.1}) based on a significance level of {\it p-value} $<$ 0.10 from Table~\ref{tab:tab011}.
For a more comprehensive analysis of \ac{SEM} and its evaluation, refer to Section~\ref{chap:app003004004} of Appendix~\ref{chap:app003}.
This section provides detailed insights into the relationships between variables, further supporting our research findings.

The coefficient of determination ($R^2$), also known as \ac{SMC}, provides insights into the explanatory power and predictive accuracy of the measurement model (Table~\ref{tab:tab010}).
Higher loadings indicate stronger relationships between the items and their corresponding latent constructs.
Behavioral intention exhibited the highest value ($R^2$ = 0.681), indicating a substantial amount of variance explained.
Conversely, \ac{AI} Guidance had the lowest value ($R^2$ = 0.238), followed by security ($R^2$ = 0.397).
These lower values can be attributed to their close association with independent variables and the inherent nature of the constructs, which are influenced by a clinician's belief in \ac{AI} systems.
The coefficient for performance expectancy ($R^2$ = 0.595), effort expectancy ($R^2$ = 0.496), and trust ($R^2$ = 0.465) were also acceptable, which is consistent with previous results~\cite{KHALILZADEH2017460}.
These coefficients indicate the extent to which the independent variables can account for the variance in the dependent variable.

\subsection{Moderating Effects}
\label{sec:chap004005005}

To examine potential moderator effects, we conducted a comprehensive analysis using the split sample approach \cite{LI2021106581, LI2021106929}.
We investigated the moderation of gender, age, nationality, education, and clinical knowledge on the relationships within our research model.
Notably, significant moderating effects were found for all variables, providing insights into the nuanced dynamics and contextual influences within the model.
The detailed results are presented in Table~\ref{tab:tab012}, which includes the path coefficients and significance levels for each moderator variable.
These findings underscore the importance of considering demographic factors in understanding the relationships in our study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/tab012}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We assessed measurement invariance using $X^2$ difference tests and fit indices, providing robust evidence of measurement invariance at a significance level of {\it p-value} $<$ 0.001.
This indicates that the moderating effects observed across different groups are statistically significant and reliable.
Our findings provide valuable insights into the influence of these factors on the relationships within the model.
For more detailed results, please refer to Section~\ref{chap:app003004005} of Appendix~\ref{chap:app003004}.

\section{Discussion}
\label{sec:chap004006}

In this section, we focus on the obtained results, delving into their significance and implications.
For a more comprehensive synthesis and reflection on our findings, please refer to Section~\ref{chap:app003005} of Appendix~\ref{chap:app003}.
The study proposed a theoretical research model to examine the influence of security, risk, and trust on the acceptance of \ac{AI} recommendations, based on the \ac{UTAUT} model.
The implications and theoretical insights derived from these results are discussed in the subsequent sections.

\subsection{Main Findings}
\label{sec:chap004006001}

Figure~\ref{fig:fig074} provides a visual representation of the main findings.
The main findings indicate that facilitating conditions positively predict the intention to use \ac{AI} ({\bf H1.1.}) in the clinical workflow, supporting the positive effect of facilitating conditions on behavioral intention~\cite{DWIVEDI2016174, Seethamraju2018}.
Additionally, there is a significant relationship between facilitating conditions and effort expectancy ({\bf H1.2.}).
Social influence has a direct impact on effort expectancy ({\bf H2.1.}), perceived security ({\bf H2.2.}), and performance expectancy ({\bf H2.3.}), indicating that it positively influences the adoption of \ac{AI} systems in this medical domain.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htpb]
\centering
\includegraphics[width=0.794\linewidth]{fig074}
\caption{Detailed results of the research model. For beta results, the significance values are: in green for *** significant at level $\alpha = 0.01$; yellow for ** significant at $\alpha = 0.05$; orange for * significant fixed at $\alpha = 0.10$; and red for \underline{n}ot \underline{s}ignificant (ns).}
\label{fig:fig074}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The study also demonstrates the positive impact of performance expectancy on clinicians' intention to use \ac{AI} ({\bf H3.}), particularly for {\it male expert} clinicians (Table~\ref{tab:tab012}).
Effort expectancy positively affects performance expectancy ({\bf H4.}), and the mediating effects vary for different groups.
Security strongly influences clinicians' trust in \ac{AI} systems ({\bf H5.1.}), impacting attitude and behavioral intention ({\bf H5.2.}).
Perceived risk is an important construct for certain groups ({\bf H6.3.}), influencing the belief in \ac{AI} performance and trust in patient privacy.
The impact of trust on performance expectancy ({\bf H7.1.}) and effort expectancy ({\bf H7.2.}) is weaker for {\it young} and {\it novice} clinicians but stronger for {\it expert} clinicians.
The role of \ac{AI} guidance on security ({\bf H9.1.}) and behavioral intention ({\bf H9.2.}) remains significant across different groups, with some variations observed.
While several of our hypotheses ({\it e.g.}, {\bf H6.1.}, {\bf H6.2.}, {\bf H8.1.}, or {\bf H8.2.}) did not find support, our comprehensive analysis in Section~\ref{chap:app003005001} of Appendix~\ref{chap:app003} delves into these constructs in detail, revealing their significance for specific groups in the moderating effects.

\subsection{Design Recommendations}
\label{sec:chap004006002}

In this section, we present design recommendations based on the findings of our study, which can provide valuable guidelines and implications for the design and development of intelligent agents in the clinical workflow of medical imaging.
These recommendations offer a new perspective for designing and developing the next generation of \ac{AI} systems, focusing on ensuring the successful implementation of the technology for patient diagnosis.
One key aspect emphasized in our study is the critical role of trust in accepting intelligent agents, aligning with the viewpoints expressed by other authors~\cite{LIU2022107026}.
Building and maintaining clinicians' trust is a high priority, as it facilitates user engagement and acceptance.
For further details on design considerations, please refer to Section~\ref{chap:app003005002} of Appendix~\ref{chap:app003}.

\vspace{1.50mm}

\noindent
From the following aspects, efforts can be made concerning our design recommendations:

\vspace{0.50mm}

\begin{enumerate}
\item An intelligent agent should inform clinicians about the consequences of following \ac{AI} guidance and its impact on the clinical workflow while providing control over choice;
\item An \ac{AI} system should consider demographic data while adapting the intelligent agent communication depending on gender, age, and country development categories of clinicians;
\item An \ac{AI} system should take into account the variability of different medical cases while paying attention to individual conditions and respective needs with varied \ac{AI} techniques;
\item Designers and developers should enhance the ease of use (usability) and usefulness (functionality) of \ac{AI} systems to improve perceived security, perceived risk, and trust of clinicians;
\end{enumerate}

Our study offers design recommendations for the successful implementation of \ac{AI} technology in the clinical workflow of medical imaging.
These recommendations include providing informative \ac{AI} guidance, personalizing communication based on demographic data, considering case variability, and enhancing usability and functionality.
For more design considerations, please refer to Section~\ref{chap:app003005002} of Appendix~\ref{chap:app003}.

\section{Boundaries}
\label{sec:chap004007}

Despite its contributions, the study has some boundaries, which provide fruitful avenues for further research.
Several of these boundaries affect the scope of our results.
Although we had a significant sample of global nationalities, there is still a bias for the small sample size (Table~\ref{tab:tab008}) of specific nationalities.
To understand the effect of this bias, we analyzed the moderator effect of country development categories in our model, which showed the same evidence of invariant measure compared to other moderators (gender, age, etc.).
However, we did not record race and ethnicity differences in our sample.
Previous works show a significant impact of cultural diversity on social influence, usefulness, and behavior intention~\cite{Belanche2019, info:doi/10.2196/27122}.

Another limitation of our study is that it involved clinicians mainly related to radiology.
Given the focus of our study, {\it i.e.}, the medical imaging field, our sample could be biased.
The effect of such characteristics limits the generalizability of this research, since the sample employed in this study could express different perceptions towards \ac{AI} systems compared to other clinical expertise.
However, the experimental design helps reduce the impact of the \ac{CMB}, which we employed in this study, particularly for the new \ac{AI} constructs.
Moreover, by combining the survey with outcome variables measured separately and more objectively ({\it e.g.}, frequency of usage/reporting), we are revealing valid results less prone to measurement and method bias.

Despite the above boundaries, this study fosters the community's understanding of the intention to use AI systems in the clinical workflow. Specifically for the associated constructs of security, risk, and trust concerns on \ac{AI} systems.
Finally, we provide valuable design guidelines and recommendations for delivering \ac{AI} systems with these concerns to different user groups.

\section{Conclusion}
\label{sec:chap004008}

In this chapter, we conducted a comprehensive investigation into the acceptance and adoption of \ac{AI} systems in the clinical workflow of medical imaging diagnosis.
The study focused on understanding the role of security, risk, and trust in influencing clinicians' acceptance of \ac{AI}-based assistance.
Through empirical testing, we confirmed that trust plays a critical role in shaping user acceptance, mediating the effects of \ac{AI}-specific aspects of user behavior.
Additionally, clinicians' demographic characteristics, such as gender, age, and educational background, moderate the relationship between these factors and their intention to use \ac{AI} systems.
These findings underscore the importance of considering clinicians' specific needs and characteristics when developing intelligent agents for medical imaging diagnosis.
Integrating \ac{HCI} principles into the development of \ac{AI}-based assistance creates user-friendly and effective systems, enhancing clinical workflows and improving patient outcomes.

The research conducted in this chapter provides valuable insights and implications for the design, development, and deployment of \ac{AI}-based assistance in healthcare.
The study highlights the significance of trust, addresses security, and risk concerns, while offering practical guidelines for integrating \ac{AI} systems into the clinical workflow, building upon the design interventions discussed in Chapter~\ref{chap:chap005}.
Chapter~\ref{chap:chap006} further explores the importance of tailoring communication methods to address the specific needs of different user groups, enhancing clinicians' engagement and acceptance of \ac{AI} technology.
This comprehensive approach, encompassing adoption studies (Chapter~\ref{chap:chap004}), design interventions (Chapter~\ref{chap:chap005}), and tailored adaptive communication (Chapter~\ref{chap:chap006}), promotes the successful acceptance and integration of \ac{AI} systems in healthcare settings.
The final remarks can be found in Section~\ref{chap:app003006} of Appendix~\ref{chap:app003}, providing a more comprehensive and in-depth perspective on the summarized conclusions.